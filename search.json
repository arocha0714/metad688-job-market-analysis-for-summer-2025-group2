[
  {
    "objectID": "modeling.html",
    "href": "modeling.html",
    "title": "Data Cleaning and Modeling",
    "section": "",
    "text": "Code\nimport findspark\nfindspark.init()\n\nfrom pyspark.sql import SparkSession\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\nimport numpy as np\n\nnp.random.seed(42)\n\npio.renderers.default = \"notebook\"\n\n# Initialize Spark Session\nspark = SparkSession.builder.appName(\"LightcastData\").getOrCreate()\n\n# Load Data\ndf = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").option(\"escape\", \"\\\"\").csv(\"./data/lightcast_job_postings.csv\")\n\n# Show Schema and Sample Data\nprint(\"---This is Diagnostic check, No need to print it in the final doc---\")\n\ndf.printSchema() # comment this line when rendering the submission\ndf.show(5)\n\n\n\nFigureÂ 1\n\n\n\n\n\nCode\nimport pandas as pd\ndf = pd.read_csv(\"./data/lightcast_job_postings.csv\")\n\n\n\n\nCode\nimport pandas as pd\n\ndf = pd.read_csv(\"./data/lightcast_job_postings.csv\")\n\ncolumns_to_drop = [\n    \"ID\", \"URL\", \"ACTIVE_URLS\", \"DUPLICATES\", \"LAST_UPDATED_TIMESTAMP\",\n    \"NAICS2\", \"NAICS3\", \"NAICS4\", \"NAICS5\", \"NAICS6\",\n    \"SOC_2\", \"SOC_3\", \"SOC_5\"\n]\n\ndf.drop(columns=columns_to_drop, inplace=True)\n\n\n\n\nCode\nprint(df.columns.tolist())\n\n\n\n\nCode\n!pip install missingno\n\n\n\n\nCode\nimport missingno as msno\nimport matplotlib.pyplot as plt\n# Visualize missing values\nmsno.heatmap(df)\nplt.title(\"Missing Values Heatmap\")\nplt.show()\n\n# Drop columns with &gt;50% missing values\ndf.dropna(thresh=len(df) * 0.5, axis=1, inplace=True)\n\n# Fill only the columns you actually have\nif 'Industry' in df.columns:\n    df[\"Industry\"].fillna(\"Unknown\", inplace=True)\n    df[\"Salary\"].fillna(df[\"Salary\"].median(), inplace=True)\n\n\n\n\nCode\ndf = df.drop_duplicates(subset=[\"TITLE\", \"COMPANY\", \"LOCATION\", \"POSTED\"], keep=\"first\")\n\n\n\n\nCode\ndf = df[df['NAICS_2022_2_NAME'] != 'Unclassified Industry']\n\n\n\n\nCode\ndf['REMOTE_TYPE_NAME'] = df['REMOTE_TYPE_NAME'].replace('[None]', 'Not Remote')\n\n\n\n\nCode\nimport pandas as pd\nimport plotly.express as px\n\n# Step 1: Prepare data\ndata = {\n    'Industry': [\n        'Wholesale Trade', 'Retail Trade', 'Real Estate and Rental and Leasing',\n        'Professional, Scientific, and Technical Services', 'Manufacturing',\n        'Information', 'Health Care and Social Assistance',\n        'Finance and Insurance', 'Educational Services',\n        'Administrative and Support and Waste Management and Remediation Services'\n    ],\n    'Flexible Remote': [87.8, 94.4, 97.6, 92.2, 89.7, 95.8, 92.1, 94.8, 89.0, 94.8],\n    'Onsite': [12.2, 5.6, 2.4, 7.8, 10.3, 4.2, 7.9, 5.2, 11.0, 5.2]\n}\n\ndf = pd.DataFrame(data)\n\n# Step 2: Sort in ascending order of Flexible Remote\ndf_sorted = df.sort_values(by='Flexible Remote', ascending=True)\ndf_sorted['Industry'] = pd.Categorical(df_sorted['Industry'], categories=df_sorted['Industry'], ordered=True)\n\n# Step 3: Melt data for stacked bar format\ndf_melted = df_sorted.melt(\n    id_vars='Industry',\n    value_vars=['Flexible Remote', 'Onsite'],\n    var_name='Remote Type',\n    value_name='Percentage'\n)\n\n# Step 4: Plot\nfig = px.bar(\n    df_melted,\n    x='Percentage',\n    y='Industry',\n    color='Remote Type',\n    orientation='h',\n    text='Percentage',\n    color_discrete_map={\n        'Flexible Remote': '#1aab89',\n        'Onsite': '#88d4c3'\n    },\n    title=\"Remote Job Distribution by Industry (Top 10 Industries)\"\n)\n\n# Step 5: Layout adjustments\nfig.update_layout(\n    xaxis_title=\"Percentage of Jobs\",\n    yaxis_title=\"\",\n    xaxis=dict(tickformat=\".0f\"),\n    legend_title=\"Remote Type\",\n    barmode='stack',\n    margin=dict(l=10, r=10, t=60, b=40),\n    height=500\n)\n\n# Step 6: Label formatting\nfig.update_traces(texttemplate='%{text:.1f}%', textposition='inside')\n\n# Save plot\nfig.write_html(\"./figures/top_industries.html\")\n\n\n# Show plot\nfig.show()\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\n\nCode\n#read files\nfile_path = \"./data/lightcast_job_postings.csv\"\ndf = pd.read_csv(file_path)\n\n\n\n\nCode\nprint(df['REMOTE_TYPE_NAME'].value_counts(dropna=False).head(10))\n\n\n\n\nCode\n# Step 1: Standardize formatting\ndf['REMOTE_TYPE_NAME'] = (\n    df['REMOTE_TYPE_NAME']\n    .astype(str)\n    .str.strip()\n    .str.title()\n    .replace({'None': pd.NA, 'Nan': pd.NA})\n)\n\n\n\n\nCode\n# Step 2: Fill missing or ambiguous entries with 'Not Remote'\ndf['REMOTE_TYPE_NAME'] = df['REMOTE_TYPE_NAME'].fillna('Not Remote')\ndf.loc[df['REMOTE_TYPE_NAME'] == \"[None]\", 'REMOTE_TYPE_NAME'] = \"Not Remote\"\nprint(df['REMOTE_TYPE_NAME'].value_counts(dropna=False).head(10))\n\n\n\n\nCode\n# Convert all values to strings and strip whitespace\ndf['REMOTE_TYPE_NAME'] = df['REMOTE_TYPE_NAME'].astype(str).str.strip()\n\n\n\n\nCode\n# Apply new classification logic\ndf['REMOTE_BINARY'] = df['REMOTE_TYPE_NAME'].apply(\n    lambda x: 1 if x in ['Remote', 'Hybrid Remote'] else 0\n)\n\n\n\n\nCode\nprint(df['REMOTE_TYPE_NAME'].value_counts())\nprint(\"\\nBinary classification:\")\nprint(df['REMOTE_BINARY'].value_counts())\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n# Ensure salary columns are numeric and handle missing values\ndf['SALARY_FROM'] = pd.to_numeric(df['SALARY_FROM'], errors='coerce').replace(0, np.nan)\ndf['SALARY_TO'] = pd.to_numeric(df['SALARY_TO'], errors='coerce').replace(0, np.nan)\n\n# Calculate average salary (mean of SALARY_FROM and SALARY_TO)\ndf['AVERAGE_SALARY'] = df[['SALARY_FROM', 'SALARY_TO']].mean(axis=1)\n\n# Drop rows with missing values in AVERAGE_SALARY, REMOTE_TYPE_NAME, or STATE_NAME\ndf_salary = df.dropna(subset=['AVERAGE_SALARY', 'REMOTE_TYPE_NAME', 'STATE_NAME'])\n\n# Group by state and remote type, then calculate average salary\navg_salary_by_state_remote = df_salary.groupby(['STATE_NAME', 'REMOTE_TYPE_NAME'])['AVERAGE_SALARY'].mean().reset_index()\n\n# Round the results for easier reading\navg_salary_by_state_remote['AVERAGE_SALARY'] = avg_salary_by_state_remote['AVERAGE_SALARY'].round(2)\n\n# Show results\nprint(avg_salary_by_state_remote)\n\n\n\n\nCode\ndf = df.merge(avg_salary_by_state_remote,\n              on=['STATE_NAME', 'REMOTE_TYPE_NAME'],\n              how='left')\n\n\n\n\nCode\ndf = df.merge(\n    avg_salary_by_state_remote,\n    on=['STATE_NAME', 'REMOTE_TYPE_NAME'],\n    how='left',\n    suffixes=('', '_STATE_REMOTE_AVG')\n)\n\n\n\n\nCode\n[col for col in df.columns if 'AVG' in col or 'SALARY' in col]\n\n\n\n\nCode\ndf = df.rename(columns={'AVERAGE_SALARY_y': 'AVERAGE_SALARY_STATE_REMOTE_AVG'})\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import (accuracy_score, f1_score, confusion_matrix,\n                             classification_report, precision_score,\n                             recall_score, balanced_accuracy_score)\nfrom sklearn.inspection import permutation_importance\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n\n\nCode\n# Remove duplicate column names across full dataframe\ndf = df.loc[:, ~df.columns.duplicated()]\n\n\n\n\nCode\ndf['AVG_YEARS_EXPERIENCE'] = (df['MIN_YEARS_EXPERIENCE'] + df['MAX_YEARS_EXPERIENCE']) / 2\ndf['EXP_SPREAD'] = df['MAX_YEARS_EXPERIENCE'] - df['MIN_YEARS_EXPERIENCE']\n\n\n\n\nCode\ndf = df.drop(columns=['MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE'])\n\n\n\n\nCode\nnum_feats = [\n    'AVG_YEARS_EXPERIENCE',\n    'AVERAGE_SALARY_STATE_REMOTE_AVG',\n    'IS_INTERNSHIP'\n]\n\ncat_feats = [\n    'STATE_NAME',\n    'NAICS_2022_2_NAME',\n    'EDUCATION_LEVELS_NAME',\n    'COMMON_SKILLS_NAME',\n    'SOFTWARE_SKILLS_NAME',\n    'TITLE_CLEAN'\n    \n]\n\nX = df[num_feats + cat_feats]\ny = df['REMOTE_BINARY']\n\n\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n\n\nCode\npreprocess = ColumnTransformer(transformers=[\n    (\"num\", StandardScaler(), num_feats),\n    (\"cat\", OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_feats)\n])\n\n\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\n# Preprocessing step\npreprocess = ColumnTransformer(transformers=[\n    (\"num\", StandardScaler(), num_feats),\n    (\"cat\", OneHotEncoder(handle_unknown='ignore', max_categories=500, sparse_output=False), cat_feats)\n])\nclf = RandomForestClassifier(random_state=42, class_weight='balanced')\n\n\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\n\nrf = RandomForestClassifier(\n    n_estimators=200,\n    max_depth=None,\n    class_weight='balanced',\n    random_state=42,\n    n_jobs=-1\n)\n\npipe = Pipeline(steps=[\n    ('prep', preprocess),\n    ('model', rf)\n])\n\n\n\n\nCode\npipe.fit(X_train, y_train)\n\n\n\n\nCode\ny_pred = pipe.predict(X_test)\n\n# Classification report and confusion matrix\nprint(classification_report(y_test, y_pred))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\n\n# Additional custom metrics\nprint(\"\\nCustom Metrics:\")\nprint(\"Accuracy:\", round(accuracy_score(y_test, y_pred), 3))\nprint(\"F1 Score:\", round(f1_score(y_test, y_pred), 3))\nprint(\"Precision:\", round(precision_score(y_test, y_pred), 3))\nprint(\"Sensitivity (Recall 1):\", round(recall_score(y_test, y_pred), 3))\nprint(\"Specificity (Recall 0):\", round(\n    recall_score(y_test, y_pred, pos_label=0), 3))\nprint(\"Balanced Accuracy:\", round(balanced_accuracy_score(y_test, y_pred), 3))\n\n\n\n\nCode\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n\n\n\n\nCode\nrf_model       = pipe.named_steps[\"model\"]          # RandomForestClassifier\nfeature_names  = pipe.named_steps[\"prep\"].get_feature_names_out()\n\nimportances = rf_model.feature_importances_\n\nfeat_imp = (\n    pd.DataFrame({\"Feature\": feature_names, \"Importance\": importances})\n      .sort_values(by=\"Importance\", ascending=False)\n      .reset_index(drop=True)\n)\n\nprint(\"\\nTop 9 â Tree-based Importances\")\nprint(feat_imp.head(9).to_string(index=False))\n\n\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ntop_n = 9                     # change to show more/less\nplt.figure(figsize=(8, 6))\nsns.barplot(\n    data=feat_imp.head(top_n),\n    x=\"Importance\", y=\"Feature\",\n    palette=\"crest\"\n)\nplt.title(f\"Top {top_n} Feature Importances (Random Forest)\")\nplt.xlabel(\"Mean Decrease in Impurity\")\nplt.ylabel(\"\")\nplt.tight_layout()\nplt.show()\n\n\n\n\nCode\nimport pandas as pd\nimport plotly.express as px\n\n# Step 1: Create state abbreviation mapping\nus_state_abbrev = {\n    'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR',\n    'California': 'CA', 'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE',\n    'Florida': 'FL', 'Georgia': 'GA', 'Hawaii': 'HI', 'Idaho': 'ID',\n    'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA', 'Kansas': 'KS',\n    'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD',\n    'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS',\n    'Missouri': 'MO', 'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV',\n    'New Hampshire': 'NH', 'New Jersey': 'NJ', 'New Mexico': 'NM', 'New York': 'NY',\n    'North Carolina': 'NC', 'North Dakota': 'ND', 'Ohio': 'OH', 'Oklahoma': 'OK',\n    'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI', 'South Carolina': 'SC',\n    'South Dakota': 'SD', 'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT',\n    'Vermont': 'VT', 'Virginia': 'VA', 'Washington': 'WA', 'West Virginia': 'WV',\n    'Wisconsin': 'WI', 'Wyoming': 'WY', 'District of Columbia': 'DC'\n}\n\n# Step 2: Map state names to abbreviations\ndf['STATE_ABBR'] = df['STATE_NAME'].map(us_state_abbrev)\n\n# Step 3: Group by state and compute metrics\nchoropleth_data = df.groupby('STATE_ABBR').agg(\n    remote_ratio=('REMOTE_BINARY', 'mean'),\n    avg_salary=('AVERAGE_SALARY_STATE_REMOTE_AVG', 'mean'),\n    avg_experience=('AVG_YEARS_EXPERIENCE', 'mean'),\n    job_count=('STATE_NAME', 'count')\n).reset_index()\n\n# Step 4: Define custom green scale (start from light, move to #1aab89)\ncustom_green_scale = [\n    [0, \"#e0f7f1\"],     # light mint\n    [0.5, \"#70d8b5\"],   # mid-green\n    [1, \"#1aab89\"]      # deep teal green\n]\n\n# Step 5: Create the choropleth with custom green\nfig = px.choropleth(\n    data_frame=choropleth_data,\n    locations='STATE_ABBR',\n    locationmode=\"USA-states\",\n    color='remote_ratio',\n    color_continuous_scale=custom_green_scale,\n    scope=\"usa\",\n    labels={'remote_ratio': 'Remote Job Ratio'},\n    hover_data={\n        'remote_ratio': ':.2f',\n        'avg_salary': ':.0f',\n        'avg_experience': ':.1f',\n        'job_count': True\n    },\n    title='Remote Job Ratio by State (Custom Green), Avg Salary & Experience in Hover'\n)\n\nfig.update_layout(margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0})\nfig.write_html(\"./figures/state_remote_job_ratio.html\")\n\nfig.show()\n\n\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Step 1: Ensure 'POSTED' is in datetime format and create Year-Month\ndf['POSTED'] = pd.to_datetime(df['POSTED'])\ndf['POSTED_YM'] = df['POSTED'].dt.to_period('M').astype(str)\n\n\n\n\nCode\nindustry_trends = (\n    df.groupby(['NAICS_2022_2_NAME', 'POSTED_YM'])['REMOTE_BINARY']\n    .mean()\n    .reset_index(name='REMOTE_RATIO')\n)\n\n\n\n\nCode\n# Step 3: Select top 5 industries with highest overall average remote ratio\ntop_industries = (\n    industry_trends.groupby('NAICS_2022_2_NAME')['REMOTE_RATIO']\n    .mean()\n    .sort_values(ascending=False)\n    .head(5)\n    .index.tolist()\n)\nfiltered_trends = industry_trends[industry_trends['NAICS_2022_2_NAME'].isin(top_industries)]\n\n\n\n\nCode\nimport plotly.express as px\n\nfig = px.line(\n    filtered_trends,\n    x='POSTED_YM',\n    y='REMOTE_RATIO',\n    color='NAICS_2022_2_NAME',\n    markers=True,\n    title=\"Top Industries: Remote Job Trends Over Time\"\n)\n\nfig.update_layout(\n    xaxis_title=\"Posted Month\",\n    yaxis_title=\"Remote Job Ratio\",\n    legend_title=\"Industry\",\n    legend=dict(x=1.02, y=1, bordercolor=\"Black\"),\n    margin=dict(l=40, r=40, t=60, b=40),\n    width=1000,\n    height=500\n)\n\nfig.update_xaxes(tickangle=45)\nfig.write_html(\"./figures/remote_job_over_time.html\")\nfig.show()\n\n\n\n\nCode\n#Groupby industry + month and calculate both:\nindustry_month_stats = df.groupby(['NAICS_2022_2_NAME', 'POSTED_YM']).agg(\n    TOTAL_JOBS=('REMOTE_BINARY', 'count'),\n    REMOTE_RATIO=('REMOTE_BINARY', 'mean')\n).reset_index()\n\n\n\n\nCode\njob_count = df.groupby(['NAICS_2022_2_NAME', 'POSTED_YM']).size().reset_index(name='JOB_COUNT')\n\n\n\n\nCode\nremote_ratio = df.groupby(['NAICS_2022_2_NAME', 'POSTED_YM'])['REMOTE_BINARY'].mean().reset_index(name='REMOTE_RATIO')\n\n\n\n\nCode\nindustry_month_stats = pd.merge(remote_ratio, job_count, on=['NAICS_2022_2_NAME', 'POSTED_YM'])\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\n# Choose 2â3 industries to plot (or loop one at a time)\nselected_industries = [\n    'Administrative and Support and Waste Management and Remediation Services',\n    'Arts, Entertainment, and Recreation',\n    'Finance and Insurance',\n    'Real Estate and Rental and Leasing',\n    'Utilities'\n]\n\n\n\nfor industry in selected_industries:\n    data = industry_month_stats[industry_month_stats['NAICS_2022_2_NAME'] == industry]  \n\n    fig, ax1 = plt.subplots(figsize=(10, 4))\n\n    # Plot remote ratio\n    ax1.plot(data['POSTED_YM'], data['REMOTE_RATIO'], color='tab:blue', marker='o')\n    ax1.set_xlabel('Month')\n    ax1.set_ylabel('Remote Job Ratio', color='tab:blue')\n    ax1.tick_params(axis='y', labelcolor='tab:blue')\n    ax1.set_title(f\"Remote Job Ratio & Volume Over Time: {industry}\")\n\n    # Plot job count on secondary y-axis\n    ax2 = ax1.twinx()\n    ax2.bar(data['POSTED_YM'], data['JOB_COUNT'], color='tab:gray', alpha=0.3)\n    ax2.set_ylabel('Total Job Postings', color='gray')\n    ax2.tick_params(axis='y', labelcolor='gray')\n\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.savefig(\"./figures/Remote_jobs_\"+str(industry)+\".jpg\", dpi=300)\n    plt.show()\n\n\n\nRandom Forest results\n  As the two plots displayed a clear results for random forest results, our group has set our target variable for predict peopleâs preferences on Remote versus Onsite job, where 1 represents a remote job and 0 represents onsite. We also had a set of independent variables, including as: âAVG_YEARS_EXPERIENCEâ, âAVERAGE_SALARY_STATE_REMOTE_AVGâ, âIS_INTERNSHIPâ, âSTATE_NAMEâ, âNAICS_2022_2_NAMEâ(industry) âEDUCATION_LEVELS_NAMEâ, âCOMMON_SKILLS_NAMEâ, âSOFTWARE_SKILLS_NAMEâ, âTITLE_CLEANâ (occupation). Meanwhile, we split the data into training and testing sets using an 80/20 ratio to ensure generalizability, This means 20% of the data will go into the test set, and 80% will go into the training set. Then we conduct the randam forest model analysis. According to plots, we can conclude the accuracy reached to 94.6%;F1 score as 84.7%, which reflects the robust balance between precision and recall; the precision as 99.4%, which means it has highly accurate rate on predict the results;the sensitivity for class1 as 73.8%, which means it correctly identified the 74% people who pick remote; the sensitivity for class 0 even reached to 99.9%, which means almost all the people who choose non-remote job has correctly classified; balanced accuracy as 86.8%, which represents there is a balance performance between both cases. From the confusion matrix, there has a detailed display, which represents the model correctly predicts 11536 people who choose onsite jobs with only 13 false positives, and it also orrectly predicts 2177 people who choose remote jobs with 774 missed results.\nOne major limitation is class imbalance. Remote jobs (class 1) are the minority, which leads the model to perform less effectively on them.To counter this, we used class_weight=âbalancedâ in our Random Forest to give more weight to underrepresented classes. We also built a preprocessing pipeline using ColumnTransformer for encoding categorical and scaling numerical features.Still, further optimization is needed. we recommended three ways to overcome this issue in the future study: Adjusting sample sizes;Tuning model hyperparameters, like n_estimators and max_categories; experimenting with resampling techniques; add crossvalidation steps to void overfitting and ensure the final resultsâ accuracy.\n\nFeatured Importance\n Based on the above plot, we can see the top 9 features could be essential in predicting the peoplesâprefrences on remote or onsite jobs, we can see the top three essential features are average remote salary by state, Average years of experience and location state-California. These three features can be easily interpreted as the jobâs salary, jobâs requirement for year experiences and location are vital elements that impact peopleâs decision on onsite or remote job types, peple prioritize jobs with high salary and better geographic location"
  },
  {
    "objectID": "market_trends.html",
    "href": "market_trends.html",
    "title": "Market Trends",
    "section": "",
    "text": "Our group has decided to evaluate the distribution of jobs related to techonology across the United States. In order to do this, we used a count of any jobs containing the word âanalystâ and categorized them by state. The results of this are shown below.\n\nAnalyst Job Distribution Across the United States\n\n\nAccording to the visual above, Texas and California are the two clear leaders in the total amount of jobs being offered that contain the word âanalystâ in the title. Additionally, eastern states show a considerably greater amount of these jobs compared to western states.\n\nThe chart above displays the salary breakdown between states for job postings impacted and not impacted by AI. The size of the data points illustrates the number of job postings that each state has. Interestingly, both Texas and California have the largest amount of job postings not affected by AI. We can conclude that a possiblity for this is that in those tech hubs, the jobs AI can replace decreased in demand.\n\nAnalyst Job Distribution Across Cities in the United States\n\n\nNew York City is the clear leader in analyst jobs available. Another insight is that Texas has 3 cities in the top 10, which indicates a strong job market. Something that surprised us, is that San Francisco finishes lower than expected by rounding out the bottom of the top 10."
  },
  {
    "objectID": "market_trends.html#remote-job-distribution-by-general-industry",
    "href": "market_trends.html#remote-job-distribution-by-general-industry",
    "title": "Market Trends",
    "section": "Remote Job Distribution by General Industry",
    "text": "Remote Job Distribution by General Industry\n\nIn terms of the Remote Job Distribution by general industry, the top 3 industries are Real Estate and Rental and Leasing, Information, Finance and Insurance. As we can see even the traditional industries such as manufacturing, educational services starte to have higher remote job proportion, which means the remote work nowadays has already became a widespread jobs or requirements across multiple industries.\n###Top Industries: Remote job Trends Over time\n\nTake a dive into the Top industries over time with specific industries, in general, this plot displays monthly trends in job postings for from May 2024 to September 2024. As we can see in here, the remote job ratio was represented by the y-axis, which is the proportion of job postings marked as remote out of total postings in each industry per month. Higher remote ratio, represents their have more remote job opportunities open in each industry.\nIndeed, we can see the general trend in this plot as the remote job among different industries, which reaches the peak in Auguest 2024 and then drop siginificantly in September, which suggests that the many companies end their summer recruitings. As we can see the real estate and rental industry goes from high and crash in September. In contrast, the Finance and Insurance industry has higher stabilityï¼ which has consistent job postings from May to September.\n From the above plot result, we can see that the remote job ratio trend started low in May, then continue drop low in June, then reached the peak in August (31%), after that drop low in September. In compare with the trend alighed with gray area: job postings are also have more postings in Auguest and less in September.\n From the above plot result, we can see that the remote job ratio trend started low in May, gradually increase in June, then reached the peak in July (33%), after that gradually drop low August. The job posting area remained high and stable through july, which is friendly for people who are seek for remote jobs.\n From the above plot result, we can see that the remote job ratio trend started high (43%) in May, drop in June, then rebound in July then reached the peak in August (53%), after that has a significantly drop in September(near 0). The job posting area remained comparable stable, which has high volumn in May and June, then has a siginificant drop in July and modest increase in Auguest and September. The differences between Job posting\n\nFrom above choreleograph, we can see that the green color depth represents the remote jobs proportions to their job postings, and with the average salary and average job experiences. To be more clearly, we can see that the Maine, Vermont, Alaska and Colorado have higher remote ratio, which represents their high demands on remote jobs souther state on the othe hand has lighter remote ratio, which represents that there are fewer remote job demands in these regionsâ industries."
  },
  {
    "objectID": "ADAM_geographic_analysis.html",
    "href": "ADAM_geographic_analysis.html",
    "title": "Geographic Analysis",
    "section": "",
    "text": "North American Industry Classification System\n\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\nimport numpy as np\n\nnp.random.seed(42)\n\npio.renderers.default = \"notebook\"\n\n\n## Listing Columns So We Can Reference them in Visuals\n\nimport pandas as pd\ndf = pd.read_csv(\"./data/lightcast_job_postings.csv\")\ndf.head()\n\n/tmp/ipykernel_4047/736325668.py:4: DtypeWarning:\n\nColumns (19,30) have mixed types. Specify dtype option on import or set low_memory=False.\n\n/tmp/ipykernel_4047/736325668.py:4: DtypeWarning:\n\nColumns (19,30) have mixed types. Specify dtype option on import or set low_memory=False.\n\n\n\n\n\n\n\n\n\n\nID\nLAST_UPDATED_DATE\nLAST_UPDATED_TIMESTAMP\nDUPLICATES\nPOSTED\nEXPIRED\nDURATION\nSOURCE_TYPES\nSOURCES\nURL\n...\nNAICS_2022_2\nNAICS_2022_2_NAME\nNAICS_2022_3\nNAICS_2022_3_NAME\nNAICS_2022_4\nNAICS_2022_4_NAME\nNAICS_2022_5\nNAICS_2022_5_NAME\nNAICS_2022_6\nNAICS_2022_6_NAME\n\n\n\n\n0\n1f57d95acf4dc67ed2819eb12f049f6a5c11782c\n9/6/2024\n2024-09-06 20:32:57.352 Z\n0.0\n6/2/2024\n6/8/2024\n6.0\n[\\n \"Company\"\\n]\n[\\n \"brassring.com\"\\n]\n[\\n \"https://sjobs.brassring.com/TGnewUI/Sear...\n...\n44.0\nRetail Trade\n441.0\nMotor Vehicle and Parts Dealers\n4413.0\nAutomotive Parts, Accessories, and Tire Retailers\n44133.0\nAutomotive Parts and Accessories Retailers\n441330.0\nAutomotive Parts and Accessories Retailers\n\n\n1\n0cb072af26757b6c4ea9464472a50a443af681ac\n8/2/2024\n2024-08-02 17:08:58.838 Z\n0.0\n6/2/2024\n8/1/2024\nNaN\n[\\n \"Job Board\"\\n]\n[\\n \"maine.gov\"\\n]\n[\\n \"https://joblink.maine.gov/jobs/1085740\"\\n]\n...\n56.0\nAdministrative and Support and Waste Managemen...\n561.0\nAdministrative and Support Services\n5613.0\nEmployment Services\n56132.0\nTemporary Help Services\n561320.0\nTemporary Help Services\n\n\n2\n85318b12b3331fa490d32ad014379df01855c557\n9/6/2024\n2024-09-06 20:32:57.352 Z\n1.0\n6/2/2024\n7/7/2024\n35.0\n[\\n \"Job Board\"\\n]\n[\\n \"dejobs.org\"\\n]\n[\\n \"https://dejobs.org/dallas-tx/data-analys...\n...\n52.0\nFinance and Insurance\n524.0\nInsurance Carriers and Related Activities\n5242.0\nAgencies, Brokerages, and Other Insurance Rela...\n52429.0\nOther Insurance Related Activities\n524291.0\nClaims Adjusting\n\n\n3\n1b5c3941e54a1889ef4f8ae55b401a550708a310\n9/6/2024\n2024-09-06 20:32:57.352 Z\n1.0\n6/2/2024\n7/20/2024\n48.0\n[\\n \"Job Board\"\\n]\n[\\n \"disabledperson.com\",\\n \"dejobs.org\"\\n]\n[\\n \"https://www.disabledperson.com/jobs/5948...\n...\n52.0\nFinance and Insurance\n522.0\nCredit Intermediation and Related Activities\n5221.0\nDepository Credit Intermediation\n52211.0\nCommercial Banking\n522110.0\nCommercial Banking\n\n\n4\ncb5ca25f02bdf25c13edfede7931508bfd9e858f\n6/19/2024\n2024-06-19 07:00:00.000 Z\n0.0\n6/2/2024\n6/17/2024\n15.0\n[\\n \"FreeJobBoard\"\\n]\n[\\n \"craigslist.org\"\\n]\n[\\n \"https://modesto.craigslist.org/sls/77475...\n...\n99.0\nUnclassified Industry\n999.0\nUnclassified Industry\n9999.0\nUnclassified Industry\n99999.0\nUnclassified Industry\n999999.0\nUnclassified Industry\n\n\n\n\n5 rows Ã 131 columns\n\n\n\n\n# Filter for Boston, MA and Austin, TX\nselected_state = ['California', 'Florida', 'Massachusetts', 'Texas', 'New York']\nfiltered_df = df[df['STATE_NAME'].isin(selected_state)]\n\n# Further filter for NAICS_2022_6 = 518210 and show relevant columns\nfinal_df = filtered_df[filtered_df['LOT_SPECIALIZED_OCCUPATION_NAME'].str.contains('analyst', case=False, na=False)]\nfinal_df[['STATE_NAME', 'NAICS2_NAME', 'NAICS_2022_6', 'LOT_SPECIALIZED_OCCUPATION_NAME']].head(100)\n\n\n\n\n\n\n\n\nSTATE_NAME\nNAICS2_NAME\nNAICS_2022_6\nLOT_SPECIALIZED_OCCUPATION_NAME\n\n\n\n\n2\nTexas\nFinance and Insurance\n524291.0\nData Analyst\n\n\n4\nCalifornia\nUnclassified Industry\n999999.0\nOracle Consultant / Analyst\n\n\n9\nNew York\nProfessional, Scientific, and Technical Services\n541511.0\nData Analyst\n\n\n10\nCalifornia\nWholesale Trade\n423830.0\nData Analyst\n\n\n15\nMassachusetts\nEducational Services\n611310.0\nData Analyst\n\n\n...\n...\n...\n...\n...\n\n\n294\nFlorida\nEducational Services\n611310.0\nSAP Analyst / Admin\n\n\n295\nCalifornia\nFinance and Insurance\n524114.0\nData Analyst\n\n\n296\nNew York\nUnclassified Industry\n999999.0\nGeneral ERP Analyst / Consultant\n\n\n297\nTexas\nProfessional, Scientific, and Technical Services\n541611.0\nSAP Analyst / Admin\n\n\n299\nTexas\nProfessional, Scientific, and Technical Services\n541511.0\nGeneral ERP Analyst / Consultant\n\n\n\n\n100 rows Ã 4 columns\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Group by STATE_NAME and count jobs for NAICS_2022_6 = 518210\nstate_counts_jobs = final_df.groupby('STATE_NAME').size().reset_index(name='job_count')\n\n# Sort state_counts_jobs from greatest to least by job_count\nstate_counts_jobs_sorted = state_counts_jobs.sort_values(by='job_count', ascending=False)\n\n# Plot column chart\nplt.figure(figsize=(8, 5))\ncolors = plt.cm.coolwarm(np.linspace(0, 1, len(state_counts_jobs_sorted)))\nplt.bar(state_counts_jobs_sorted['STATE_NAME'], state_counts_jobs_sorted['job_count'], color=colors)\nplt.xlabel('State')\nplt.ylabel('Number of Jobs')\nplt.title('Tech Jobs by State (Job Title Contains \"Analyst\")')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.savefig('figures/states_analyst_jobs.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Group by STATE_NAME and count jobs for analysts\ncity_counts_jobs = final_df.groupby('CITY_NAME').size().reset_index(name='job_count')\n\n# Sort state_counts_jobs from greatest to least by job_count and get top 10\ncity_counts_jobs_sorted = city_counts_jobs.sort_values(by='job_count', ascending=False).head(10)\n\n# Plot column chart\nplt.figure(figsize=(10, 6))\ncolors = plt.cm.coolwarm(np.linspace(0, 1, len(city_counts_jobs_sorted)))\nplt.bar(city_counts_jobs_sorted['CITY_NAME'], city_counts_jobs_sorted['job_count'], color=colors)\nplt.xlabel('City')\nplt.ylabel('Number of Jobs')\nplt.title('Top 10 Cities - Tech Jobs (Job Title Contains \"Analyst\")')\nplt.xticks(rotation=45)\nplt.tight_layout()\n\n# Save the figure to the figures folder\nplt.savefig('figures/top_10_cities_analyst_jobs.png', dpi=300, bbox_inches='tight')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nimport plotly.express as px\nimport plotly.io as pio\n\n# Create nationwide data - remove state filtering to include all states\ntry:\n    # Use the original df (before state filtering) to get all states\n    all_states_df = df[df['LOT_SPECIALIZED_OCCUPATION_NAME'].str.contains('analyst', case=False, na=False)]\nexcept NameError:\n    # If df not available, load it fresh\n    import pandas as pd\n    df = pd.read_csv(\"./data/lightcast_job_postings.csv\")\n    all_states_df = df[df['LOT_SPECIALIZED_OCCUPATION_NAME'].str.contains('analyst', case=False, na=False)]\n\n# Group by all states and count jobs\nall_state_counts = all_states_df.groupby('STATE_NAME').size().reset_index(name='job_count')\nall_state_counts_sorted = all_state_counts.sort_values(by='job_count', ascending=False)\n\nprint(\"Top 10 states with most analyst jobs:\")\nprint(all_state_counts_sorted.head(10))\nprint(f\"\\nTotal states with analyst jobs: {len(all_state_counts_sorted)}\")\n\n# Comprehensive state abbreviation mapping\nstate_abbrev_map = {\n    'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR', 'California': 'CA',\n    'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE', 'Florida': 'FL', 'Georgia': 'GA',\n    'Hawaii': 'HI', 'Idaho': 'ID', 'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA',\n    'Kansas': 'KS', 'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD',\n    'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS', 'Missouri': 'MO',\n    'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV', 'New Hampshire': 'NH', 'New Jersey': 'NJ',\n    'New Mexico': 'NM', 'New York': 'NY', 'North Carolina': 'NC', 'North Dakota': 'ND', 'Ohio': 'OH',\n    'Oklahoma': 'OK', 'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI', 'South Carolina': 'SC',\n    'South Dakota': 'SD', 'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT', 'Vermont': 'VT',\n    'Virginia': 'VA', 'Washington': 'WA', 'West Virginia': 'WV', 'Wisconsin': 'WI', 'Wyoming': 'WY',\n    'District of Columbia': 'DC'\n}\n\n# Add state abbreviations to the data\nall_state_counts_sorted['state_abbrev'] = all_state_counts_sorted['STATE_NAME'].map(state_abbrev_map)\n\n# Filter out any states that couldn't be mapped (in case of data issues)\nmapped_states = all_state_counts_sorted.dropna(subset=['state_abbrev'])\n\nprint(f\"\\nStates successfully mapped: {len(mapped_states)}\")\nif len(mapped_states) &lt; len(all_state_counts_sorted):\n    unmapped = all_state_counts_sorted[all_state_counts_sorted['state_abbrev'].isna()]\n    print(\"Unmapped states:\")\n    print(unmapped['STATE_NAME'].tolist())\n\n# Create a choropleth map showing job counts for all states\nfig = px.choropleth(\n    mapped_states,\n    locations='state_abbrev',\n    color='job_count',\n    locationmode='USA-states',\n    color_continuous_scale='Greens',\n    labels={'job_count': 'Number of Jobs', 'STATE_NAME': 'State'},\n    hover_name='STATE_NAME',\n    hover_data={'state_abbrev': False, 'job_count': True}\n)\n\n# Update layout with no title to maximize map space\nfig.update_layout(\n    geo_scope='usa',\n    width=1000,\n    height=700,\n    margin=dict(t=5, b=5, l=5, r=5),  # Minimal margins on all sides\n    geo=dict(\n        projection_type='albers usa',\n        showlakes=True,\n        lakecolor='rgb(255, 255, 255)',\n        bgcolor='rgba(0,0,0,0)'\n    )\n)\n\n# Show the interactive map\nfig.show()\n\nTop 10 states with most analyst jobs:\n        STATE_NAME  job_count\n42           Texas       7084\n4       California       6501\n8          Florida       3206\n31        New York       3056\n12        Illinois       3045\n45        Virginia       2989\n32  North Carolina       2423\n34            Ohio       2403\n9          Georgia       2363\n29      New Jersey       2289\n\nTotal states with analyst jobs: 51\n\nStates successfully mapped: 50\nUnmapped states:\n['Washington, D.C. (District of Columbia)']\nTop 10 states with most analyst jobs:\n        STATE_NAME  job_count\n42           Texas       7084\n4       California       6501\n8          Florida       3206\n31        New York       3056\n12        Illinois       3045\n45        Virginia       2989\n32  North Carolina       2423\n34            Ohio       2403\n9          Georgia       2363\n29      New Jersey       2289\n\nTotal states with analyst jobs: 51\n\nStates successfully mapped: 50\nUnmapped states:\n['Washington, D.C. (District of Columbia)']\n\n\n        \n        \n        \n\n\n                                                    \n\n\n\n\nInteractive map showing the distribution of analyst jobs across all US states.\n\n# filter df for NAICS_2022_2 is 44 and is 51\n\n# df_41 = df[df['NAICS_2022_2'].isin(['44'])]\n# \n# df_41.head()\n\n\nselected_naics = [11, 21, 22]\nfiltered_df = df[df['NAICS_2022_2'].isin(selected_naics)]\n\n# Further filter for NAICS_2022_6 = 518210 and show relevant columns\n# final_df = filtered_df[filtered_df['NAICS_2022_2'].str.contains('analyst', case=False, na=False)]\nfiltered_df[['STATE_NAME', 'NAICS_2022_2', 'NAICS_2022_2_NAME', 'LOT_SPECIALIZED_OCCUPATION_NAME']].head(100)\n\n\n\n\n\n\n\n\nSTATE_NAME\nNAICS_2022_2\nNAICS_2022_2_NAME\nLOT_SPECIALIZED_OCCUPATION_NAME\n\n\n\n\n376\nNorth Carolina\n21.0\nMining, Quarrying, and Oil and Gas Extraction\nData Analyst\n\n\n394\nNorth Carolina\n22.0\nUtilities\nGeneral ERP Analyst / Consultant\n\n\n502\nCalifornia\n22.0\nUtilities\nBusiness Analyst (General)\n\n\n525\nNebraska\n22.0\nUtilities\nBusiness Analyst (General)\n\n\n632\nMassachusetts\n22.0\nUtilities\nBusiness Analyst (General)\n\n\n...\n...\n...\n...\n...\n\n\n8531\nMichigan\n21.0\nMining, Quarrying, and Oil and Gas Extraction\nData Analyst\n\n\n8553\nTexas\n22.0\nUtilities\nEnterprise Architect\n\n\n8682\nFlorida\n11.0\nAgriculture, Forestry, Fishing and Hunting\nGeneral ERP Analyst / Consultant\n\n\n8698\nColorado\n22.0\nUtilities\nEnterprise Architect\n\n\n8702\nOregon\n22.0\nUtilities\nEnterprise Architect\n\n\n\n\n100 rows Ã 4 columns\n\n\n\n\nimport plotly.express as px\n\nanalyst_df = df[df['LOT_SPECIALIZED_OCCUPATION_NAME'].str.contains('analyst', case=False, na=False)]\nanalyst_salary_df = analyst_df[analyst_df['SALARY'].notna()]\n\nprint(f\"Total analyst jobs: {len(analyst_df)}\")\nprint(f\"Analyst jobs with salary data: {len(analyst_salary_df)}\")\n\n# Group by state and calculate metrics\nbubble_data = analyst_salary_df.groupby('STATE_NAME').agg({\n    'SALARY': 'mean',  # Average salary for bubble size\n    'STATE_NAME': 'count'  # Count of jobs for y-axis\n}).rename(columns={'STATE_NAME': 'job_count'})\n\nbubble_data = bubble_data.reset_index()\n\nprint(f\"\\nStates with analyst salary data: {len(bubble_data)}\")\nprint(\"\\nTop 10 states by job count:\")\nprint(bubble_data.sort_values('job_count', ascending=False).head(10))\n\n# Filter to top 10 states by job count\ntop_10_states = bubble_data.sort_values('job_count', ascending=False).head(10)\n\n# Create the bubble chart\nfig = px.scatter(\n    top_10_states,\n    x='STATE_NAME',\n    y='job_count',\n    size='SALARY',\n    color='SALARY',\n    hover_name='STATE_NAME',\n    hover_data={\n        'SALARY': ':,.0f',\n        'job_count': True,\n        'STATE_NAME': False\n    },\n    labels={\n        'SALARY': 'Average Salary ($)',\n        'STATE_NAME': 'State',\n        'job_count': 'Number of Jobs (Excludes Null Salaries)'\n    },\n    title='Top 10 States for Analyst Jobs (Bubble Size = Salary)',\n    color_continuous_scale='Greens'\n)\n\n# Customize the layout\nfig.update_layout(\n    width=1200,\n    height=700,\n    xaxis_tickangle=-45,\n    showlegend=True\n)\n\n# Update traces for better bubble appearance\nfig.update_traces(\n    marker=dict(\n        sizemode='diameter',\n        sizeref=2.0*max(top_10_states['SALARY'])/(15.**2),\n        sizemin=4,\n        line=dict(width=1, color='white')\n    )\n)\n\nfig.show()"
  },
  {
    "objectID": "ADAM_geographic_analysis.html#tech-jobs-nationwide-job-title-contains-analyst",
    "href": "ADAM_geographic_analysis.html#tech-jobs-nationwide-job-title-contains-analyst",
    "title": "Geographic Analysis",
    "section": "",
    "text": "Interactive map showing the distribution of analyst jobs across all US states.\n\n# filter df for NAICS_2022_2 is 44 and is 51\n\n# df_41 = df[df['NAICS_2022_2'].isin(['44'])]\n# \n# df_41.head()\n\n\nselected_naics = [11, 21, 22]\nfiltered_df = df[df['NAICS_2022_2'].isin(selected_naics)]\n\n# Further filter for NAICS_2022_6 = 518210 and show relevant columns\n# final_df = filtered_df[filtered_df['NAICS_2022_2'].str.contains('analyst', case=False, na=False)]\nfiltered_df[['STATE_NAME', 'NAICS_2022_2', 'NAICS_2022_2_NAME', 'LOT_SPECIALIZED_OCCUPATION_NAME']].head(100)\n\n\n\n\n\n\n\n\nSTATE_NAME\nNAICS_2022_2\nNAICS_2022_2_NAME\nLOT_SPECIALIZED_OCCUPATION_NAME\n\n\n\n\n376\nNorth Carolina\n21.0\nMining, Quarrying, and Oil and Gas Extraction\nData Analyst\n\n\n394\nNorth Carolina\n22.0\nUtilities\nGeneral ERP Analyst / Consultant\n\n\n502\nCalifornia\n22.0\nUtilities\nBusiness Analyst (General)\n\n\n525\nNebraska\n22.0\nUtilities\nBusiness Analyst (General)\n\n\n632\nMassachusetts\n22.0\nUtilities\nBusiness Analyst (General)\n\n\n...\n...\n...\n...\n...\n\n\n8531\nMichigan\n21.0\nMining, Quarrying, and Oil and Gas Extraction\nData Analyst\n\n\n8553\nTexas\n22.0\nUtilities\nEnterprise Architect\n\n\n8682\nFlorida\n11.0\nAgriculture, Forestry, Fishing and Hunting\nGeneral ERP Analyst / Consultant\n\n\n8698\nColorado\n22.0\nUtilities\nEnterprise Architect\n\n\n8702\nOregon\n22.0\nUtilities\nEnterprise Architect\n\n\n\n\n100 rows Ã 4 columns\n\n\n\n\nimport plotly.express as px\n\nanalyst_df = df[df['LOT_SPECIALIZED_OCCUPATION_NAME'].str.contains('analyst', case=False, na=False)]\nanalyst_salary_df = analyst_df[analyst_df['SALARY'].notna()]\n\nprint(f\"Total analyst jobs: {len(analyst_df)}\")\nprint(f\"Analyst jobs with salary data: {len(analyst_salary_df)}\")\n\n# Group by state and calculate metrics\nbubble_data = analyst_salary_df.groupby('STATE_NAME').agg({\n    'SALARY': 'mean',  # Average salary for bubble size\n    'STATE_NAME': 'count'  # Count of jobs for y-axis\n}).rename(columns={'STATE_NAME': 'job_count'})\n\nbubble_data = bubble_data.reset_index()\n\nprint(f\"\\nStates with analyst salary data: {len(bubble_data)}\")\nprint(\"\\nTop 10 states by job count:\")\nprint(bubble_data.sort_values('job_count', ascending=False).head(10))\n\n# Filter to top 10 states by job count\ntop_10_states = bubble_data.sort_values('job_count', ascending=False).head(10)\n\n# Create the bubble chart\nfig = px.scatter(\n    top_10_states,\n    x='STATE_NAME',\n    y='job_count',\n    size='SALARY',\n    color='SALARY',\n    hover_name='STATE_NAME',\n    hover_data={\n        'SALARY': ':,.0f',\n        'job_count': True,\n        'STATE_NAME': False\n    },\n    labels={\n        'SALARY': 'Average Salary ($)',\n        'STATE_NAME': 'State',\n        'job_count': 'Number of Jobs (Excludes Null Salaries)'\n    },\n    title='Top 10 States for Analyst Jobs (Bubble Size = Salary)',\n    color_continuous_scale='Greens'\n)\n\n# Customize the layout\nfig.update_layout(\n    width=1200,\n    height=700,\n    xaxis_tickangle=-45,\n    showlegend=True\n)\n\n# Update traces for better bubble appearance\nfig.update_traces(\n    marker=dict(\n        sizemode='diameter',\n        sizeref=2.0*max(top_10_states['SALARY'])/(15.**2),\n        sizemin=4,\n        line=dict(width=1, color='white')\n    )\n)\n\nfig.show()"
  },
  {
    "objectID": "report.html",
    "href": "report.html",
    "title": "About",
    "section": "",
    "text": "title: âPreliminary Researchâ author: âGroup 2â date: June 26, 2025 format: html: bibliography: references.bib csl: csl/econometrica.csl toc: true â"
  },
  {
    "objectID": "report.html#purpose",
    "href": "report.html#purpose",
    "title": "About",
    "section": "Purpose",
    "text": "Purpose\nWith this in mind, our research will look at the rising trends within the job market, with a particular focus on roles that align with our own career interests. This includes analyst positions, some of which may be influenced by advances in AI. We will also look at what working conditions, more specifically remote versus not remote jobs."
  },
  {
    "objectID": "report.html#references",
    "href": "report.html#references",
    "title": "About",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "report.html#remote-job-distribution-by-general-industry",
    "href": "report.html#remote-job-distribution-by-general-industry",
    "title": "About",
    "section": "Remote Job Distribution by General Industry",
    "text": "Remote Job Distribution by General Industry\n\nIn terms of the Remote Job Distribution by general industry, the top 3 industries are Real Estate and Rental and Leasing, Information, Finance and Insurance. As we can see even the traditional industries such as manufacturing, educational services starte to have higher remote job proportion, which means the remote work nowadays has already became a widespread jobs or requirements across multiple industries.\n###Top Industries: Remote job Trends Over time\n\nTake a dive into the Top industries over time with specific industries, in general, this plot displays monthly trends in job postings for from May 2024 to September 2024. As we can see in here, the remote job ratio was represented by the y-axis, which is the proportion of job postings marked as remote out of total postings in each industry per month. Higher remote ratio, represents their have more remote job opportunities open in each industry.\nIndeed, we can see the general trend in this plot as the remote job among different industries, which reaches the peak in Auguest 2024 and then drop siginificantly in September, which suggests that the many companies end their summer recruitings. As we can see the real estate and rental industry goes from high and crash in September. In contrast, the Finance and Insurance industry has higher stabilityï¼ which has consistent job postings from May to September.\n From the above plot result, we can see that the remote job ratio trend started low in May, then continue drop low in June, then reached the peak in August (31%), after that drop low in September. In compare with the trend alighed with gray area: job postings are also have more postings in Auguest and less in September.\n From the above plot result, we can see that the remote job ratio trend started low in May, gradually increase in June, then reached the peak in July (33%), after that gradually drop low August. The job posting area remained high and stable through july, which is friendly for people who are seek for remote jobs.\n From the above plot result, we can see that the remote job ratio trend started high (43%) in May, drop in June, then rebound in July then reached the peak in August (53%), after that has a significantly drop in September(near 0). The job posting area remained comparable stable, which has high volumn in May and June, then has a siginificant drop in July and modest increase in Auguest and September. The differences between Job posting\n\nFrom above choreleograph, we can see that the green color depth represents the remote jobs proportions to their job postings, and with the average salary and average job experiences. To be more clearly, we can see that the Maine, Vermont, Alaska and Colorado have higher remote ratio, which represents their high demands on remote jobs souther state on the othe hand has lighter remote ratio, which represents that there are fewer remote job demands in these regionsâ industries.\n\nimport findspark\nfindspark.init()\n\nfrom pyspark.sql import SparkSession\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\nimport numpy as np\n\nnp.random.seed(423548)\n\npio.renderers.default = \"notebook\"\n\n# Initialize Spark Session\nspark = SparkSession.builder.appName(\"LightcastData\").getOrCreate()\n\n# Load Data\ndf = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").option(\"escape\", \"\\\"\").csv(\"./data/lightcast_job_postings.csv\")\n\n\ndf.printSchema() # comment this line when rendering the submission\ndf.show(5)\n\n\n# clean the data\n\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql.functions import when, col\n\ndf= df.drop(\"ID\" ,\"LAST_UPDATED_DATE\",\"LAST_UPDATED_TIMESTAMP\",\"DUPLICATES\", \"EXPIRED\",\"SOURCE_TYPES\",\"SOURCES\",\n            \"URL\", \"ACTIVE_URLS\", \"ACTIVE_SOURCES_INFO\",\"TITLE_RAW\",\"BODY\", \"MODELED_EXPIRED\", \"MODELED_DURATION\", \"COMPANY\", \n            \"COMPANY_NAME\", \"COMPANY_RAW\", \"COMPANY_IS_STAFFING\", \"EDUCATION_LEVELS\")\n\ndf = df.withColumn(\"REMOTE_TYPE_NAME\", \n                   when(col(\"REMOTE_TYPE_NAME\") == \"[None]\", \"Not Remote\")\n                   .otherwise(col(\"REMOTE_TYPE_NAME\")))\n\ndf_clean = df.na.drop(subset=[\n    \"salary\", \"MIN_YEARS_EXPERIENCE\", \"MAX_YEARS_EXPERIENCE\",\n    \"EDUCATION_LEVELS_NAME\", \"EMPLOYMENT_TYPE_NAME\", \"REMOTE_TYPE_NAME\",\n    \"DURATION\",\n])\n\ndf_clean.show(5)\n\n\ntitles = df_clean.select(\"TITLE_NAME\").distinct().rdd.flatMap(lambda x: x).collect()\nfor title in titles:\n    print(title)\n\n\n#gave chatgpt the list and told it to split into ai impacted and not\nai_impacted_jobs = [\n    'Trust Officers',\n    'Cloud Migration Engineers',\n    'EDI Developers',\n    'Permit Specialists',\n    'Data Integration Leads',\n    'Blockchain Developers',\n    'Lead Intelligence Analysts',\n    'Sales Architects',\n    'Business Intelligence Leads',\n    'Data Analyst Managers',\n    'Project Support Analysts',\n    'Sales Planning Analysts',\n    'Hourly Managers',\n    'Industrial Equipment Mechanics',\n    'Value Engineers',\n    'Enterprise Applications Consultants',\n    'SAP FICO Consultants',\n    'SAP ABAP Consultants',\n    'Oracle Erp Consultants',\n    'Member Liaisons',\n    'Data Solutions Analysts',\n    'Transformation Analysts',\n    'GIS Data Analysts',\n    'Japanese Bilingual Administrative Assistants',\n    'Project Leads',\n    'People Analytics Analysts',\n    'Human Resources Reporting Analysts',\n    'Implementation Consultants',\n    'Lead Enterprise Architects',\n    'Branch Bankers',\n    'Growth Marketing Analysts',\n    'Chargeback Analysts',\n    'Strategy Leads',\n    'Innovation Analysts',\n    'Business Insights Managers',\n    'CRM Business Analysts',\n    'Localization Producers',\n    'EDI Analysts',\n    'Scientific Data Analysts',\n    'Bilingual Japanese Customer Service Representatives',\n    'SQL/ETL Developers',\n    'Data Quality Leads',\n    'Data Visualization Analysts',\n    'Data Analytics Engineers',\n    'Foundation Administrators',\n    'SQL Reporting Analysts',\n    'Procurement Analysts',\n    'Manual Testers',\n    'Analytics Associates',\n    'Supply Chain Architects',\n    'SAP SD Analysts',\n    'Oracle Cloud Financials Consultants',\n    'Data Quality Assurance Analysts',\n    'Client Finance Directors',\n    'Population Health Analysts',\n    'Enterprise Solutions Consultants',\n    'Digital Product Analysts',\n    'Line Pilots',\n    'Processing Clerks',\n    'Client Solutions Strategists',\n    'Business Intelligence Specialists',\n    'Accounting Consultants',\n    'Business Intelligence Analysts',\n    'SAP CRM Consultants',\n    'Human Capital Management Consultants',\n    'IAM Architects',\n    'SAP Ariba Consultants',\n    'Territory Assistants',\n    'Immigration Analysts',\n    'Customer Experience Associates',\n    'Global Analysts',\n    'Analysts',\n    'Document Management Clerks',\n    'Data Reporting Analysts',\n    'Quality Analysts',\n    'SAP HANA Consultants',\n    'Site Analysts',\n    'Business Architects',\n    'Data Analytics Developers',\n    'Digital Solution Architect Managers',\n    'Information Governance Analysts',\n    'Business Coaches',\n    'Configuration Management Analysts',\n    'Commercial Analysts',\n    'Analytics Consultants',\n    'Buyers',\n    'Enterprise Solutions Architects',\n    'Remediation Analysts',\n    'Search Planners',\n    'Insurance Associates',\n    'People Operations Generalists',\n    'Appeals Specialists',\n    'IT Project Assistants',\n    'Data Analytics Architects',\n    'Methods Analysts',\n    'Liquidity Analysts',\n    'Functional Consultants',\n    'SAP HR Consultants',\n    'Performance Directors',\n    'ERP Implementation Specialists',\n    'Tribal Administrators',\n    'Study Design Leads',\n    'Equipment Analysts',\n    'Quality Assurance Monitors',\n    'Integration Team Leads',\n    'Invoice Analysts',\n    'SAP FICO Functional Consultants',\n    'Enterprise Relationship Managers',\n    'Enterprise Data Architects',\n    'SAP PP/QM Consultants',\n    'Management Analysts',\n    'Speech Assistants',\n    'SAP Business Analysts',\n    'Data Migration Analysts',\n    'SAP Techno Functional Consultants',\n    'SAP Supply Chain Consultants',\n    'Enterprise Directors',\n    'Data Processing Analysts',\n    'SAS Data Analysts',\n    'Manufacturing Consultants',\n    'Demand Analysts',\n    'Netsuite Principal Consultants',\n    'OTM Consultants',\n    'Modernization Superintendents',\n    'Mapping Analysts',\n    'Enterprise Systems Architects',\n    'Business Intelligence Associates',\n    'Oracle Business Analysts',\n    'SAP Support Analysts',\n    'Automation Engineers',\n    'Excel VBA Developers',\n    'Financial Data Analysts',\n    'Power Analysts',\n    'Sales Analysts',\n    'Lead IT Analysts',\n    'Development Analysts',\n    'Analytics Managers',\n    'Financial Applications Specialists',\n    'Sales Solution Architects',\n    'Survey Analysts',\n    'Data Analysts/Data Scientists',\n    'Quality Control Reviewers',\n    'Oracle EBS Consultants',\n    'Data Services Analysts',\n    'ERP Implementation Managers',\n    'Knowledge Analysts',\n    'Enterprise Business Analysts',\n    'Test Data Analysts',\n    'Techno Functional Analysts',\n    'Netsuite Consultants',\n    'Cryptologists',\n    'PMO Analysts',\n    'Reference Data Analysts',\n    'Clinical Data Analysts',\n    'Enterprise Business Architects',\n    'Enterprise Cloud Architects',\n    'SAP Security Consultants',\n    'Data Leads',\n    'Tools Developers',\n    'Marketing Analysts',\n    'Financial Planning and Analysis Analysts',\n    'Finance Systems Analysts',\n    'Production Operators',\n    'Oracle Functional Analysts',\n    'IT Buyers',\n    'Process Engineers',\n    'Privacy Analysts',\n    'Enterprise Resources Planning Managers',\n    'Real Estate Analysts',\n    'Systems Integration Architects',\n    'End User Computing Analysts',\n    'Data Analysts/Developers',\n    'Publishing Specialists',\n    'SQL Analysts',\n    'Account Analysts',\n    'Engineering Data Analysts',\n    'Oracle EBS Business Analysts',\n    'Directors of Business Intelligence',\n    'Reporting Associates',\n    'SAP HCM Consultants',\n    'Feasibility Managers',\n    'Data Management Administrators',\n    'Walkers',\n    'Production Analysts',\n    'HRIS Associates',\n    'Data Analytics Leads',\n    'Data Analytics Specialists',\n    'Data Security Analysts',\n    'Principal Data Scientists',\n    'Researchers',\n    'Procurement Business Analysts',\n    'Oracle Applications Analysts',\n    'Forecast Analysts',\n    'Supply Chain Data Analysts',\n    'Analytics and Insights Managers',\n    'MDM Developers',\n    'Business Support Analysts',\n    'Food and Beverage Analysts',\n    'Intelligence Research Analysts',\n    'Validation Leads',\n    'Associate Business Managers',\n    'Enterprise Data Analysts',\n    'IT Governance Analysts',\n    'Domain Architects',\n    'Compliance Business Analysts',\n    'Implementation Specialists',\n    'Placement Managers',\n    'Corporate Architects',\n    'Splunk Developers',\n    'Work Force Management Analysts',\n    'Banking Consultants',\n    'Data Stewards',\n    'SAP MDM Consultants',\n    'SAP Specialists',\n    'Data Validation Analysts',\n    'Business Intelligence Data Warehouse Architects',\n    'Data Science Associates',\n    'Solution Leads',\n    'SAP Data Analysts',\n    'SAP Finance Consultants',\n    'Doctors',\n    'SQL Data Analysts',\n    'Patient Revenue Cycle Specialists',\n    'People Analytics Managers',\n    'Data Scientists',\n    'Digital Data Analysts',\n    'Data Control Clerks',\n    'Storeroom Clerks',\n    'Finance Business Analysts',\n    'SAP HR Analysts',\n    'Business Intelligence and Analytics Managers',\n    'Brand Activation Managers',\n    'Enterprise Project Managers',\n    'Data Analytics Consultants',\n    'Programmer Analysts',\n    'Sales Data Analysts',\n    'Data Reviewers',\n    'Contract Analysts',\n    'Decision Support Analysts',\n    'Data Associates',\n    'E-Commerce Architects',\n    'Risk Control Managers',\n    'Debt Specialists',\n    'Risk and Controls Managers',\n    'Data Entry Analysts',\n    'Platform Analysts',\n    'Financial Systems Analysts',\n    'Claims Resolution Analysts',\n    'Lead Business Intelligence Analysts',\n    'Inside Auto Claims Representatives',\n    'Customer Contact Center Managers',\n    'Data Governance Analysts',\n    'Business Operations Specialists',\n    'SAP Technical Consultants',\n    'Data Engineering Managers',\n    'SAP EWM Consultants',\n    'Tax Controllers',\n    'Transmission Analysts',\n    'Business Analysts',\n    'Credit Analysts',\n    'CSV Consultants',\n    'Patient Services Associates',\n    'Research Data Analysts',\n    'Medical Economics Analysts',\n    'SQL Administrators',\n    'SAP Master Data Analysts',\n    'Customer Care Analysts',\n    'Title Processors',\n    'Power Business Intelligence Developers',\n    'Data Quality Analysts',\n    'Quality Assurance Analysts',\n    'Consulting Engagement Managers',\n    'Vice Presidents of Operational Excellence',\n    'Insights Analysts',\n    'Inventory Analysts',\n    'Data Support Analysts',\n    'SAP Business One Consultants',\n    'Data Management Leads',\n    'Platform Administrators',\n    'Oracle Financial Functional Consultants',\n    'Architecture Managers',\n    'ERP Analysts',\n    'Growth Associates',\n    'Directors of School Nutrition',\n    'Operations and Policy Analysts',\n    'Lead Data Analysts',\n    'SAP SD Functional Consultants',\n    'Assistant Federal Security Directors',\n    'Oracle Cloud Architects',\n    'Quality Assurance Leads',\n    'SAP FICO Analysts',\n    'Repair Analysts',\n    'SAP SD Consultants',\n    'SAP SRM Consultants',\n    'Hyperion Consultants',\n    'SAP Systems Analysts',\n    'Adobe Experience Manager Architects',\n    'Quality Assurance Analysts/Engineers',\n    'Oracle Utilities Professional Services CC&B Implementation Consultants',\n    'SAP OTC Consultants',\n    'Data Warehouse Business Analysts',\n    'Client Data Analysts',\n    'Documentum Administrators',\n    'Project Analysts',\n    'Korean Linguists',\n    'Data Governance Managers',\n    'Oracle Consultants',\n    'Navigators',\n    'Customer Data Analysts',\n    'Enterprise Technical Architects',\n    'Cash Management Associates',\n    'Equity Traders',\n    'Financial Analysts',\n    'ERP Architects',\n    'Business Intelligence and Data Analysts',\n    'Automation Consultants',\n    'Governance Analysts',\n    'ERP Systems Analysts',\n    'Commercial Banking Credit Analysts',\n    'Certified Public Accountants',\n    'Data Operations Analysts',\n    'Category Analysts',\n    'Regional Business Managers',\n    'GCP Auditors',\n    'Architectural Project Coordinators',\n    'Delivery Analysts',\n    'SAP MM Functional Consultants',\n    'Techno Functional Leads',\n    'Clinical Data Managers',\n    'Administrators',\n    'Enterprise Network Managers',\n    'Healthcare Business Analysts',\n    'Solutions Architects',\n    'Ward Clerks',\n    'Account Coordinators',\n    'Data Abstractors',\n    'Data Scientists/Analysts',\n    'SAP Functional Consultants',\n    'Product Lifecycle Managers',\n    'Data Intelligence Analysts',\n    'Data Warehouse Analysts',\n    'CNC Administrators',\n    'Life Sciences Consultants',\n    'Data Processing Clerks',\n    'Production Planning Analysts',\n    'Business Growth Consultants',\n    'Chief Architects',\n    'Logistics Administrators',\n    'Vendor Analysts',\n    'Quantitative Analytics Managers',\n    'Treasury Consultants',\n    'Crime Analysts',\n    'SAP Business Process Analysts',\n    'Call Center Analysts',\n    'GRC Analysts',\n    'Competitive Intelligence Analysts',\n    'Oracle Technical Consultants',\n    'Headend Engineers',\n    'Claims Business Analysts',\n    'General Managers',\n    'Systems Architecture Engineers',\n    'SAP Ariba Managers',\n    'Customer Analytics Managers',\n    'Customer Experience Analysts',\n    'LIS Analysts',\n    'SAP Consultants',\n    'SAP Hybris Consultants',\n    'SAP PI Consultants',\n    'Enterprise Application Analysts',\n    'Data and Analytics Consultants',\n    'Agile Product Owners',\n    'Business Intelligence Reporting Analysts',\n    'Management Associates',\n    'Health Data Analysts',\n    'Enterprise Systems Engineers',\n    'Lead Analysts',\n    'Data Migration Consultants',\n    'Professors of Biological Sciences',\n    'Oracle Business Systems Analysts',\n    'Regional Fleet Managers',\n    'Enterprise Systems Analysts',\n    'Inflight Supervisors',\n    'Operations Process Engineers',\n    'SAP Developers',\n    'Customer Care Experts',\n    'Reporting Analysts',\n    'Commercial Excellence Managers',\n    'Lead Quality Analysts',\n    'Oracle EBS Technical Consultants',\n    'Records Assistants',\n    'Directors of Cloud Security',\n    '.NET Technical Architects',\n    'Associates',\n    'Trend Analysts',\n    'Digital Analysts',\n    'Innovation Specialists',\n    'Data Governance Leads',\n    'Data Analysts',\n    'Directors of Sponsored Programs',\n    'Enterprise Resource Planning Managers',\n    'Wealth Management Analysts',\n    'Quantitative Data Analysts',\n    'Origination Associates',\n    'Enterprise Business Consultants',\n    'SAP Application Consultants',\n    'Information Analysts',\n    'Process Improvement Analysts',\n    'Consumer Analysts',\n    'Communications Analysts',\n    'GIS Consultants',\n    'Data Analyst Specialists',\n    'Plant Maintenance Managers',\n    'Netsuite Functional Consultants',\n    'Plant Chemists',\n    'Integration Consultants',\n    'Data Management Analysts',\n    'Oracle Financial Consultants',\n    'SAP Plant Maintenance Consultants',\n    'Analytics Leads',\n    'Lead Level Designers',\n    'Solutions Analysts',\n    'Data and Reporting Analysts',\n    'Data Analytics Product Managers',\n    'SAP Analysts',\n    'Implementation Analysts',\n    'Revenue Cycle Analysts',\n    'Case Management Associates',\n    'SAP Successfactors Consultants',\n    'IT Governance Managers',\n    'Sales Professionals',\n    'Principal Architects',\n    'Oracle HCM Consultants',\n    'Data Science Analysts',\n    'HANA Consultants',\n    'OSP Managers',\n    'Interface Analysts',\n    'Transportation Systems Analysts',\n    'Forms Analysts',\n    'Master Data Coordinators',\n    'Enterprise Application Architects',\n    'Epic Security Analysts',\n    'Intercompany Accountants',\n    'Client Insights Analysts',\n    'Data Analytics Associates',\n    'SAP SD/MM Consultants',\n    'SAP Administrators',\n    'Supply Chain Business Managers',\n    'Assistant Service Experience Managers',\n    'Data Clerks',\n    'Data Visualization Specialists',\n    'Growth Specialists',\n    'SAP Basis Leads',\n    'Functional Analysts',\n    'SAP Functional Analysts',\n    'Knowledge Experts',\n    'Artificial Intelligence Engineers',\n    'Financial Planning and Analysis Managers',\n    'Business Intelligence Data Analysts',\n    'Peoplesoft Business Analysts',\n    'Platform Architects',\n    'Systems Architects',\n    'Assessment Analysts',\n    'Solutions Consultants',\n    'Surface Designers',\n    'SAP GTS Consultants',\n    'Data Insights Analysts',\n    'Oracle Cloud Technical Consultants',\n    'Data Modelers',\n    'Distribution Consultants',\n    'ERP Administrators',\n    'SAP MM Consultants',\n    'ERP Consultants',\n    'Financial Planning Analysts',\n    'Intelligence Analysts',\n    'SAP Basis Consultants',\n    'Consulting Technical Managers',\n    'Insights and Reporting Analysts',\n    'Advanced Analytics Analysts',\n    'Medical Data Analysts',\n    'Solution Architects/Principal Consultants',\n    'Technical Architects',\n    'Data and Analytics Managers',\n    'Travel and Expense Analysts',\n    'OFSAA Business Analysts',\n    'Facilities HVAC Technicians',\n    'Data Analytics Analysts',\n    'Web Data Analysts',\n    'Staff Data Engineers',\n    'Program Data Analysts',\n    'Associate Consultants',\n    'Safety Data Analysts',\n    'Controllers',\n    'CAD Operators',\n    'Security Analysts',\n    'Oracle Finance Functional Consultants',\n    'Enterprise Architects',\n    'Finance Data Analysts',\n    'ERP Business Systems Analysts',\n    'IT Data Analysts',\n    'Business Analysis Interns',\n    'Information Services Analysts',\n    'Data Solutions Consultants',\n    'Enterprise Program Managers',\n    'SAP MM Analysts',\n    'IT Applications Specialists',\n    'Oracle EBS Functional Consultants',\n    'Research and Evaluation Analysts',\n    'Customer Support Leads',\n    'Oracle SCM Functional Consultants',\n    'IT Data Analytics Analysts',\n    'Finance Assistants',\n    'Business Operations Associates',\n    'Quality Assurance Automation Testers',\n    'Technical Project Managers',\n    'Data Science and Analytics Managers',\n    'Oracle Cloud HCM Consultants',\n    'Business Intelligence Managers',\n    'Customer Support Administrators',\n    'Procurement Operations Associates',\n    'Analytics and Reporting Analysts',\n    'Research Specialists',\n    'BSA Consultants',\n    'Oracle Systems Analysts',\n    'Bilingual Office Managers',\n    'Institutional Sales Analysts',\n    'Consulting Solutions Architects',\n    'Corporate Finance Associates',\n    'Immunology Specialists',\n    'Global Trade Analysts',\n    'Search Coordinators',\n    'Higher Education Consultants',\n    'Enterprise Risk Analysts',\n    'ERP Support Specialists',\n    'Spanish and English Teachers',\n    'Assistant Vice Presidents',\n    'Data Protection Specialists',\n    'Manufacturing Services Managers',\n    'Cash Managers',\n    'Pricing Data Analysts',\n    'Directors of Toxicology',\n    'Data Acquisition Analysts',\n    'Process Analysts',\n    'Data Technicians',\n    'Clinical Quality Analysts',\n    'ERP Specialists',\n    'IAM Analysts',\n    'Data Infrastructure Engineers',\n    'Industry Analysts',\n    'Oracle Database Developers',\n    'Platform Support Specialists',\n    'School Education Managers',\n    'Clinical Data Associates',\n    'Oracle Functional Consultants',\n    'Enterprise Analysts',\n    'Manufacturing Analysts',\n    'Targeting Analysts',\n    'Master Data Analysts',\n    'SAP Basis Administrators',\n    'Data Center Analysts',\n    'Business Assistants',\n    'Philanthropy Assistants',\n    'IT Analysts',\n    'Middle Office Analysts',\n    'Investment Data Analysts',\n    'Salesforce Consultants'\n]\n\n\nfrom pyspark.sql.functions import when, col, lit, avg, count\n\ndf_final = df_clean.withColumn(\n    'ai_impacted',\n    when(col('title_name').isin(ai_impacted_jobs), lit('yes')).otherwise(lit('no'))\n)\n\navg_salary = df_final.groupBy(\"state_name\").agg(avg(\"salary\").alias(\"avg_salary\"))\n\n\ndf_final = df_final.join(avg_salary, on=\"state_name\", how=\"left\")\n\ncount = df_final.groupBy(\"state_name\").agg(count(\"*\").alias(\"count\"))\n\n\ndf_final = df_final.join(count, on=\"state_name\", how=\"left\")\n\ndf_final.select(\"count\").distinct().show()\n\n\ndf_final_pd = df_final.toPandas()\n\nfig = px.scatter(\n    df_final_pd,  \n    x=\"STATE_NAME\",                       \n    y=\"avg_salary\",                 \n    size=\"count\",                 \n    color=\"ai_impacted\",                  \n    title=\"Average Salary by State and AI Impact\",\n    size_max=60,\n    color_discrete_map={\n        \"yes\": \"#2dbf78\",   \n        \"no\": \"#8adfbd\"  \n    }\n\n)\n\nfig.update_layout(\n    legend_title_text=\"AI Impacted\",\n    xaxis_title=\"State\",\n    yaxis_title=\"Average Salary\",\n    xaxis=dict(tickangle=45),\n    yaxis=dict(categoryorder='category descending')\n)\n\nfig.show()\n\n\n\n\nimport findspark\nfindspark.init()\n\nfrom pyspark.sql import SparkSession\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\nimport numpy as np\n\nnp.random.seed(42)\n\npio.renderers.default = \"notebook\"\n\n# Initialize Spark Session\nspark = SparkSession.builder.appName(\"LightcastData\").getOrCreate()\n\n# Load Data\ndf = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").option(\"escape\", \"\\\"\").csv(\"./data/lightcast_job_postings.csv\")\n\n# Show Schema and Sample Data\nprint(\"---This is Diagnostic check, No need to print it in the final doc---\")\n\ndf.printSchema() # comment this line when rendering the submission\ndf.show(5)\n\n\nFigureÂ 1\n\n\n\n\nimport pandas as pd\ndf = pd.read_csv(\"./data/lightcast_job_postings.csv\")\n\n\nimport pandas as pd\n\ndf = pd.read_csv(\"./data/lightcast_job_postings.csv\")\n\ncolumns_to_drop = [\n    \"ID\", \"URL\", \"ACTIVE_URLS\", \"DUPLICATES\", \"LAST_UPDATED_TIMESTAMP\",\n    \"NAICS2\", \"NAICS3\", \"NAICS4\", \"NAICS5\", \"NAICS6\",\n    \"SOC_2\", \"SOC_3\", \"SOC_5\"\n]\n\ndf.drop(columns=columns_to_drop, inplace=True)\n\n\nprint(df.columns.tolist())\n\n\n!pip install missingno\n\n\nimport missingno as msno\nimport matplotlib.pyplot as plt\n# Visualize missing values\nmsno.heatmap(df)\nplt.title(\"Missing Values Heatmap\")\nplt.show()\n\n# Drop columns with &gt;50% missing values\ndf.dropna(thresh=len(df) * 0.5, axis=1, inplace=True)\n\n# Fill only the columns you actually have\nif 'Industry' in df.columns:\n    df[\"Industry\"].fillna(\"Unknown\", inplace=True)\n    df[\"Salary\"].fillna(df[\"Salary\"].median(), inplace=True)\n\n\ndf = df.drop_duplicates(subset=[\"TITLE\", \"COMPANY\", \"LOCATION\", \"POSTED\"], keep=\"first\")\n\n\ndf = df[df['NAICS_2022_2_NAME'] != 'Unclassified Industry']\n\n\ndf['REMOTE_TYPE_NAME'] = df['REMOTE_TYPE_NAME'].replace('[None]', 'Not Remote')\n\n\nimport pandas as pd\nimport plotly.express as px\n\n# Step 1: Prepare data\ndata = {\n    'Industry': [\n        'Wholesale Trade', 'Retail Trade', 'Real Estate and Rental and Leasing',\n        'Professional, Scientific, and Technical Services', 'Manufacturing',\n        'Information', 'Health Care and Social Assistance',\n        'Finance and Insurance', 'Educational Services',\n        'Administrative and Support and Waste Management and Remediation Services'\n    ],\n    'Flexible Remote': [87.8, 94.4, 97.6, 92.2, 89.7, 95.8, 92.1, 94.8, 89.0, 94.8],\n    'Onsite': [12.2, 5.6, 2.4, 7.8, 10.3, 4.2, 7.9, 5.2, 11.0, 5.2]\n}\n\ndf = pd.DataFrame(data)\n\n# Step 2: Sort in ascending order of Flexible Remote\ndf_sorted = df.sort_values(by='Flexible Remote', ascending=True)\ndf_sorted['Industry'] = pd.Categorical(df_sorted['Industry'], categories=df_sorted['Industry'], ordered=True)\n\n# Step 3: Melt data for stacked bar format\ndf_melted = df_sorted.melt(\n    id_vars='Industry',\n    value_vars=['Flexible Remote', 'Onsite'],\n    var_name='Remote Type',\n    value_name='Percentage'\n)\n\n# Step 4: Plot\nfig = px.bar(\n    df_melted,\n    x='Percentage',\n    y='Industry',\n    color='Remote Type',\n    orientation='h',\n    text='Percentage',\n    color_discrete_map={\n        'Flexible Remote': '#1aab89',\n        'Onsite': '#88d4c3'\n    },\n    title=\"Remote Job Distribution by Industry (Top 10 Industries)\"\n)\n\n# Step 5: Layout adjustments\nfig.update_layout(\n    xaxis_title=\"Percentage of Jobs\",\n    yaxis_title=\"\",\n    xaxis=dict(tickformat=\".0f\"),\n    legend_title=\"Remote Type\",\n    barmode='stack',\n    margin=dict(l=10, r=10, t=60, b=40),\n    height=500\n)\n\n# Step 6: Label formatting\nfig.update_traces(texttemplate='%{text:.1f}%', textposition='inside')\n\n# Save plot\nfig.write_html(\"./figures/top_industries.html\")\n\n\n# Show plot\nfig.show()\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n#read files\nfile_path = \"./data/lightcast_job_postings.csv\"\ndf = pd.read_csv(file_path)\n\n\nprint(df['REMOTE_TYPE_NAME'].value_counts(dropna=False).head(10))\n\n\n# Step 1: Standardize formatting\ndf['REMOTE_TYPE_NAME'] = (\n    df['REMOTE_TYPE_NAME']\n    .astype(str)\n    .str.strip()\n    .str.title()\n    .replace({'None': pd.NA, 'Nan': pd.NA})\n)\n\n\n# Step 2: Fill missing or ambiguous entries with 'Not Remote'\ndf['REMOTE_TYPE_NAME'] = df['REMOTE_TYPE_NAME'].fillna('Not Remote')\ndf.loc[df['REMOTE_TYPE_NAME'] == \"[None]\", 'REMOTE_TYPE_NAME'] = \"Not Remote\"\nprint(df['REMOTE_TYPE_NAME'].value_counts(dropna=False).head(10))\n\n\n# Convert all values to strings and strip whitespace\ndf['REMOTE_TYPE_NAME'] = df['REMOTE_TYPE_NAME'].astype(str).str.strip()\n\n\n# Apply new classification logic\ndf['REMOTE_BINARY'] = df['REMOTE_TYPE_NAME'].apply(\n    lambda x: 1 if x in ['Remote', 'Hybrid Remote'] else 0\n)\n\n\nprint(df['REMOTE_TYPE_NAME'].value_counts())\nprint(\"\\nBinary classification:\")\nprint(df['REMOTE_BINARY'].value_counts())\n\n\nimport pandas as pd\nimport numpy as np\n\n# Ensure salary columns are numeric and handle missing values\ndf['SALARY_FROM'] = pd.to_numeric(df['SALARY_FROM'], errors='coerce').replace(0, np.nan)\ndf['SALARY_TO'] = pd.to_numeric(df['SALARY_TO'], errors='coerce').replace(0, np.nan)\n\n# Calculate average salary (mean of SALARY_FROM and SALARY_TO)\ndf['AVERAGE_SALARY'] = df[['SALARY_FROM', 'SALARY_TO']].mean(axis=1)\n\n# Drop rows with missing values in AVERAGE_SALARY, REMOTE_TYPE_NAME, or STATE_NAME\ndf_salary = df.dropna(subset=['AVERAGE_SALARY', 'REMOTE_TYPE_NAME', 'STATE_NAME'])\n\n# Group by state and remote type, then calculate average salary\navg_salary_by_state_remote = df_salary.groupby(['STATE_NAME', 'REMOTE_TYPE_NAME'])['AVERAGE_SALARY'].mean().reset_index()\n\n# Round the results for easier reading\navg_salary_by_state_remote['AVERAGE_SALARY'] = avg_salary_by_state_remote['AVERAGE_SALARY'].round(2)\n\n# Show results\nprint(avg_salary_by_state_remote)\n\n\ndf = df.merge(avg_salary_by_state_remote,\n              on=['STATE_NAME', 'REMOTE_TYPE_NAME'],\n              how='left')\n\n\ndf = df.merge(\n    avg_salary_by_state_remote,\n    on=['STATE_NAME', 'REMOTE_TYPE_NAME'],\n    how='left',\n    suffixes=('', '_STATE_REMOTE_AVG')\n)\n\n\n[col for col in df.columns if 'AVG' in col or 'SALARY' in col]\n\n\ndf = df.rename(columns={'AVERAGE_SALARY_y': 'AVERAGE_SALARY_STATE_REMOTE_AVG'})\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import (accuracy_score, f1_score, confusion_matrix,\n                             classification_report, precision_score,\n                             recall_score, balanced_accuracy_score)\nfrom sklearn.inspection import permutation_importance\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Remove duplicate column names across full dataframe\ndf = df.loc[:, ~df.columns.duplicated()]\n\n\ndf['AVG_YEARS_EXPERIENCE'] = (df['MIN_YEARS_EXPERIENCE'] + df['MAX_YEARS_EXPERIENCE']) / 2\ndf['EXP_SPREAD'] = df['MAX_YEARS_EXPERIENCE'] - df['MIN_YEARS_EXPERIENCE']\n\n\ndf = df.drop(columns=['MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE'])\n\n\nnum_feats = [\n    'AVG_YEARS_EXPERIENCE',\n    'AVERAGE_SALARY_STATE_REMOTE_AVG',\n    'IS_INTERNSHIP'\n]\n\ncat_feats = [\n    'STATE_NAME',\n    'NAICS_2022_2_NAME',\n    'EDUCATION_LEVELS_NAME',\n    'COMMON_SKILLS_NAME',\n    'SOFTWARE_SKILLS_NAME',\n    'TITLE_CLEAN'\n    \n]\n\nX = df[num_feats + cat_feats]\ny = df['REMOTE_BINARY']\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\n\n\npreprocess = ColumnTransformer(transformers=[\n    (\"num\", StandardScaler(), num_feats),\n    (\"cat\", OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_feats)\n])\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n# Preprocessing step\npreprocess = ColumnTransformer(transformers=[\n    (\"num\", StandardScaler(), num_feats),\n    (\"cat\", OneHotEncoder(handle_unknown='ignore', max_categories=500, sparse_output=False), cat_feats)\n])\nclf = RandomForestClassifier(random_state=42, class_weight='balanced')\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\n\nrf = RandomForestClassifier(\n    n_estimators=200,\n    max_depth=None,\n    class_weight='balanced',\n    random_state=42,\n    n_jobs=-1\n)\n\npipe = Pipeline(steps=[\n    ('prep', preprocess),\n    ('model', rf)\n])\n\n\npipe.fit(X_train, y_train)\n\n\ny_pred = pipe.predict(X_test)\n\n# Classification report and confusion matrix\nprint(classification_report(y_test, y_pred))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\n\n# Additional custom metrics\nprint(\"\\nCustom Metrics:\")\nprint(\"Accuracy:\", round(accuracy_score(y_test, y_pred), 3))\nprint(\"F1 Score:\", round(f1_score(y_test, y_pred), 3))\nprint(\"Precision:\", round(precision_score(y_test, y_pred), 3))\nprint(\"Sensitivity (Recall 1):\", round(recall_score(y_test, y_pred), 3))\nprint(\"Specificity (Recall 0):\", round(\n    recall_score(y_test, y_pred, pos_label=0), 3))\nprint(\"Balanced Accuracy:\", round(balanced_accuracy_score(y_test, y_pred), 3))\n\n\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n\n\nrf_model       = pipe.named_steps[\"model\"]          # RandomForestClassifier\nfeature_names  = pipe.named_steps[\"prep\"].get_feature_names_out()\n\nimportances = rf_model.feature_importances_\n\nfeat_imp = (\n    pd.DataFrame({\"Feature\": feature_names, \"Importance\": importances})\n      .sort_values(by=\"Importance\", ascending=False)\n      .reset_index(drop=True)\n)\n\nprint(\"\\nTop 9 â Tree-based Importances\")\nprint(feat_imp.head(9).to_string(index=False))\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ntop_n = 9                     # change to show more/less\nplt.figure(figsize=(8, 6))\nsns.barplot(\n    data=feat_imp.head(top_n),\n    x=\"Importance\", y=\"Feature\",\n    palette=\"crest\"\n)\nplt.title(f\"Top {top_n} Feature Importances (Random Forest)\")\nplt.xlabel(\"Mean Decrease in Impurity\")\nplt.ylabel(\"\")\nplt.tight_layout()\nplt.show()\n\n\nimport pandas as pd\nimport plotly.express as px\n\n# Step 1: Create state abbreviation mapping\nus_state_abbrev = {\n    'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR',\n    'California': 'CA', 'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE',\n    'Florida': 'FL', 'Georgia': 'GA', 'Hawaii': 'HI', 'Idaho': 'ID',\n    'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA', 'Kansas': 'KS',\n    'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD',\n    'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS',\n    'Missouri': 'MO', 'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV',\n    'New Hampshire': 'NH', 'New Jersey': 'NJ', 'New Mexico': 'NM', 'New York': 'NY',\n    'North Carolina': 'NC', 'North Dakota': 'ND', 'Ohio': 'OH', 'Oklahoma': 'OK',\n    'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI', 'South Carolina': 'SC',\n    'South Dakota': 'SD', 'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT',\n    'Vermont': 'VT', 'Virginia': 'VA', 'Washington': 'WA', 'West Virginia': 'WV',\n    'Wisconsin': 'WI', 'Wyoming': 'WY', 'District of Columbia': 'DC'\n}\n\n# Step 2: Map state names to abbreviations\ndf['STATE_ABBR'] = df['STATE_NAME'].map(us_state_abbrev)\n\n# Step 3: Group by state and compute metrics\nchoropleth_data = df.groupby('STATE_ABBR').agg(\n    remote_ratio=('REMOTE_BINARY', 'mean'),\n    avg_salary=('AVERAGE_SALARY_STATE_REMOTE_AVG', 'mean'),\n    avg_experience=('AVG_YEARS_EXPERIENCE', 'mean'),\n    job_count=('STATE_NAME', 'count')\n).reset_index()\n\n# Step 4: Define custom green scale (start from light, move to #1aab89)\ncustom_green_scale = [\n    [0, \"#e0f7f1\"],     # light mint\n    [0.5, \"#70d8b5\"],   # mid-green\n    [1, \"#1aab89\"]      # deep teal green\n]\n\n# Step 5: Create the choropleth with custom green\nfig = px.choropleth(\n    data_frame=choropleth_data,\n    locations='STATE_ABBR',\n    locationmode=\"USA-states\",\n    color='remote_ratio',\n    color_continuous_scale=custom_green_scale,\n    scope=\"usa\",\n    labels={'remote_ratio': 'Remote Job Ratio'},\n    hover_data={\n        'remote_ratio': ':.2f',\n        'avg_salary': ':.0f',\n        'avg_experience': ':.1f',\n        'job_count': True\n    },\n    title='Remote Job Ratio by State (Custom Green), Avg Salary & Experience in Hover'\n)\n\nfig.update_layout(margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0})\nfig.write_html(\"./figures/state_remote_job_ratio.html\")\n\nfig.show()\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Step 1: Ensure 'POSTED' is in datetime format and create Year-Month\ndf['POSTED'] = pd.to_datetime(df['POSTED'])\ndf['POSTED_YM'] = df['POSTED'].dt.to_period('M').astype(str)\n\n\nindustry_trends = (\n    df.groupby(['NAICS_2022_2_NAME', 'POSTED_YM'])['REMOTE_BINARY']\n    .mean()\n    .reset_index(name='REMOTE_RATIO')\n)\n\n\n# Step 3: Select top 5 industries with highest overall average remote ratio\ntop_industries = (\n    industry_trends.groupby('NAICS_2022_2_NAME')['REMOTE_RATIO']\n    .mean()\n    .sort_values(ascending=False)\n    .head(5)\n    .index.tolist()\n)\nfiltered_trends = industry_trends[industry_trends['NAICS_2022_2_NAME'].isin(top_industries)]\n\n\nimport plotly.express as px\n\nfig = px.line(\n    filtered_trends,\n    x='POSTED_YM',\n    y='REMOTE_RATIO',\n    color='NAICS_2022_2_NAME',\n    markers=True,\n    title=\"Top Industries: Remote Job Trends Over Time\"\n)\n\nfig.update_layout(\n    xaxis_title=\"Posted Month\",\n    yaxis_title=\"Remote Job Ratio\",\n    legend_title=\"Industry\",\n    legend=dict(x=1.02, y=1, bordercolor=\"Black\"),\n    margin=dict(l=40, r=40, t=60, b=40),\n    width=1000,\n    height=500\n)\n\nfig.update_xaxes(tickangle=45)\nfig.write_html(\"./figures/remote_job_over_time.html\")\nfig.show()\n\n\n#Groupby industry + month and calculate both:\nindustry_month_stats = df.groupby(['NAICS_2022_2_NAME', 'POSTED_YM']).agg(\n    TOTAL_JOBS=('REMOTE_BINARY', 'count'),\n    REMOTE_RATIO=('REMOTE_BINARY', 'mean')\n).reset_index()\n\n\njob_count = df.groupby(['NAICS_2022_2_NAME', 'POSTED_YM']).size().reset_index(name='JOB_COUNT')\n\n\nremote_ratio = df.groupby(['NAICS_2022_2_NAME', 'POSTED_YM'])['REMOTE_BINARY'].mean().reset_index(name='REMOTE_RATIO')\n\n\nindustry_month_stats = pd.merge(remote_ratio, job_count, on=['NAICS_2022_2_NAME', 'POSTED_YM'])\n\n\nimport matplotlib.pyplot as plt\n\n# Choose 2â3 industries to plot (or loop one at a time)\nselected_industries = [\n    'Administrative and Support and Waste Management and Remediation Services',\n    'Arts, Entertainment, and Recreation',\n    'Finance and Insurance',\n    'Real Estate and Rental and Leasing',\n    'Utilities'\n]\n\n\n\nfor industry in selected_industries:\n    data = industry_month_stats[industry_month_stats['NAICS_2022_2_NAME'] == industry]  \n\n    fig, ax1 = plt.subplots(figsize=(10, 4))\n\n    # Plot remote ratio\n    ax1.plot(data['POSTED_YM'], data['REMOTE_RATIO'], color='tab:blue', marker='o')\n    ax1.set_xlabel('Month')\n    ax1.set_ylabel('Remote Job Ratio', color='tab:blue')\n    ax1.tick_params(axis='y', labelcolor='tab:blue')\n    ax1.set_title(f\"Remote Job Ratio & Volume Over Time: {industry}\")\n\n    # Plot job count on secondary y-axis\n    ax2 = ax1.twinx()\n    ax2.bar(data['POSTED_YM'], data['JOB_COUNT'], color='tab:gray', alpha=0.3)\n    ax2.set_ylabel('Total Job Postings', color='gray')\n    ax2.tick_params(axis='y', labelcolor='gray')\n\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.savefig(\"./figures/Remote_jobs_\"+str(industry)+\".jpg\", dpi=300)\n    plt.show()\n\n\nRandom Forest results\n  As the two plots displayed a clear results for random forest results, our group has set our target variable for predict peopleâs preferences on Remote versus Onsite job, where 1 represents a remote job and 0 represents onsite. We also had a set of independent variables, including as: âAVG_YEARS_EXPERIENCEâ, âAVERAGE_SALARY_STATE_REMOTE_AVGâ, âIS_INTERNSHIPâ, âSTATE_NAMEâ, âNAICS_2022_2_NAMEâ(industry) âEDUCATION_LEVELS_NAMEâ, âCOMMON_SKILLS_NAMEâ, âSOFTWARE_SKILLS_NAMEâ, âTITLE_CLEANâ (occupation). Meanwhile, we split the data into training and testing sets using an 80/20 ratio to ensure generalizability, This means 20% of the data will go into the test set, and 80% will go into the training set. Then we conduct the randam forest model analysis. According to plots, we can conclude the accuracy reached to 94.6%;F1 score as 84.7%, which reflects the robust balance between precision and recall; the precision as 99.4%, which means it has highly accurate rate on predict the results;the sensitivity for class1 as 73.8%, which means it correctly identified the 74% people who pick remote; the sensitivity for class 0 even reached to 99.9%, which means almost all the people who choose non-remote job has correctly classified; balanced accuracy as 86.8%, which represents there is a balance performance between both cases. From the confusion matrix, there has a detailed display, which represents the model correctly predicts 11536 people who choose onsite jobs with only 13 false positives, and it also orrectly predicts 2177 people who choose remote jobs with 774 missed results.\nOne major limitation is class imbalance. Remote jobs (class 1) are the minority, which leads the model to perform less effectively on them.To counter this, we used class_weight=âbalancedâ in our Random Forest to give more weight to underrepresented classes. We also built a preprocessing pipeline using ColumnTransformer for encoding categorical and scaling numerical features.Still, further optimization is needed. we recommended three ways to overcome this issue in the future study: Adjusting sample sizes;Tuning model hyperparameters, like n_estimators and max_categories; experimenting with resampling techniques; add crossvalidation steps to void overfitting and ensure the final resultsâ accuracy.\n\nFeatured Importance\n Based on the above plot, we can see the top 9 features could be essential in predicting the peoplesâprefrences on remote or onsite jobs, we can see the top three essential features are average remote salary by state, Average years of experience and location state-California. These three features can be easily interpreted as the jobâs salary, jobâs requirement for year experiences and location are vital elements that impact peopleâs decision on onsite or remote job types, peple prioritize jobs with high salary and better geographic location\n\nimport findspark\nfindspark.init()\n\nfrom pyspark.sql import SparkSession\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\nimport numpy as np\n\nnp.random.seed(42)\n\npio.renderers.default = \"notebook\"\n\n# Initialize Spark Session\nspark = SparkSession.builder.appName(\"LightcastData\").getOrCreate()\n\n# Load Data\ndf = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").option(\"escape\", \"\\\"\").csv(\"./data/lightcast_job_postings.csv\")\n\n\ndf.createOrReplaceTempView(\"jobs\")\n\n\nsoftware_skill_counts_by_type = spark.sql(\"\"\"\n    SELECT software_skills_name, COUNT(*) AS count\n    FROM jobs\n    WHERE LOWER(title_name) LIKE '%analyst%'\n       OR LOWER(title_name) LIKE '%analysis%'\n       OR LOWER(title_name) LIKE '%analytics%'\n    GROUP BY software_skills_name\n    ORDER BY count DESC\n    LIMIT 10\n\"\"\")\nsoftware_skill_counts_by_type.show(truncate=False)\n\n\nskill_counts_by_type = spark.sql(\"\"\"\n    SELECT skills_name, COUNT(*) AS count\n    FROM jobs\n    WHERE LOWER(title_name) LIKE '%analyst%'\n    OR LOWER(title_name) LIKE '%analysis%'\n    OR LOWER(title_name) LIKE '%analytics%'\n    GROUP BY skills_name\n    ORDER BY count DESC\n    LIMIT 10\n\"\"\")\nskill_counts_by_type.show(truncate=False)\n\n\nimport pandas as pd\n\nskills_data = {\n    \"Name\": [\"Alyssa\", \"Adam\", \"Yihan\"],\n    \"Microsoft Office\": [4, 5, 3],\n    \"Dashboard\": [3, 3, 1],\n    \"SQL\": [2, 2, 2],\n    \"OneStream\": [1, 1, 1],\n    \"Cloud Computing\": [2, 2, 2]\n}\n\ndf_skills = pd.DataFrame(skills_data)\ndf_skills.set_index(\"Name\", inplace=True)\ndf_skills\n\n\nimport pandas as pd\nimport plotly.graph_objects as go\n\n# Your data\nskills_data = {\n    \"Name\": [\"Alyssa\", \"Adam\", \"Yihan\"],\n    \"Microsoft Office\": [4, 5, 3],\n    \"Dashboard\": [3, 3, 1],\n    \"SQL\": [2, 2, 2],\n    \"OneStream\": [0, 0, 0],\n    \"Cloud Computing\": [2, 2, 2]\n}\n\n# Create DataFrame\ndf_skills = pd.DataFrame(skills_data)\ndf_skills.set_index(\"Name\", inplace=True)\n\n# Get skill categories\ncategories = df_skills.columns.tolist()\n\n# Create Plotly radar chart\nfig = go.Figure()\n\nfor name in df_skills.index:\n    values = df_skills.loc[name].tolist()\n    values += values[:1]  # close the radar loop\n\n    fig.add_trace(go.Scatterpolar(\n        r=values,\n        theta=categories + [categories[0]],  # close the loop\n        fill='toself',\n        name=name\n    ))\n\n# Customize layout\nfig.update_layout(\n    polar=dict(\n        radialaxis=dict(\n            visible=True,\n            range=[0, 6]\n        )),\n    showlegend=True,\n    title=\"Team Skillset Levels\"\n)\n\n\n\nfig.write_html(\"./figures/skill_gap.html\", include_plotlyjs='cdn')\n\n#https://plotly.com/python/radar-chart/\n\n\nThe radar chart above displays each individual evaluation of our skills for the top five skills on demand for analyst roles."
  },
  {
    "objectID": "modeling copy.html",
    "href": "modeling copy.html",
    "title": "Geographic Analysis",
    "section": "",
    "text": "Introduction\nbla balal\n\n\nimport findspark\nfindspark.init()\n\nfrom pyspark.sql import SparkSession\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\nimport numpy as np\n\nnp.random.seed(42)\n\npio.renderers.default = \"notebook\"\n\n# Initialize Spark Session\nspark = SparkSession.builder.appName(\"LightcastData\").getOrCreate()\n\n# Load Data\ndf = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").option(\"escape\", \"\\\"\").csv(\"./data/lightcast_job_postings.csv\")\n\n# Show Schema and Sample Data\nprint(\"---This is Diagnostic check, No need to print it in the final doc---\")\n\ndf.printSchema() # comment this line when rendering the submission\ndf.show(5)\n\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/06/27 02:20:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n                                                                                \n\n\n---This is Diagnostic check, No need to print it in the final doc---\nroot\n |-- ID: string (nullable = true)\n |-- LAST_UPDATED_DATE: string (nullable = true)\n |-- LAST_UPDATED_TIMESTAMP: timestamp (nullable = true)\n |-- DUPLICATES: integer (nullable = true)\n |-- POSTED: string (nullable = true)\n |-- EXPIRED: string (nullable = true)\n |-- DURATION: integer (nullable = true)\n |-- SOURCE_TYPES: string (nullable = true)\n |-- SOURCES: string (nullable = true)\n |-- URL: string (nullable = true)\n |-- ACTIVE_URLS: string (nullable = true)\n |-- ACTIVE_SOURCES_INFO: string (nullable = true)\n |-- TITLE_RAW: string (nullable = true)\n |-- BODY: string (nullable = true)\n |-- MODELED_EXPIRED: string (nullable = true)\n |-- MODELED_DURATION: integer (nullable = true)\n |-- COMPANY: integer (nullable = true)\n |-- COMPANY_NAME: string (nullable = true)\n |-- COMPANY_RAW: string (nullable = true)\n |-- COMPANY_IS_STAFFING: boolean (nullable = true)\n |-- EDUCATION_LEVELS: string (nullable = true)\n |-- EDUCATION_LEVELS_NAME: string (nullable = true)\n |-- MIN_EDULEVELS: integer (nullable = true)\n |-- MIN_EDULEVELS_NAME: string (nullable = true)\n |-- MAX_EDULEVELS: integer (nullable = true)\n |-- MAX_EDULEVELS_NAME: string (nullable = true)\n |-- EMPLOYMENT_TYPE: integer (nullable = true)\n |-- EMPLOYMENT_TYPE_NAME: string (nullable = true)\n |-- MIN_YEARS_EXPERIENCE: integer (nullable = true)\n |-- MAX_YEARS_EXPERIENCE: integer (nullable = true)\n |-- IS_INTERNSHIP: boolean (nullable = true)\n |-- SALARY: integer (nullable = true)\n |-- REMOTE_TYPE: integer (nullable = true)\n |-- REMOTE_TYPE_NAME: string (nullable = true)\n |-- ORIGINAL_PAY_PERIOD: string (nullable = true)\n |-- SALARY_TO: integer (nullable = true)\n |-- SALARY_FROM: integer (nullable = true)\n |-- LOCATION: string (nullable = true)\n |-- CITY: string (nullable = true)\n |-- CITY_NAME: string (nullable = true)\n |-- COUNTY: integer (nullable = true)\n |-- COUNTY_NAME: string (nullable = true)\n |-- MSA: integer (nullable = true)\n |-- MSA_NAME: string (nullable = true)\n |-- STATE: integer (nullable = true)\n |-- STATE_NAME: string (nullable = true)\n |-- COUNTY_OUTGOING: integer (nullable = true)\n |-- COUNTY_NAME_OUTGOING: string (nullable = true)\n |-- COUNTY_INCOMING: integer (nullable = true)\n |-- COUNTY_NAME_INCOMING: string (nullable = true)\n |-- MSA_OUTGOING: integer (nullable = true)\n |-- MSA_NAME_OUTGOING: string (nullable = true)\n |-- MSA_INCOMING: integer (nullable = true)\n |-- MSA_NAME_INCOMING: string (nullable = true)\n |-- NAICS2: integer (nullable = true)\n |-- NAICS2_NAME: string (nullable = true)\n |-- NAICS3: integer (nullable = true)\n |-- NAICS3_NAME: string (nullable = true)\n |-- NAICS4: integer (nullable = true)\n |-- NAICS4_NAME: string (nullable = true)\n |-- NAICS5: integer (nullable = true)\n |-- NAICS5_NAME: string (nullable = true)\n |-- NAICS6: integer (nullable = true)\n |-- NAICS6_NAME: string (nullable = true)\n |-- TITLE: string (nullable = true)\n |-- TITLE_NAME: string (nullable = true)\n |-- TITLE_CLEAN: string (nullable = true)\n |-- SKILLS: string (nullable = true)\n |-- SKILLS_NAME: string (nullable = true)\n |-- SPECIALIZED_SKILLS: string (nullable = true)\n |-- SPECIALIZED_SKILLS_NAME: string (nullable = true)\n |-- CERTIFICATIONS: string (nullable = true)\n |-- CERTIFICATIONS_NAME: string (nullable = true)\n |-- COMMON_SKILLS: string (nullable = true)\n |-- COMMON_SKILLS_NAME: string (nullable = true)\n |-- SOFTWARE_SKILLS: string (nullable = true)\n |-- SOFTWARE_SKILLS_NAME: string (nullable = true)\n |-- ONET: string (nullable = true)\n |-- ONET_NAME: string (nullable = true)\n |-- ONET_2019: string (nullable = true)\n |-- ONET_2019_NAME: string (nullable = true)\n |-- CIP6: string (nullable = true)\n |-- CIP6_NAME: string (nullable = true)\n |-- CIP4: string (nullable = true)\n |-- CIP4_NAME: string (nullable = true)\n |-- CIP2: string (nullable = true)\n |-- CIP2_NAME: string (nullable = true)\n |-- SOC_2021_2: string (nullable = true)\n |-- SOC_2021_2_NAME: string (nullable = true)\n |-- SOC_2021_3: string (nullable = true)\n |-- SOC_2021_3_NAME: string (nullable = true)\n |-- SOC_2021_4: string (nullable = true)\n |-- SOC_2021_4_NAME: string (nullable = true)\n |-- SOC_2021_5: string (nullable = true)\n |-- SOC_2021_5_NAME: string (nullable = true)\n |-- LOT_CAREER_AREA: integer (nullable = true)\n |-- LOT_CAREER_AREA_NAME: string (nullable = true)\n |-- LOT_OCCUPATION: integer (nullable = true)\n |-- LOT_OCCUPATION_NAME: string (nullable = true)\n |-- LOT_SPECIALIZED_OCCUPATION: integer (nullable = true)\n |-- LOT_SPECIALIZED_OCCUPATION_NAME: string (nullable = true)\n |-- LOT_OCCUPATION_GROUP: integer (nullable = true)\n |-- LOT_OCCUPATION_GROUP_NAME: string (nullable = true)\n |-- LOT_V6_SPECIALIZED_OCCUPATION: integer (nullable = true)\n |-- LOT_V6_SPECIALIZED_OCCUPATION_NAME: string (nullable = true)\n |-- LOT_V6_OCCUPATION: integer (nullable = true)\n |-- LOT_V6_OCCUPATION_NAME: string (nullable = true)\n |-- LOT_V6_OCCUPATION_GROUP: integer (nullable = true)\n |-- LOT_V6_OCCUPATION_GROUP_NAME: string (nullable = true)\n |-- LOT_V6_CAREER_AREA: integer (nullable = true)\n |-- LOT_V6_CAREER_AREA_NAME: string (nullable = true)\n |-- SOC_2: string (nullable = true)\n |-- SOC_2_NAME: string (nullable = true)\n |-- SOC_3: string (nullable = true)\n |-- SOC_3_NAME: string (nullable = true)\n |-- SOC_4: string (nullable = true)\n |-- SOC_4_NAME: string (nullable = true)\n |-- SOC_5: string (nullable = true)\n |-- SOC_5_NAME: string (nullable = true)\n |-- LIGHTCAST_SECTORS: string (nullable = true)\n |-- LIGHTCAST_SECTORS_NAME: string (nullable = true)\n |-- NAICS_2022_2: integer (nullable = true)\n |-- NAICS_2022_2_NAME: string (nullable = true)\n |-- NAICS_2022_3: integer (nullable = true)\n |-- NAICS_2022_3_NAME: string (nullable = true)\n |-- NAICS_2022_4: integer (nullable = true)\n |-- NAICS_2022_4_NAME: string (nullable = true)\n |-- NAICS_2022_5: integer (nullable = true)\n |-- NAICS_2022_5_NAME: string (nullable = true)\n |-- NAICS_2022_6: integer (nullable = true)\n |-- NAICS_2022_6_NAME: string (nullable = true)\n\n\n\n25/06/27 02:21:14 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n                                                                                \n\n\n+--------------------+-----------------+----------------------+----------+--------+---------+--------+--------------------+--------------------+--------------------+-----------+-------------------+--------------------+--------------------+---------------+----------------+--------+--------------------+-----------+-------------------+----------------+---------------------+-------------+-------------------+-------------+------------------+---------------+--------------------+--------------------+--------------------+-------------+------+-----------+----------------+-------------------+---------+-----------+--------------------+--------------------+-------------+------+--------------+-----+--------------------+-----+----------+---------------+--------------------+---------------+--------------------+------------+--------------------+------------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+----------+--------------------+----------+---------------+----------+---------------+---------------+--------------------+--------------+--------------------+--------------------------+-------------------------------+--------------------+-------------------------+-----------------------------+----------------------------------+-----------------+----------------------+-----------------------+----------------------------+------------------+-----------------------+-------+--------------------+-------+--------------------+-------+---------------+-------+---------------+-----------------+----------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+\n|                  ID|LAST_UPDATED_DATE|LAST_UPDATED_TIMESTAMP|DUPLICATES|  POSTED|  EXPIRED|DURATION|        SOURCE_TYPES|             SOURCES|                 URL|ACTIVE_URLS|ACTIVE_SOURCES_INFO|           TITLE_RAW|                BODY|MODELED_EXPIRED|MODELED_DURATION| COMPANY|        COMPANY_NAME|COMPANY_RAW|COMPANY_IS_STAFFING|EDUCATION_LEVELS|EDUCATION_LEVELS_NAME|MIN_EDULEVELS| MIN_EDULEVELS_NAME|MAX_EDULEVELS|MAX_EDULEVELS_NAME|EMPLOYMENT_TYPE|EMPLOYMENT_TYPE_NAME|MIN_YEARS_EXPERIENCE|MAX_YEARS_EXPERIENCE|IS_INTERNSHIP|SALARY|REMOTE_TYPE|REMOTE_TYPE_NAME|ORIGINAL_PAY_PERIOD|SALARY_TO|SALARY_FROM|            LOCATION|                CITY|    CITY_NAME|COUNTY|   COUNTY_NAME|  MSA|            MSA_NAME|STATE|STATE_NAME|COUNTY_OUTGOING|COUNTY_NAME_OUTGOING|COUNTY_INCOMING|COUNTY_NAME_INCOMING|MSA_OUTGOING|   MSA_NAME_OUTGOING|MSA_INCOMING|   MSA_NAME_INCOMING|NAICS2|         NAICS2_NAME|NAICS3|         NAICS3_NAME|NAICS4|         NAICS4_NAME|NAICS5|         NAICS5_NAME|NAICS6|         NAICS6_NAME|             TITLE|         TITLE_NAME|         TITLE_CLEAN|              SKILLS|         SKILLS_NAME|  SPECIALIZED_SKILLS|SPECIALIZED_SKILLS_NAME|      CERTIFICATIONS| CERTIFICATIONS_NAME|       COMMON_SKILLS|  COMMON_SKILLS_NAME|     SOFTWARE_SKILLS|SOFTWARE_SKILLS_NAME|      ONET|           ONET_NAME| ONET_2019|      ONET_2019_NAME|                CIP6|           CIP6_NAME|                CIP4|           CIP4_NAME|                CIP2|           CIP2_NAME|SOC_2021_2|     SOC_2021_2_NAME|SOC_2021_3|     SOC_2021_3_NAME|SOC_2021_4|SOC_2021_4_NAME|SOC_2021_5|SOC_2021_5_NAME|LOT_CAREER_AREA|LOT_CAREER_AREA_NAME|LOT_OCCUPATION| LOT_OCCUPATION_NAME|LOT_SPECIALIZED_OCCUPATION|LOT_SPECIALIZED_OCCUPATION_NAME|LOT_OCCUPATION_GROUP|LOT_OCCUPATION_GROUP_NAME|LOT_V6_SPECIALIZED_OCCUPATION|LOT_V6_SPECIALIZED_OCCUPATION_NAME|LOT_V6_OCCUPATION|LOT_V6_OCCUPATION_NAME|LOT_V6_OCCUPATION_GROUP|LOT_V6_OCCUPATION_GROUP_NAME|LOT_V6_CAREER_AREA|LOT_V6_CAREER_AREA_NAME|  SOC_2|          SOC_2_NAME|  SOC_3|          SOC_3_NAME|  SOC_4|     SOC_4_NAME|  SOC_5|     SOC_5_NAME|LIGHTCAST_SECTORS|LIGHTCAST_SECTORS_NAME|NAICS_2022_2|   NAICS_2022_2_NAME|NAICS_2022_3|   NAICS_2022_3_NAME|NAICS_2022_4|   NAICS_2022_4_NAME|NAICS_2022_5|   NAICS_2022_5_NAME|NAICS_2022_6|   NAICS_2022_6_NAME|\n+--------------------+-----------------+----------------------+----------+--------+---------+--------+--------------------+--------------------+--------------------+-----------+-------------------+--------------------+--------------------+---------------+----------------+--------+--------------------+-----------+-------------------+----------------+---------------------+-------------+-------------------+-------------+------------------+---------------+--------------------+--------------------+--------------------+-------------+------+-----------+----------------+-------------------+---------+-----------+--------------------+--------------------+-------------+------+--------------+-----+--------------------+-----+----------+---------------+--------------------+---------------+--------------------+------------+--------------------+------------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+----------+--------------------+----------+---------------+----------+---------------+---------------+--------------------+--------------+--------------------+--------------------------+-------------------------------+--------------------+-------------------------+-----------------------------+----------------------------------+-----------------+----------------------+-----------------------+----------------------------+------------------+-----------------------+-------+--------------------+-------+--------------------+-------+---------------+-------+---------------+-----------------+----------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+\n|1f57d95acf4dc67ed...|         9/6/2024|  2024-09-06 20:32:...|         0|6/2/2024| 6/8/2024|       6|   [\\n  \"Company\"\\n]|[\\n  \"brassring.c...|[\\n  \"https://sjo...|         []|               NULL|Enterprise Analys...|31-May-2024\\n\\nEn...|       6/8/2024|               6|  894731|          Murphy USA| Murphy USA|              false|       [\\n  2\\n]| [\\n  \"Bachelor's ...|            2|  Bachelor's degree|         NULL|              NULL|              1|Full-time (&gt; 32 h...|                   2|                   2|        false|  NULL|          0|          [None]|               NULL|     NULL|       NULL|{\\n  \"lat\": 33.20...|RWwgRG9yYWRvLCBBUg==|El Dorado, AR|  5139|     Union, AR|20980|       El Dorado, AR|    5|  Arkansas|           5139|           Union, AR|           5139|           Union, AR|       20980|       El Dorado, AR|       20980|       El Dorado, AR|    44|        Retail Trade|   441|Motor Vehicle and...|  4413|Automotive Parts,...| 44133|Automotive Parts ...|441330|Automotive Parts ...|ET29C073C03D1F86B4|Enterprise Analysts|enterprise analys...|[\\n  \"KS126DB6T06...|[\\n  \"Merchandisi...|[\\n  \"KS126DB6T06...|   [\\n  \"Merchandisi...|                  []|                  []|[\\n  \"KS126706DPF...|[\\n  \"Mathematics...|[\\n  \"KS440W865GC...|[\\n  \"SQL (Progra...|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|[\\n  \"45.0601\",\\n...|[\\n  \"Economics, ...|[\\n  \"45.06\",\\n  ...|[\\n  \"Economics\",...|[\\n  \"45\",\\n  \"27...|[\\n  \"Social Scie...|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231010|Business Intellig...|                  23101011|           General ERP Analy...|                2310|     Business Intellig...|                     23101011|              General ERP Analy...|           231010|  Business Intellig...|                   2310|        Business Intellig...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|        [\\n  7\\n]|  [\\n  \"Artificial ...|          44|        Retail Trade|         441|Motor Vehicle and...|        4413|Automotive Parts,...|       44133|Automotive Parts ...|      441330|Automotive Parts ...|\n|0cb072af26757b6c4...|         8/2/2024|  2024-08-02 17:08:...|         0|6/2/2024| 8/1/2024|    NULL| [\\n  \"Job Board\"\\n]| [\\n  \"maine.gov\"\\n]|[\\n  \"https://job...|         []|               NULL|Oracle Consultant...|Oracle Consultant...|       8/1/2024|            NULL|  133098|Smx Corporation L...|        SMX|               true|      [\\n  99\\n]| [\\n  \"No Educatio...|           99|No Education Listed|         NULL|              NULL|              1|Full-time (&gt; 32 h...|                   3|                   3|        false|  NULL|          1|          Remote|               NULL|     NULL|       NULL|{\\n  \"lat\": 44.31...|    QXVndXN0YSwgTUU=|  Augusta, ME| 23011|  Kennebec, ME|12300|Augusta-Watervill...|   23|     Maine|          23011|        Kennebec, ME|          23011|        Kennebec, ME|       12300|Augusta-Watervill...|       12300|Augusta-Watervill...|    56|Administrative an...|   561|Administrative an...|  5613| Employment Services| 56132|Temporary Help Se...|561320|Temporary Help Se...|ET21DDA63780A7DC09| Oracle Consultants|oracle consultant...|[\\n  \"KS122626T55...|[\\n  \"Procurement...|[\\n  \"KS122626T55...|   [\\n  \"Procurement...|                  []|                  []|                  []|                  []|[\\n  \"BGSBF3F508F...|[\\n  \"Oracle Busi...|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|                  []|                  []|                  []|                  []|                  []|                  []|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231010|Business Intellig...|                  23101012|           Oracle Consultant...|                2310|     Business Intellig...|                     23101012|              Oracle Consultant...|           231010|  Business Intellig...|                   2310|        Business Intellig...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|             NULL|                  NULL|          56|Administrative an...|         561|Administrative an...|        5613| Employment Services|       56132|Temporary Help Se...|      561320|Temporary Help Se...|\n|85318b12b3331fa49...|         9/6/2024|  2024-09-06 20:32:...|         1|6/2/2024| 7/7/2024|      35| [\\n  \"Job Board\"\\n]|[\\n  \"dejobs.org\"\\n]|[\\n  \"https://dej...|         []|               NULL|        Data Analyst|Taking care of pe...|      6/10/2024|               8|39063746|            Sedgwick|   Sedgwick|              false|       [\\n  2\\n]| [\\n  \"Bachelor's ...|            2|  Bachelor's degree|         NULL|              NULL|              1|Full-time (&gt; 32 h...|                   5|                NULL|        false|  NULL|          0|          [None]|               NULL|     NULL|       NULL|{\\n  \"lat\": 32.77...|    RGFsbGFzLCBUWA==|   Dallas, TX| 48113|    Dallas, TX|19100|Dallas-Fort Worth...|   48|     Texas|          48113|          Dallas, TX|          48113|          Dallas, TX|       19100|Dallas-Fort Worth...|       19100|Dallas-Fort Worth...|    52|Finance and Insur...|   524|Insurance Carrier...|  5242|Agencies, Brokera...| 52429|Other Insurance R...|524291|    Claims Adjusting|ET3037E0C947A02404|      Data Analysts|        data analyst|[\\n  \"KS1218W78FG...|[\\n  \"Management\"...|[\\n  \"ESF3939CE1F...|   [\\n  \"Exception R...|[\\n  \"KS683TN76T7...|[\\n  \"Security Cl...|[\\n  \"KS1218W78FG...|[\\n  \"Management\"...|[\\n  \"KS126HY6YLT...|[\\n  \"Microsoft O...|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|                  []|                  []|                  []|                  []|                  []|                  []|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231113|Data / Data Minin...|                  23111310|                   Data Analyst|                2311|     Data Analysis and...|                     23111310|                      Data Analyst|           231113|  Data / Data Minin...|                   2311|        Data Analysis and...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|             NULL|                  NULL|          52|Finance and Insur...|         524|Insurance Carrier...|        5242|Agencies, Brokera...|       52429|Other Insurance R...|      524291|    Claims Adjusting|\n|1b5c3941e54a1889e...|         9/6/2024|  2024-09-06 20:32:...|         1|6/2/2024|7/20/2024|      48| [\\n  \"Job Board\"\\n]|[\\n  \"disabledper...|[\\n  \"https://www...|         []|               NULL|Sr. Lead Data Mgm...|About this role:\\...|      6/12/2024|              10|37615159|         Wells Fargo|Wells Fargo|              false|      [\\n  99\\n]| [\\n  \"No Educatio...|           99|No Education Listed|         NULL|              NULL|              1|Full-time (&gt; 32 h...|                   3|                NULL|        false|  NULL|          0|          [None]|               NULL|     NULL|       NULL|{\\n  \"lat\": 33.44...|    UGhvZW5peCwgQVo=|  Phoenix, AZ|  4013|  Maricopa, AZ|38060|Phoenix-Mesa-Chan...|    4|   Arizona|           4013|        Maricopa, AZ|           4013|        Maricopa, AZ|       38060|Phoenix-Mesa-Chan...|       38060|Phoenix-Mesa-Chan...|    52|Finance and Insur...|   522|Credit Intermedia...|  5221|Depository Credit...| 52211|  Commercial Banking|522110|  Commercial Banking|ET2114E0404BA30075|Management Analysts|sr lead data mgmt...|[\\n  \"KS123QX62QY...|[\\n  \"Exit Strate...|[\\n  \"KS123QX62QY...|   [\\n  \"Exit Strate...|                  []|                  []|[\\n  \"KS7G6NP6R6L...|[\\n  \"Reliability...|[\\n  \"KS4409D76NW...|[\\n  \"SAS (Softwa...|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|                  []|                  []|                  []|                  []|                  []|                  []|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231113|Data / Data Minin...|                  23111310|                   Data Analyst|                2311|     Data Analysis and...|                     23111310|                      Data Analyst|           231113|  Data / Data Minin...|                   2311|        Data Analysis and...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|        [\\n  6\\n]|  [\\n  \"Data Privac...|          52|Finance and Insur...|         522|Credit Intermedia...|        5221|Depository Credit...|       52211|  Commercial Banking|      522110|  Commercial Banking|\n|cb5ca25f02bdf25c1...|        6/19/2024|   2024-06-19 07:00:00|         0|6/2/2024|6/17/2024|      15|[\\n  \"FreeJobBoar...|[\\n  \"craigslist....|[\\n  \"https://mod...|         []|               NULL|Comisiones de $10...|Comisiones de $10...|      6/17/2024|              15|       0|        Unclassified|      LH/GM|              false|      [\\n  99\\n]| [\\n  \"No Educatio...|           99|No Education Listed|         NULL|              NULL|              3|Part-time / full-...|                NULL|                NULL|        false| 92500|          0|          [None]|               year|   150000|      35000|{\\n  \"lat\": 37.63...|    TW9kZXN0bywgQ0E=|  Modesto, CA|  6099|Stanislaus, CA|33700|         Modesto, CA|    6|California|           6099|      Stanislaus, CA|           6099|      Stanislaus, CA|       33700|         Modesto, CA|       33700|         Modesto, CA|    99|Unclassified Indu...|   999|Unclassified Indu...|  9999|Unclassified Indu...| 99999|Unclassified Indu...|999999|Unclassified Indu...|ET0000000000000000|       Unclassified|comisiones de por...|                  []|                  []|                  []|                     []|                  []|                  []|                  []|                  []|                  []|                  []|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|                  []|                  []|                  []|                  []|                  []|                  []|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231010|Business Intellig...|                  23101012|           Oracle Consultant...|                2310|     Business Intellig...|                     23101012|              Oracle Consultant...|           231010|  Business Intellig...|                   2310|        Business Intellig...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|             NULL|                  NULL|          99|Unclassified Indu...|         999|Unclassified Indu...|        9999|Unclassified Indu...|       99999|Unclassified Indu...|      999999|Unclassified Indu...|\n+--------------------+-----------------+----------------------+----------+--------+---------+--------+--------------------+--------------------+--------------------+-----------+-------------------+--------------------+--------------------+---------------+----------------+--------+--------------------+-----------+-------------------+----------------+---------------------+-------------+-------------------+-------------+------------------+---------------+--------------------+--------------------+--------------------+-------------+------+-----------+----------------+-------------------+---------+-----------+--------------------+--------------------+-------------+------+--------------+-----+--------------------+-----+----------+---------------+--------------------+---------------+--------------------+------------+--------------------+------------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+----------+--------------------+----------+---------------+----------+---------------+---------------+--------------------+--------------+--------------------+--------------------------+-------------------------------+--------------------+-------------------------+-----------------------------+----------------------------------+-----------------+----------------------+-----------------------+----------------------------+------------------+-----------------------+-------+--------------------+-------+--------------------+-------+---------------+-------+---------------+-----------------+----------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+\nonly showing top 5 rows\n\n\n\n\n## Listing Columns So We Can Reference them in Visuals\n\nimport pandas as pd\ndf = pd.read_csv(\"./data/lightcast_job_postings.csv\")\nprint(df.columns.tolist())\n\n/tmp/ipykernel_1690/3265774581.py:4: DtypeWarning:\n\nColumns (19,30) have mixed types. Specify dtype option on import or set low_memory=False.\n\n\n\n['ID', 'LAST_UPDATED_DATE', 'LAST_UPDATED_TIMESTAMP', 'DUPLICATES', 'POSTED', 'EXPIRED', 'DURATION', 'SOURCE_TYPES', 'SOURCES', 'URL', 'ACTIVE_URLS', 'ACTIVE_SOURCES_INFO', 'TITLE_RAW', 'BODY', 'MODELED_EXPIRED', 'MODELED_DURATION', 'COMPANY', 'COMPANY_NAME', 'COMPANY_RAW', 'COMPANY_IS_STAFFING', 'EDUCATION_LEVELS', 'EDUCATION_LEVELS_NAME', 'MIN_EDULEVELS', 'MIN_EDULEVELS_NAME', 'MAX_EDULEVELS', 'MAX_EDULEVELS_NAME', 'EMPLOYMENT_TYPE', 'EMPLOYMENT_TYPE_NAME', 'MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE', 'IS_INTERNSHIP', 'SALARY', 'REMOTE_TYPE', 'REMOTE_TYPE_NAME', 'ORIGINAL_PAY_PERIOD', 'SALARY_TO', 'SALARY_FROM', 'LOCATION', 'CITY', 'CITY_NAME', 'COUNTY', 'COUNTY_NAME', 'MSA', 'MSA_NAME', 'STATE', 'STATE_NAME', 'COUNTY_OUTGOING', 'COUNTY_NAME_OUTGOING', 'COUNTY_INCOMING', 'COUNTY_NAME_INCOMING', 'MSA_OUTGOING', 'MSA_NAME_OUTGOING', 'MSA_INCOMING', 'MSA_NAME_INCOMING', 'NAICS2', 'NAICS2_NAME', 'NAICS3', 'NAICS3_NAME', 'NAICS4', 'NAICS4_NAME', 'NAICS5', 'NAICS5_NAME', 'NAICS6', 'NAICS6_NAME', 'TITLE', 'TITLE_NAME', 'TITLE_CLEAN', 'SKILLS', 'SKILLS_NAME', 'SPECIALIZED_SKILLS', 'SPECIALIZED_SKILLS_NAME', 'CERTIFICATIONS', 'CERTIFICATIONS_NAME', 'COMMON_SKILLS', 'COMMON_SKILLS_NAME', 'SOFTWARE_SKILLS', 'SOFTWARE_SKILLS_NAME', 'ONET', 'ONET_NAME', 'ONET_2019', 'ONET_2019_NAME', 'CIP6', 'CIP6_NAME', 'CIP4', 'CIP4_NAME', 'CIP2', 'CIP2_NAME', 'SOC_2021_2', 'SOC_2021_2_NAME', 'SOC_2021_3', 'SOC_2021_3_NAME', 'SOC_2021_4', 'SOC_2021_4_NAME', 'SOC_2021_5', 'SOC_2021_5_NAME', 'LOT_CAREER_AREA', 'LOT_CAREER_AREA_NAME', 'LOT_OCCUPATION', 'LOT_OCCUPATION_NAME', 'LOT_SPECIALIZED_OCCUPATION', 'LOT_SPECIALIZED_OCCUPATION_NAME', 'LOT_OCCUPATION_GROUP', 'LOT_OCCUPATION_GROUP_NAME', 'LOT_V6_SPECIALIZED_OCCUPATION', 'LOT_V6_SPECIALIZED_OCCUPATION_NAME', 'LOT_V6_OCCUPATION', 'LOT_V6_OCCUPATION_NAME', 'LOT_V6_OCCUPATION_GROUP', 'LOT_V6_OCCUPATION_GROUP_NAME', 'LOT_V6_CAREER_AREA', 'LOT_V6_CAREER_AREA_NAME', 'SOC_2', 'SOC_2_NAME', 'SOC_3', 'SOC_3_NAME', 'SOC_4', 'SOC_4_NAME', 'SOC_5', 'SOC_5_NAME', 'LIGHTCAST_SECTORS', 'LIGHTCAST_SECTORS_NAME', 'NAICS_2022_2', 'NAICS_2022_2_NAME', 'NAICS_2022_3', 'NAICS_2022_3_NAME', 'NAICS_2022_4', 'NAICS_2022_4_NAME', 'NAICS_2022_5', 'NAICS_2022_5_NAME', 'NAICS_2022_6', 'NAICS_2022_6_NAME']\n\n\n\nimport pandas as pd\n\ndf = pd.read_csv(\"./data/lightcast_job_postings.csv\")\n\ncolumns_to_drop = [\n    \"ID\", \"URL\", \"ACTIVE_URLS\", \"DUPLICATES\", \"LAST_UPDATED_TIMESTAMP\",\n    \"NAICS2\", \"NAICS3\", \"NAICS4\", \"NAICS5\", \"NAICS6\",\n    \"SOC_2\", \"SOC_3\", \"SOC_5\"\n]\n\ndf.drop(columns=columns_to_drop, inplace=True)\n\n/tmp/ipykernel_1690/304705447.py:3: DtypeWarning:\n\nColumns (19,30) have mixed types. Specify dtype option on import or set low_memory=False.\n\n\n\n\nprint(df.columns.tolist())\n\n['LAST_UPDATED_DATE', 'POSTED', 'EXPIRED', 'DURATION', 'SOURCE_TYPES', 'SOURCES', 'ACTIVE_SOURCES_INFO', 'TITLE_RAW', 'BODY', 'MODELED_EXPIRED', 'MODELED_DURATION', 'COMPANY', 'COMPANY_NAME', 'COMPANY_RAW', 'COMPANY_IS_STAFFING', 'EDUCATION_LEVELS', 'EDUCATION_LEVELS_NAME', 'MIN_EDULEVELS', 'MIN_EDULEVELS_NAME', 'MAX_EDULEVELS', 'MAX_EDULEVELS_NAME', 'EMPLOYMENT_TYPE', 'EMPLOYMENT_TYPE_NAME', 'MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE', 'IS_INTERNSHIP', 'SALARY', 'REMOTE_TYPE', 'REMOTE_TYPE_NAME', 'ORIGINAL_PAY_PERIOD', 'SALARY_TO', 'SALARY_FROM', 'LOCATION', 'CITY', 'CITY_NAME', 'COUNTY', 'COUNTY_NAME', 'MSA', 'MSA_NAME', 'STATE', 'STATE_NAME', 'COUNTY_OUTGOING', 'COUNTY_NAME_OUTGOING', 'COUNTY_INCOMING', 'COUNTY_NAME_INCOMING', 'MSA_OUTGOING', 'MSA_NAME_OUTGOING', 'MSA_INCOMING', 'MSA_NAME_INCOMING', 'NAICS2_NAME', 'NAICS3_NAME', 'NAICS4_NAME', 'NAICS5_NAME', 'NAICS6_NAME', 'TITLE', 'TITLE_NAME', 'TITLE_CLEAN', 'SKILLS', 'SKILLS_NAME', 'SPECIALIZED_SKILLS', 'SPECIALIZED_SKILLS_NAME', 'CERTIFICATIONS', 'CERTIFICATIONS_NAME', 'COMMON_SKILLS', 'COMMON_SKILLS_NAME', 'SOFTWARE_SKILLS', 'SOFTWARE_SKILLS_NAME', 'ONET', 'ONET_NAME', 'ONET_2019', 'ONET_2019_NAME', 'CIP6', 'CIP6_NAME', 'CIP4', 'CIP4_NAME', 'CIP2', 'CIP2_NAME', 'SOC_2021_2', 'SOC_2021_2_NAME', 'SOC_2021_3', 'SOC_2021_3_NAME', 'SOC_2021_4', 'SOC_2021_4_NAME', 'SOC_2021_5', 'SOC_2021_5_NAME', 'LOT_CAREER_AREA', 'LOT_CAREER_AREA_NAME', 'LOT_OCCUPATION', 'LOT_OCCUPATION_NAME', 'LOT_SPECIALIZED_OCCUPATION', 'LOT_SPECIALIZED_OCCUPATION_NAME', 'LOT_OCCUPATION_GROUP', 'LOT_OCCUPATION_GROUP_NAME', 'LOT_V6_SPECIALIZED_OCCUPATION', 'LOT_V6_SPECIALIZED_OCCUPATION_NAME', 'LOT_V6_OCCUPATION', 'LOT_V6_OCCUPATION_NAME', 'LOT_V6_OCCUPATION_GROUP', 'LOT_V6_OCCUPATION_GROUP_NAME', 'LOT_V6_CAREER_AREA', 'LOT_V6_CAREER_AREA_NAME', 'SOC_2_NAME', 'SOC_3_NAME', 'SOC_4', 'SOC_4_NAME', 'SOC_5_NAME', 'LIGHTCAST_SECTORS', 'LIGHTCAST_SECTORS_NAME', 'NAICS_2022_2', 'NAICS_2022_2_NAME', 'NAICS_2022_3', 'NAICS_2022_3_NAME', 'NAICS_2022_4', 'NAICS_2022_4_NAME', 'NAICS_2022_5', 'NAICS_2022_5_NAME', 'NAICS_2022_6', 'NAICS_2022_6_NAME']\n\n\n\n!pip install missingno\n\nRequirement already satisfied: missingno in ./.venv/lib/python3.12/site-packages (0.5.2)\nRequirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from missingno) (2.2.6)\nRequirement already satisfied: matplotlib in ./.venv/lib/python3.12/site-packages (from missingno) (3.10.3)\nRequirement already satisfied: scipy in ./.venv/lib/python3.12/site-packages (from missingno) (1.15.3)\nRequirement already satisfied: seaborn in ./.venv/lib/python3.12/site-packages (from missingno) (0.13.2)\nRequirement already satisfied: contourpy&gt;=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib-&gt;missingno) (1.3.2)\nRequirement already satisfied: cycler&gt;=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib-&gt;missingno) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib-&gt;missingno) (4.58.0)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib-&gt;missingno) (1.4.8)\nRequirement already satisfied: packaging&gt;=20.0 in ./.venv/lib/python3.12/site-packages (from matplotlib-&gt;missingno) (25.0)\nRequirement already satisfied: pillow&gt;=8 in ./.venv/lib/python3.12/site-packages (from matplotlib-&gt;missingno) (11.2.1)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib-&gt;missingno) (3.2.3)\nRequirement already satisfied: python-dateutil&gt;=2.7 in ./.venv/lib/python3.12/site-packages (from matplotlib-&gt;missingno) (2.9.0.post0)\nRequirement already satisfied: pandas&gt;=1.2 in ./.venv/lib/python3.12/site-packages (from seaborn-&gt;missingno) (2.2.3)\nRequirement already satisfied: pytz&gt;=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas&gt;=1.2-&gt;seaborn-&gt;missingno) (2025.2)\nRequirement already satisfied: tzdata&gt;=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas&gt;=1.2-&gt;seaborn-&gt;missingno) (2025.2)\nRequirement already satisfied: six&gt;=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib-&gt;missingno) (1.17.0)\n\n\n\nimport missingno as msno\nimport matplotlib.pyplot as plt\n# Visualize missing values\nmsno.heatmap(df)\nplt.title(\"Missing Values Heatmap\")\nplt.show()\n\n# Drop columns with &gt;50% missing values\ndf.dropna(thresh=len(df) * 0.5, axis=1, inplace=True)\n\n# Fill only the columns you actually have\nif 'Industry' in df.columns:\n    df[\"Industry\"].fillna(\"Unknown\", inplace=True)\n    df[\"Salary\"].fillna(df[\"Salary\"].median(), inplace=True)\n\n\n\n\n\n\n\n\n\ndf = df.drop_duplicates(subset=[\"TITLE\", \"COMPANY\", \"LOCATION\", \"POSTED\"], keep=\"first\")\n\n\ndf = df[df['NAICS_2022_2_NAME'] != 'Unclassified Industry']\n\n\ndf['REMOTE_TYPE_NAME'] = df['REMOTE_TYPE_NAME'].replace('[None]', 'Not Remote')\n\n\n\n\nimport pandas as pd\nimport plotly.express as px\n\n# Step 1: Prepare data\ndata = {\n    'Industry': [\n        'Wholesale Trade', 'Retail Trade', 'Real Estate and Rental and Leasing',\n        'Professional, Scientific, and Technical Services', 'Manufacturing',\n        'Information', 'Health Care and Social Assistance',\n        'Finance and Insurance', 'Educational Services',\n        'Administrative and Support and Waste Management and Remediation Services'\n    ],\n    'Flexible Remote': [87.8, 94.4, 97.6, 92.2, 89.7, 95.8, 92.1, 94.8, 89.0, 94.8],\n    'Onsite': [12.2, 5.6, 2.4, 7.8, 10.3, 4.2, 7.9, 5.2, 11.0, 5.2]\n}\n\ndf = pd.DataFrame(data)\n\n# Step 2: Sort in ascending order of Flexible Remote\ndf_sorted = df.sort_values(by='Flexible Remote', ascending=True)\ndf_sorted['Industry'] = pd.Categorical(df_sorted['Industry'], categories=df_sorted['Industry'], ordered=True)\n\n# Step 3: Melt data for stacked bar format\ndf_melted = df_sorted.melt(\n    id_vars='Industry',\n    value_vars=['Flexible Remote', 'Onsite'],\n    var_name='Remote Type',\n    value_name='Percentage'\n)\n\n# Step 4: Plot\nfig = px.bar(\n    df_melted,\n    x='Percentage',\n    y='Industry',\n    color='Remote Type',\n    orientation='h',\n    text='Percentage',\n    color_discrete_map={\n        'Flexible Remote': '#1aab89',\n        'Onsite': '#88d4c3'\n    },\n    title=\"Remote Job Distribution by Industry (Top 10 Industries)\"\n)\n\n# Step 5: Layout adjustments\nfig.update_layout(\n    xaxis_title=\"Percentage of Jobs\",\n    yaxis_title=\"\",\n    xaxis=dict(tickformat=\".0f\"),\n    legend_title=\"Remote Type\",\n    barmode='stack',\n    margin=dict(l=10, r=10, t=60, b=40),\n    height=500\n)\n\n# Step 6: Label formatting\nfig.update_traces(texttemplate='%{text:.1f}%', textposition='inside')\n\n# Save plot\nfig.write_html(\"./figures/top_industries.html\")\n\n\n# Show plot\nfig.show()\n\n\n\n\n\n\n\n\n\n\n        \n        \n        \n\n\n                                                    \n\n\n\nimport pandas as pd\nfrom IPython.display import display\n# 1. Load the dataset and parse dates\ndf = pd.read_csv(\"./data/lightcast_job_postings.csv\", parse_dates=[\"POSTED\", \"EXPIRED\"])\n# Calculate DURATION in days\ndf['DURATION'] = (df['EXPIRED'] - df['POSTED']).dt.days\n# Extract POSTED month\ndf['POSTED_MONTH'] = df['POSTED'].dt.to_period(\"M\")\n\n/tmp/ipykernel_1690/784347523.py:4: DtypeWarning:\n\nColumns (19,30) have mixed types. Specify dtype option on import or set low_memory=False.\n\n\n\n\n# Define AI and non-AI impacted NAICS codes\nai_impacted_naics = [31, 32, 33, 42, 44, 45, 51, 52, 54, 55, 56]\nnon_ai_impacted_naics = [11, 21, 22, 23, 48, 49, 61, 62, 71, 72]\n\n\n# Create IS_AI_JOB flag\ndf['IS_AI_JOB'] = df['NAICS_2022_2'].apply(lambda x: 1 if pd.notna(x) and x in ai_impacted_naics else 0)\n\n# Remove rows with missing STATE_NAME or POSTED_MONTH\ndf = df[df['STATE_NAME'].notna() & df['POSTED_MONTH'].notna()]\n\n# Group by POSTED_MONTH, STATE_NAME, IS_AI_JOB to count jobs\njob_counts = (\n    df.groupby(['POSTED_MONTH', 'STATE_NAME', 'IS_AI_JOB'])\n      .size()\n      .reset_index(name='JOB_COUNT')\n)\n\n\n# Pivot to put months as rows, states as columns\npivot_df = job_counts.pivot_table(index=['STATE_NAME', 'IS_AI_JOB'], \n                                   columns='POSTED_MONTH', \n                                   values='JOB_COUNT', \n                                   fill_value=0)\n\n# print(pivot_df.columns)\n# print(pivot_df.columns[-1])\n# pivot_df.rename(columns={'2024-05': 'May', \n# '2024-06': 'June',\n# '2024-07': 'July',\n# '2024-08': 'August',\n# '2024-09': 'September'\n# },inplace=True)\npivot_df['2024-06'].reset_index(drop=True)\n\n0       38.0\n1      110.0\n2       19.0\n3       42.0\n4       87.0\n       ...  \n97      17.0\n98      53.0\n99     151.0\n100      1.0\n101     18.0\nName: 2024-06, Length: 102, dtype: float64\n\n\n\n# Calculate % growth from first to last month\n\n# pivot_df[\"PCT_7\"]=(pivot_df['2024-07']-pivot_df['2024-06'])*100/pivot_df['2024-06']\n# pivot_df[\"PCT_8\"]=(pivot_df['2024-08']-pivot_df['2024-07'])*100/pivot_df['2024-07']\n# pivot_df[\"PCT_9\"]=(pivot_df['2024-09']-pivot_df['2024-08'])*100/pivot_df['2024-08']\npivot_df[\"PCT_CHANGE\"]=(pivot_df['2024-09']-pivot_df['2024-05'])*100/pivot_df['2024-05']\n\npivot_df.head()\n\n\n\n\n\n\n\n\nPOSTED_MONTH\n2024-05\n2024-06\n2024-07\n2024-08\n2024-09\nPCT_CHANGE\n\n\nSTATE_NAME\nIS_AI_JOB\n\n\n\n\n\n\n\n\n\n\nAlabama\n0\n62.0\n38.0\n18.0\n31.0\n30.0\n-51.612903\n\n\n1\n100.0\n110.0\n77.0\n132.0\n92.0\n-8.000000\n\n\nAlaska\n0\n13.0\n19.0\n7.0\n18.0\n25.0\n92.307692\n\n\n1\n26.0\n42.0\n30.0\n29.0\n27.0\n3.846154\n\n\nArizona\n0\n102.0\n87.0\n71.0\n72.0\n85.0\n-16.666667\n\n\n\n\n\n\n\n\n\n# Drop rows with undefined % growth (divide by zero)\npivot_df = pivot_df.dropna(subset=['PCT_CHANGE'])\n\n# Reset index for sorting\npivot_df = pivot_df.reset_index()\n\n\n# Get top 10 states by % growth\ntop_ai = pivot_df[pivot_df['IS_AI_JOB'] == 1].sort_values('PCT_CHANGE', ascending=False).head(10)\ntop_non_ai = pivot_df[pivot_df['IS_AI_JOB'] == 0].sort_values('PCT_CHANGE', ascending=False).head(10)\n\n# Combine and label\ntop_combined = pd.concat([\n    top_ai.assign(JOB_TYPE='AI'),\n    top_non_ai.assign(JOB_TYPE='Non-AI')\n])\n\n# Display \ndisplay(top_combined[['STATE_NAME', 'IS_AI_JOB', 'PCT_CHANGE', 'JOB_TYPE']])\n\n\n\n\n\n\n\nPOSTED_MONTH\nSTATE_NAME\nIS_AI_JOB\nPCT_CHANGE\nJOB_TYPE\n\n\n\n\n101\nWyoming\n1\n380.000000\nAI\n\n\n97\nWest Virginia\n1\n380.000000\nAI\n\n\n81\nSouth Dakota\n1\n175.000000\nAI\n\n\n21\nHawaii\n1\n75.000000\nAI\n\n\n67\nNorth Dakota\n1\n64.285714\nAI\n\n\n61\nNew Mexico\n1\n56.000000\nAI\n\n\n73\nOregon\n1\n54.716981\nAI\n\n\n41\nMassachusetts\n1\n40.074906\nAI\n\n\n95\nWashington, D.C. (District of Columbia)\n1\n36.363636\nAI\n\n\n53\nNebraska\n1\n36.065574\nAI\n\n\n66\nNorth Dakota\n0\n275.000000\nNon-AI\n\n\n88\nVermont\n0\n225.000000\nNon-AI\n\n\n96\nWest Virginia\n0\n130.000000\nNon-AI\n\n\n52\nNebraska\n0\n100.000000\nNon-AI\n\n\n72\nOregon\n0\n100.000000\nNon-AI\n\n\n2\nAlaska\n0\n92.307692\nNon-AI\n\n\n32\nKentucky\n0\n53.125000\nNon-AI\n\n\n94\nWashington, D.C. (District of Columbia)\n0\n40.000000\nNon-AI\n\n\n48\nMissouri\n0\n37.931034\nNon-AI\n\n\n60\nNew Mexico\n0\n36.842105\nNon-AI\n\n\n\n\n\n\n\n\ndef label_ai_impact(naics):\n    try:\n        code = int(naics)\n        if code in ai_impacted_naics:\n            return \"AI-Impacted\"\n        elif code in non_ai_impacted_naics:\n            return \"Non-AI-Impacted\"\n        else:\n            return \"Unclassified\"\n    except:\n        return \"Unclassified\"\n\n\ndf['AI_IMPACTED'] = df['NAICS_2022_2'].apply(label_ai_impact)\n# Filter only AI and Non-AI impacted\ndf = df[df[\"AI_IMPACTED\"].isin([\"AI-Impacted\", \"Non-AI-Impacted\"])]\n\n# Now split the data\nai_df = df[df[\"AI_IMPACTED\"] == \"AI-Impacted\"][\n    [\"STATE_NAME\", \"NAICS_2022_2\", \"NAICS_2022_2_NAME\", \"LOT_SPECIALIZED_OCCUPATION_NAME\"]\n]\n\nnon_ai_df = df[df[\"AI_IMPACTED\"] == \"Non-AI-Impacted\"][\n    [\"STATE_NAME\", \"NAICS_2022_2\", \"NAICS_2022_2_NAME\", \"LOT_SPECIALIZED_OCCUPATION_NAME\"]\n]\n\n\nfrom IPython.display import display\n\nprint(\"AI-Impacted Industries:\")\ndisplay(ai_df)\n\nprint(\"\\nNon-AI-Impacted Industries:\")\ndisplay(non_ai_df)\n\nAI-Impacted Industries:\n\n\n\n\n\n\n\n\n\nSTATE_NAME\nNAICS_2022_2\nNAICS_2022_2_NAME\nLOT_SPECIALIZED_OCCUPATION_NAME\n\n\n\n\n0\nArkansas\n44.0\nRetail Trade\nGeneral ERP Analyst / Consultant\n\n\n1\nMaine\n56.0\nAdministrative and Support and Waste Managemen...\nOracle Consultant / Analyst\n\n\n2\nTexas\n52.0\nFinance and Insurance\nData Analyst\n\n\n3\nArizona\n52.0\nFinance and Insurance\nData Analyst\n\n\n5\nArkansas\n51.0\nInformation\nData Analyst\n\n\n...\n...\n...\n...\n...\n\n\n72493\nVirginia\n54.0\nProfessional, Scientific, and Technical Services\nData Analyst\n\n\n72494\nMassachusetts\n51.0\nInformation\nEnterprise Architect\n\n\n72495\nMichigan\n56.0\nAdministrative and Support and Waste Managemen...\nData Analyst\n\n\n72496\nMaine\n51.0\nInformation\nData Analyst\n\n\n72497\nTexas\n54.0\nProfessional, Scientific, and Technical Services\nOracle Consultant / Analyst\n\n\n\n\n52444 rows Ã 4 columns\n\n\n\n\nNon-AI-Impacted Industries:\n\n\n\n\n\n\n\n\n\nSTATE_NAME\nNAICS_2022_2\nNAICS_2022_2_NAME\nLOT_SPECIALIZED_OCCUPATION_NAME\n\n\n\n\n15\nMassachusetts\n61.0\nEducational Services\nData Analyst\n\n\n18\nAlabama\n62.0\nHealth Care and Social Assistance\nEnterprise Architect\n\n\n37\nVirginia\n62.0\nHealth Care and Social Assistance\nData Analyst\n\n\n77\nOhio\n23.0\nConstruction\nEnterprise Architect\n\n\n94\nTexas\n61.0\nEducational Services\nBusiness Analyst (General)\n\n\n...\n...\n...\n...\n...\n\n\n72451\nSouth Carolina\n62.0\nHealth Care and Social Assistance\nData Analyst\n\n\n72460\nCalifornia\n11.0\nAgriculture, Forestry, Fishing and Hunting\nData Analyst\n\n\n72472\nCalifornia\n61.0\nEducational Services\nGeneral ERP Analyst / Consultant\n\n\n72479\nWyoming\n61.0\nEducational Services\nData Analyst\n\n\n72489\nTexas\n22.0\nUtilities\nSAP Analyst / Admin\n\n\n\n\n7942 rows Ã 4 columns\n\n\n\n\n# Step 1: Count total jobs by state\ntotal_jobs_by_state = df.groupby('STATE_NAME').size().rename('Total_Jobs')\n\n\n# Step 2: Count AI-impacted jobs by state\nai_jobs_by_state = df[df['AI_IMPACTED'] == 'AI-Impacted'].groupby('STATE_NAME').size().rename('AI_Impacted_Jobs')\n# Step 3: Merge the two counts into a single DataFrame\nai_impact_summary = pd.concat([total_jobs_by_state, ai_jobs_by_state], axis=1).fillna(0)\n\n\n# Step 1: Clean and convert date columns\ndf = df[df['NAICS_2022_2_NAME'] != 'Unclassified Industry']\ndf['POSTED'] = pd.to_datetime(df['POSTED'], errors='coerce')\ndf['EXPIRED'] = pd.to_datetime(df['EXPIRED'], errors='coerce')\n\n# Drop rows with missing POSTED or EXPIRED dates\ndf = df.dropna(subset=['POSTED', 'EXPIRED'])\n\n\nimport plotly.express as px\n\n# Count postings per industry\nindustry_counts = df['NAICS2_NAME'].value_counts().reset_index()\nindustry_counts.columns = ['NAICS2_NAME', 'count']\n\n# Sort values for better readability\nindustry_counts = industry_counts.sort_values(by='count', ascending=True)\n\n# Horizontal bar plot\nfig = px.bar(\n    industry_counts,\n    x='count',\n    y='NAICS2_NAME',\n    orientation='h',\n    title='Job Postings by Industry',\n    labels={'NAICS2_NAME': 'Industry', 'count': 'Number of Postings'},\n    color='count',\n    color_continuous_scale='Blues'\n)\n\n# Clean layout\nfig.update_layout(\n    yaxis_title='Industry',\n    xaxis_title='Number of Postings',\n    title_font_size=20,\n    plot_bgcolor='white',\n    xaxis=dict(showgrid=True),\n    yaxis=dict(showgrid=False)\n)\n\nfig.show()\n\n                                                    \n\n\n\n# import plotly.express as px\n\n# # AI job growth plot\n# fig_ai = px.bar(\n#     top_ai,\n#     x='STATE_NAME',\n#     y='Avg_AI_Job_Growth',\n#     title='Top 10 States by Average AI Job Growth',\n#     labels={'STATE_NAME': 'State', 'Avg_AI_Job_Growth': 'Avg % AI Job Growth'},\n#     text='Avg_AI_Job_Growth'\n# )\n# fig_ai.update_traces(texttemplate='%{text:.2f}%', textposition='outside')\n# fig_ai.update_layout(yaxis_title='Average % Growth', xaxis_title='State')\n# fig_ai.show()\n\n# # Non-AI job growth plot\n# fig_non_ai = px.bar(\n#     top_non_ai,\n#     x='STATE_NAME',\n#     y='Avg_Non_AI_Job_Growth',\n#     title='Top 10 States by Average Non-AI Job Growth',\n#     labels={'STATE_NAME': 'State', 'Avg_Non_AI_Job_Growth': 'Avg % Non-AI Job Growth'},\n#     text='Avg_Non_AI_Job_Growth'\n# )\n# fig_non_ai.update_traces(texttemplate='%{text:.2f}%', textposition='outside')\n# fig_non_ai.update_layout(yaxis_title='Average % Growth', xaxis_title='State')\n# fig_non_ai.show()\n\n\n\ndf['AVERAGE_SALARY'] = df[['SALARY_FROM', 'SALARY_TO']].mean(axis=1)\n\n\navg_salary_by_state_type = (\n    df.groupby(['STATE_NAME', 'AI_IMPACTED'])['AVERAGE_SALARY']\n    .mean()\n    .reset_index()\n)\n\n\n\n# import numpy as np\n\n# # Step 1: Replace infinite values (result of division by zero) with NaN\n# pivoted.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n# # Step 2: Drop rows with missing percentage growth values\n# pivoted.dropna(subset=['AI_Growth_Pct', 'Non_AI_Growth_Pct'], inplace=True)\n\n# # Step 3: Group by state and calculate average percent growth\n# avg_growth = pivoted.groupby('STATE_NAME')[['AI_Growth_Pct', 'Non_AI_Growth_Pct']].mean().reset_index()\n\n# # Step 4: Sort states by AI and Non-AI growth\n# ranked = avg_growth.sort_values(by=['AI_Growth_Pct', 'Non_AI_Growth_Pct'], ascending=False)\n\n# # Step 5: Display top states for AI and Non-AI job growth\n# from IPython.display import display\n# print(\"Top states by AI job growth:\")\n# display(ranked[['STATE_NAME', 'AI_Growth_Pct']].head(10))\n\n# print(\"\\nTop states by Non-AI job growth:\")\n# display(ranked[['STATE_NAME', 'Non_AI_Growth_Pct']].sort_values(by='Non_AI_Growth_Pct', ascending=False).head(10))\n\n\n# import seaborn as sns\n# import matplotlib.pyplot as plt\n\n# # Step 9: Visualize\n# plt.figure(figsize=(14, 6))\n# avg_growth[['AI_Growth', 'Non_AI_Growth']].head(10).plot(kind='bar')\n# plt.title('Average Monthly Job Growth (Top 10 States) for AI vs. Non-AI Careers')\n# plt.ylabel('Average Monthly Job Growth')\n# plt.xticks(rotation=45)\n# plt.tight_layout()\n# plt.grid(True)\n# plt.show()\n\n\n# Filter for Boston, MA and Austin, TX\nselected_state = ['California', 'Florida', 'Massachusetts', 'Texas', 'New York']\nfiltered_df = df[df['STATE_NAME'].isin(selected_state)]\n\n# Further filter for NAICS_2022_6 = 518210 and show relevant columns\nfinal_df = filtered_df[filtered_df['LOT_SPECIALIZED_OCCUPATION_NAME'].str.contains('analyst', case=False, na=False)]\nfinal_df[['STATE_NAME', 'NAICS2_NAME', 'NAICS_2022_6', 'LOT_SPECIALIZED_OCCUPATION_NAME']].head(100)\n\n\n\n\n\n\n\n\nSTATE_NAME\nNAICS2_NAME\nNAICS_2022_6\nLOT_SPECIALIZED_OCCUPATION_NAME\n\n\n\n\n2\nTexas\nFinance and Insurance\n524291.0\nData Analyst\n\n\n9\nNew York\nProfessional, Scientific, and Technical Services\n541511.0\nData Analyst\n\n\n10\nCalifornia\nWholesale Trade\n423830.0\nData Analyst\n\n\n15\nMassachusetts\nEducational Services\n611310.0\nData Analyst\n\n\n28\nMassachusetts\nFinance and Insurance\n522320.0\nBusiness Intelligence Analyst\n\n\n...\n...\n...\n...\n...\n\n\n462\nTexas\nEducational Services\n611210.0\nGeneral ERP Analyst / Consultant\n\n\n465\nTexas\nFinance and Insurance\n522320.0\nBusiness Analyst (General)\n\n\n472\nTexas\nInformation\n518210.0\nSAP Analyst / Admin\n\n\n473\nTexas\nProfessional, Scientific, and Technical Services\n541511.0\nOracle Consultant / Analyst\n\n\n487\nNew York\nAdministrative and Support and Waste Managemen...\n561320.0\nData Analyst\n\n\n\n\n100 rows Ã 4 columns\n\n\n\n\n# Filter for Boston, MA and Austin, TX\nselected_state = ['California', 'Florida', 'Massachusetts', 'Texas', 'New York']\nfiltered_df = df[df['STATE_NAME'].isin(selected_state)]\n\n\n# Further filter for NAICS_2022_6 = 518210 and show relevant columns\nfinal_df = filtered_df[filtered_df['LOT_SPECIALIZED_OCCUPATION_NAME'].str.contains('analyst', case=False, na=False)]\nfinal_df[['STATE_NAME', 'NAICS2_NAME', 'NAICS_2022_6', 'LOT_SPECIALIZED_OCCUPATION_NAME']].head(100)\n\n\n\n\n\n\n\n\nSTATE_NAME\nNAICS2_NAME\nNAICS_2022_6\nLOT_SPECIALIZED_OCCUPATION_NAME\n\n\n\n\n2\nTexas\nFinance and Insurance\n524291.0\nData Analyst\n\n\n9\nNew York\nProfessional, Scientific, and Technical Services\n541511.0\nData Analyst\n\n\n10\nCalifornia\nWholesale Trade\n423830.0\nData Analyst\n\n\n15\nMassachusetts\nEducational Services\n611310.0\nData Analyst\n\n\n28\nMassachusetts\nFinance and Insurance\n522320.0\nBusiness Intelligence Analyst\n\n\n...\n...\n...\n...\n...\n\n\n462\nTexas\nEducational Services\n611210.0\nGeneral ERP Analyst / Consultant\n\n\n465\nTexas\nFinance and Insurance\n522320.0\nBusiness Analyst (General)\n\n\n472\nTexas\nInformation\n518210.0\nSAP Analyst / Admin\n\n\n473\nTexas\nProfessional, Scientific, and Technical Services\n541511.0\nOracle Consultant / Analyst\n\n\n487\nNew York\nAdministrative and Support and Waste Managemen...\n561320.0\nData Analyst\n\n\n\n\n100 rows Ã 4 columns\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n#read files\nfile_path = \"./data/lightcast_job_postings.csv\"\ndf = pd.read_csv(file_path)\n\n/tmp/ipykernel_1690/1191241380.py:3: DtypeWarning:\n\nColumns (19,30) have mixed types. Specify dtype option on import or set low_memory=False.\n\n\n\n\nprint(df['REMOTE_TYPE_NAME'].value_counts(dropna=False).head(10))\n\nREMOTE_TYPE_NAME\n[None]           56570\nRemote           12497\nHybrid Remote     2260\nNot Remote        1127\nNaN                 44\nName: count, dtype: int64\n\n\n\n# Step 1: Standardize formatting\ndf['REMOTE_TYPE_NAME'] = (\n    df['REMOTE_TYPE_NAME']\n    .astype(str)\n    .str.strip()\n    .str.title()\n    .replace({'None': pd.NA, 'Nan': pd.NA})\n)\n\n\n# Step 2: Fill missing or ambiguous entries with 'Not Remote'\ndf['REMOTE_TYPE_NAME'] = df['REMOTE_TYPE_NAME'].fillna('Not Remote')\ndf.loc[df['REMOTE_TYPE_NAME'] == \"[None]\", 'REMOTE_TYPE_NAME'] = \"Not Remote\"\nprint(df['REMOTE_TYPE_NAME'].value_counts(dropna=False).head(10))\n\nREMOTE_TYPE_NAME\nNot Remote       57741\nRemote           12497\nHybrid Remote     2260\nName: count, dtype: int64\n\n\n\n# Convert all values to strings and strip whitespace\ndf['REMOTE_TYPE_NAME'] = df['REMOTE_TYPE_NAME'].astype(str).str.strip()\n\n\n# Apply new classification logic\ndf['REMOTE_BINARY'] = df['REMOTE_TYPE_NAME'].apply(\n    lambda x: 1 if x in ['Remote', 'Hybrid Remote'] else 0\n)\n\n\n\n# Check result\nprint(df['REMOTE_TYPE_NAME'].value_counts())\nprint(\"\\nBinary classification:\")\nprint(df['REMOTE_BINARY'].value_counts())\n\nREMOTE_TYPE_NAME\nNot Remote       57741\nRemote           12497\nHybrid Remote     2260\nName: count, dtype: int64\n\nBinary classification:\nREMOTE_BINARY\n0    57741\n1    14757\nName: count, dtype: int64\n\n\n\nimport pandas as pd\nimport numpy as np\n\n# Ensure salary columns are numeric and handle missing values\ndf['SALARY_FROM'] = pd.to_numeric(df['SALARY_FROM'], errors='coerce').replace(0, np.nan)\ndf['SALARY_TO'] = pd.to_numeric(df['SALARY_TO'], errors='coerce').replace(0, np.nan)\n\n# Calculate average salary (mean of SALARY_FROM and SALARY_TO)\ndf['AVERAGE_SALARY'] = df[['SALARY_FROM', 'SALARY_TO']].mean(axis=1)\n\n# Drop rows with missing values in AVERAGE_SALARY, REMOTE_TYPE_NAME, or STATE_NAME\ndf_salary = df.dropna(subset=['AVERAGE_SALARY', 'REMOTE_TYPE_NAME', 'STATE_NAME'])\n\n# Group by state and remote type, then calculate average salary\navg_salary_by_state_remote = df_salary.groupby(['STATE_NAME', 'REMOTE_TYPE_NAME'])['AVERAGE_SALARY'].mean().reset_index()\n\n# Round the results for easier reading\navg_salary_by_state_remote['AVERAGE_SALARY'] = avg_salary_by_state_remote['AVERAGE_SALARY'].round(2)\n\n# Show results\nprint(avg_salary_by_state_remote)\n\n    STATE_NAME REMOTE_TYPE_NAME  AVERAGE_SALARY\n0      Alabama    Hybrid Remote        95779.32\n1      Alabama       Not Remote       110741.22\n2      Alabama           Remote       112252.79\n3       Alaska    Hybrid Remote        85384.29\n4       Alaska       Not Remote        91767.29\n..         ...              ...             ...\n147  Wisconsin       Not Remote       114918.55\n148  Wisconsin           Remote       104307.56\n149    Wyoming    Hybrid Remote       221727.00\n150    Wyoming       Not Remote       106695.18\n151    Wyoming           Remote       111982.65\n\n[152 rows x 3 columns]\n\n\n\ndf = df.merge(avg_salary_by_state_remote,\n              on=['STATE_NAME', 'REMOTE_TYPE_NAME'],\n              how='left')\n\n\ndf = df.merge(\n    avg_salary_by_state_remote,\n    on=['STATE_NAME', 'REMOTE_TYPE_NAME'],\n    how='left',\n    suffixes=('', '_STATE_REMOTE_AVG')\n)\n\n\n[col for col in df.columns if 'AVG' in col or 'SALARY' in col]\n\n['SALARY',\n 'SALARY_TO',\n 'SALARY_FROM',\n 'AVERAGE_SALARY_x',\n 'AVERAGE_SALARY_y',\n 'AVERAGE_SALARY']\n\n\n\ndf = df.rename(columns={'AVERAGE_SALARY_y': 'AVERAGE_SALARY_STATE_REMOTE_AVG'})\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import (accuracy_score, f1_score, confusion_matrix,\n                             classification_report, precision_score,\n                             recall_score, balanced_accuracy_score)\nfrom sklearn.inspection import permutation_importance\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Remove duplicate column names across full dataframe\ndf = df.loc[:, ~df.columns.duplicated()]\n\n\ndf['AVG_YEARS_EXPERIENCE'] = (df['MIN_YEARS_EXPERIENCE'] + df['MAX_YEARS_EXPERIENCE']) / 2\ndf['EXP_SPREAD'] = df['MAX_YEARS_EXPERIENCE'] - df['MIN_YEARS_EXPERIENCE']\n\n\n\ndf = df.drop(columns=['MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE'])\n\n\nnum_feats = [\n    'AVG_YEARS_EXPERIENCE',\n    'AVERAGE_SALARY_STATE_REMOTE_AVG',\n    'IS_INTERNSHIP'\n]\n\ncat_feats = [\n    'STATE_NAME',\n    'NAICS_2022_2_NAME',\n    'EDUCATION_LEVELS_NAME',\n    'COMMON_SKILLS_NAME',\n    'SOFTWARE_SKILLS_NAME',\n    'TITLE_CLEAN'\n    \n]\n\nX = df[num_feats + cat_feats]\ny = df['REMOTE_BINARY']\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n# Preprocessing\npreprocess = ColumnTransformer(transformers=[\n    (\"num\", StandardScaler(), num_feats),\n    (\"cat\", OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_feats)\n])\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n# Preprocessing step\npreprocess = ColumnTransformer(transformers=[\n    (\"num\", StandardScaler(), num_feats),\n    (\"cat\", OneHotEncoder(handle_unknown='ignore', max_categories=500, sparse_output=False), cat_feats)\n])\nclf = RandomForestClassifier(random_state=42, class_weight='balanced')\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\n\nrf = RandomForestClassifier(\n    n_estimators=200,\n    max_depth=None,\n    class_weight='balanced',\n    random_state=42,\n    n_jobs=-1\n)\n\npipe = Pipeline(steps=[\n    ('prep', preprocess),\n    ('model', rf)\n])\n\n\npipe.fit(X_train, y_train)\n\nPipeline(steps=[('prep',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['AVG_YEARS_EXPERIENCE',\n                                                   'AVERAGE_SALARY_STATE_REMOTE_AVG',\n                                                   'IS_INTERNSHIP']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                max_categories=500,\n                                                                sparse_output=False),\n                                                  ['STATE_NAME',\n                                                   'NAICS_2022_2_NAME',\n                                                   'EDUCATION_LEVELS_NAME',\n                                                   'COMMON_SKILLS_NAME',\n                                                   'SOFTWARE_SKILLS_NAME',\n                                                   'TITLE_CLEAN'])])),\n                ('model',\n                 RandomForestClassifier(class_weight='balanced',\n                                        n_estimators=200, n_jobs=-1,\n                                        random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('prep',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['AVG_YEARS_EXPERIENCE',\n                                                   'AVERAGE_SALARY_STATE_REMOTE_AVG',\n                                                   'IS_INTERNSHIP']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                max_categories=500,\n                                                                sparse_output=False),\n                                                  ['STATE_NAME',\n                                                   'NAICS_2022_2_NAME',\n                                                   'EDUCATION_LEVELS_NAME',\n                                                   'COMMON_SKILLS_NAME',\n                                                   'SOFTWARE_SKILLS_NAME',\n                                                   'TITLE_CLEAN'])])),\n                ('model',\n                 RandomForestClassifier(class_weight='balanced',\n                                        n_estimators=200, n_jobs=-1,\n                                        random_state=42))]) prep: ColumnTransformer?Documentation for prep: ColumnTransformerColumnTransformer(transformers=[('num', StandardScaler(),\n                                 ['AVG_YEARS_EXPERIENCE',\n                                  'AVERAGE_SALARY_STATE_REMOTE_AVG',\n                                  'IS_INTERNSHIP']),\n                                ('cat',\n                                 OneHotEncoder(handle_unknown='ignore',\n                                               max_categories=500,\n                                               sparse_output=False),\n                                 ['STATE_NAME', 'NAICS_2022_2_NAME',\n                                  'EDUCATION_LEVELS_NAME', 'COMMON_SKILLS_NAME',\n                                  'SOFTWARE_SKILLS_NAME', 'TITLE_CLEAN'])]) num['AVG_YEARS_EXPERIENCE', 'AVERAGE_SALARY_STATE_REMOTE_AVG', 'IS_INTERNSHIP'] StandardScaler?Documentation for StandardScalerStandardScaler() cat['STATE_NAME', 'NAICS_2022_2_NAME', 'EDUCATION_LEVELS_NAME', 'COMMON_SKILLS_NAME', 'SOFTWARE_SKILLS_NAME', 'TITLE_CLEAN'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore', max_categories=500, sparse_output=False) RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(class_weight='balanced', n_estimators=200, n_jobs=-1,\n                       random_state=42) \n\n\n\ny_pred = pipe.predict(X_test)\n\n# Classification report and confusion matrix\nprint(classification_report(y_test, y_pred))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\n\n# Additional custom metrics\nprint(\"\\nCustom Metrics:\")\nprint(\"Accuracy:\", round(accuracy_score(y_test, y_pred), 3))\nprint(\"F1 Score:\", round(f1_score(y_test, y_pred), 3))\nprint(\"Precision:\", round(precision_score(y_test, y_pred), 3))\nprint(\"Sensitivity (Recall 1):\", round(recall_score(y_test, y_pred), 3))\nprint(\"Specificity (Recall 0):\", round(\n    recall_score(y_test, y_pred, pos_label=0), 3))\nprint(\"Balanced Accuracy:\", round(balanced_accuracy_score(y_test, y_pred), 3))\n\n\n\n              precision    recall  f1-score   support\n\n           0       0.94      1.00      0.97     11549\n           1       0.99      0.74      0.85      2951\n\n    accuracy                           0.95     14500\n   macro avg       0.97      0.87      0.91     14500\nweighted avg       0.95      0.95      0.94     14500\n\nConfusion Matrix:\n[[11536    13]\n [  774  2177]]\n\nCustom Metrics:\nAccuracy: 0.946\nF1 Score: 0.847\nPrecision: 0.994\nSensitivity (Recall 1): 0.738\nSpecificity (Recall 0): 0.999\nBalanced Accuracy: 0.868\n\n\n\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n\n\n\n\n\n\n\n\n\nrf_model       = pipe.named_steps[\"model\"]          # RandomForestClassifier\nfeature_names  = pipe.named_steps[\"prep\"].get_feature_names_out()\n\nimportances = rf_model.feature_importances_\n\nfeat_imp = (\n    pd.DataFrame({\"Feature\": feature_names, \"Importance\": importances})\n      .sort_values(by=\"Importance\", ascending=False)\n      .reset_index(drop=True)\n)\n\nprint(\"\\nTop 9 â Tree-based Importances\")\nprint(feat_imp.head(9).to_string(index=False))\n\n\nTop 9 â Tree-based Importances\n                                   Feature  Importance\n      num__AVERAGE_SALARY_STATE_REMOTE_AVG    0.392680\n                 num__AVG_YEARS_EXPERIENCE    0.014525\n                cat__STATE_NAME_California    0.014173\n                     cat__STATE_NAME_Texas    0.011198\n               cat__STATE_NAME_Connecticut    0.010292\ncat__COMMON_SKILLS_NAME_infrequent_sklearn    0.009475\n                cat__STATE_NAME_New Jersey    0.009274\n       cat__TITLE_CLEAN_infrequent_sklearn    0.008570\n                  cat__STATE_NAME_Virginia    0.008235\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ntop_n = 9                     # change to show more/less\nplt.figure(figsize=(8, 6))\nsns.barplot(\n    data=feat_imp.head(top_n),\n    x=\"Importance\", y=\"Feature\",\n    palette=\"crest\"\n)\nplt.title(f\"Top {top_n} Feature Importances (Random Forest)\")\nplt.xlabel(\"Mean Decrease in Impurity\")\nplt.ylabel(\"\")\nplt.tight_layout()\nplt.show()\n\n/tmp/ipykernel_1690/590212382.py:6: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport plotly.express as px\n\n# Step 1: Create state abbreviation mapping\nus_state_abbrev = {\n    'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR',\n    'California': 'CA', 'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE',\n    'Florida': 'FL', 'Georgia': 'GA', 'Hawaii': 'HI', 'Idaho': 'ID',\n    'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA', 'Kansas': 'KS',\n    'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD',\n    'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS',\n    'Missouri': 'MO', 'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV',\n    'New Hampshire': 'NH', 'New Jersey': 'NJ', 'New Mexico': 'NM', 'New York': 'NY',\n    'North Carolina': 'NC', 'North Dakota': 'ND', 'Ohio': 'OH', 'Oklahoma': 'OK',\n    'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI', 'South Carolina': 'SC',\n    'South Dakota': 'SD', 'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT',\n    'Vermont': 'VT', 'Virginia': 'VA', 'Washington': 'WA', 'West Virginia': 'WV',\n    'Wisconsin': 'WI', 'Wyoming': 'WY', 'District of Columbia': 'DC'\n}\n\n# Step 2: Map state names to abbreviations\ndf['STATE_ABBR'] = df['STATE_NAME'].map(us_state_abbrev)\n\n# Step 3: Group by state and compute metrics\nchoropleth_data = df.groupby('STATE_ABBR').agg(\n    remote_ratio=('REMOTE_BINARY', 'mean'),\n    avg_salary=('AVERAGE_SALARY_STATE_REMOTE_AVG', 'mean'),\n    avg_experience=('AVG_YEARS_EXPERIENCE', 'mean'),\n    job_count=('STATE_NAME', 'count')\n).reset_index()\n\n# Step 4: Define custom green scale (start from light, move to #1aab89)\ncustom_green_scale = [\n    [0, \"#e0f7f1\"],     # light mint\n    [0.5, \"#70d8b5\"],   # mid-green\n    [1, \"#1aab89\"]      # deep teal green\n]\n\n# Step 5: Create the choropleth with custom green\nfig = px.choropleth(\n    data_frame=choropleth_data,\n    locations='STATE_ABBR',\n    locationmode=\"USA-states\",\n    color='remote_ratio',\n    color_continuous_scale=custom_green_scale,\n    scope=\"usa\",\n    labels={'remote_ratio': 'Remote Job Ratio'},\n    hover_data={\n        'remote_ratio': ':.2f',\n        'avg_salary': ':.0f',\n        'avg_experience': ':.1f',\n        'job_count': True\n    },\n    title='Remote Job Ratio by State (Custom Green), Avg Salary & Experience in Hover'\n)\n\nfig.update_layout(margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0})\nfig.write_html(\"./figures/state_remote_job_ratio.html\")\n\nfig.show()\n\n\n\n                                                    \n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Step 1: Ensure 'POSTED' is in datetime format and create Year-Month\ndf['POSTED'] = pd.to_datetime(df['POSTED'])\ndf['POSTED_YM'] = df['POSTED'].dt.to_period('M').astype(str)\n\n\nindustry_trends = (\n    df.groupby(['NAICS_2022_2_NAME', 'POSTED_YM'])['REMOTE_BINARY']\n    .mean()\n    .reset_index(name='REMOTE_RATIO')\n)\n\n\n# Step 3: Select top 5 industries with highest overall average remote ratio\ntop_industries = (\n    industry_trends.groupby('NAICS_2022_2_NAME')['REMOTE_RATIO']\n    .mean()\n    .sort_values(ascending=False)\n    .head(5)\n    .index.tolist()\n)\nfiltered_trends = industry_trends[industry_trends['NAICS_2022_2_NAME'].isin(top_industries)]\n\n\nimport plotly.express as px\n\nfig = px.line(\n    filtered_trends,\n    x='POSTED_YM',\n    y='REMOTE_RATIO',\n    color='NAICS_2022_2_NAME',\n    markers=True,\n    title=\"Top Industries: Remote Job Trends Over Time\"\n)\n\nfig.update_layout(\n    xaxis_title=\"Posted Month\",\n    yaxis_title=\"Remote Job Ratio\",\n    legend_title=\"Industry\",\n    legend=dict(x=1.02, y=1, bordercolor=\"Black\"),\n    margin=dict(l=40, r=40, t=60, b=40),\n    width=1000,\n    height=500\n)\n\nfig.update_xaxes(tickangle=45)\nfig.write_html(\"./figures/remote_job_over_time.html\")\nfig.show()\n\n                                                    \n\n\n\n#Groupby industry + month and calculate both:\nindustry_month_stats = df.groupby(['NAICS_2022_2_NAME', 'POSTED_YM']).agg(\n    TOTAL_JOBS=('REMOTE_BINARY', 'count'),\n    REMOTE_RATIO=('REMOTE_BINARY', 'mean')\n).reset_index()\n\n\njob_count = df.groupby(['NAICS_2022_2_NAME', 'POSTED_YM']).size().reset_index(name='JOB_COUNT')\n\n\nremote_ratio = df.groupby(['NAICS_2022_2_NAME', 'POSTED_YM'])['REMOTE_BINARY'].mean().reset_index(name='REMOTE_RATIO')\n\n\nindustry_month_stats = pd.merge(remote_ratio, job_count, on=['NAICS_2022_2_NAME', 'POSTED_YM'])\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Choose 2â3 industries to plot (or loop one at a time)\nselected_industries = [\n    'Administrative and Support and Waste Management and Remediation Services',\n    'Arts, Entertainment, and Recreation',\n    'Finance and Insurance',\n    'Real Estate and Rental and Leasing',\n    'Utilities'\n]\n\n\n\nfor industry in selected_industries:\n    data = industry_month_stats[industry_month_stats['NAICS_2022_2_NAME'] == industry]  \n\n    fig, ax1 = plt.subplots(figsize=(10, 4))\n\n    # Plot remote ratio\n    ax1.plot(data['POSTED_YM'], data['REMOTE_RATIO'], color='tab:blue', marker='o')\n    ax1.set_xlabel('Month')\n    ax1.set_ylabel('Remote Job Ratio', color='tab:blue')\n    ax1.tick_params(axis='y', labelcolor='tab:blue')\n    ax1.set_title(f\"Remote Job Ratio & Volume Over Time: {industry}\")\n\n    # Plot job count on secondary y-axis\n    ax2 = ax1.twinx()\n    ax2.bar(data['POSTED_YM'], data['JOB_COUNT'], color='tab:gray', alpha=0.3)\n    ax2.set_ylabel('Total Job Postings', color='gray')\n    ax2.tick_params(axis='y', labelcolor='gray')\n\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.savefig(\"./figures/Remote_jobs_\"+str(industry)+\".jpg\", dpi=300)\n    plt.show()"
  },
  {
    "objectID": "skill_gap.html",
    "href": "skill_gap.html",
    "title": "Skill Gap Analysis",
    "section": "",
    "text": "import findspark\nfindspark.init()\n\nfrom pyspark.sql import SparkSession\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\nimport numpy as np\n\nnp.random.seed(42)\n\npio.renderers.default = \"notebook\"\n\n# Initialize Spark Session\nspark = SparkSession.builder.appName(\"LightcastData\").getOrCreate()\n\n# Load Data\ndf = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").option(\"escape\", \"\\\"\").csv(\"./data/lightcast_job_postings.csv\")\n\n\ndf.createOrReplaceTempView(\"jobs\")\n\n\nsoftware_skill_counts_by_type = spark.sql(\"\"\"\n    SELECT software_skills_name, COUNT(*) AS count\n    FROM jobs\n    WHERE LOWER(title_name) LIKE '%analyst%'\n       OR LOWER(title_name) LIKE '%analysis%'\n       OR LOWER(title_name) LIKE '%analytics%'\n    GROUP BY software_skills_name\n    ORDER BY count DESC\n    LIMIT 10\n\"\"\")\nsoftware_skill_counts_by_type.show(truncate=False)\n\n\nskill_counts_by_type = spark.sql(\"\"\"\n    SELECT skills_name, COUNT(*) AS count\n    FROM jobs\n    WHERE LOWER(title_name) LIKE '%analyst%'\n    OR LOWER(title_name) LIKE '%analysis%'\n    OR LOWER(title_name) LIKE '%analytics%'\n    GROUP BY skills_name\n    ORDER BY count DESC\n    LIMIT 10\n\"\"\")\nskill_counts_by_type.show(truncate=False)\n\n\nimport pandas as pd\n\nskills_data = {\n    \"Name\": [\"Alyssa\", \"Adam\", \"Yihan\"],\n    \"Microsoft Office\": [4, 5, 3],\n    \"Dashboard\": [3, 3, 1],\n    \"SQL\": [2, 2, 2],\n    \"OneStream\": [1, 1, 1],\n    \"Cloud Computing\": [2, 2, 2]\n}\n\ndf_skills = pd.DataFrame(skills_data)\ndf_skills.set_index(\"Name\", inplace=True)\ndf_skills\n\n\nimport pandas as pd\nimport plotly.graph_objects as go\n\n# Your data\nskills_data = {\n    \"Name\": [\"Alyssa\", \"Adam\", \"Yihan\"],\n    \"Microsoft Office\": [4, 5, 3],\n    \"Dashboard\": [3, 3, 1],\n    \"SQL\": [2, 2, 2],\n    \"OneStream\": [0, 0, 0],\n    \"Cloud Computing\": [2, 2, 2]\n}\n\n# Create DataFrame\ndf_skills = pd.DataFrame(skills_data)\ndf_skills.set_index(\"Name\", inplace=True)\n\n# Get skill categories\ncategories = df_skills.columns.tolist()\n\n# Create Plotly radar chart\nfig = go.Figure()\n\nfor name in df_skills.index:\n    values = df_skills.loc[name].tolist()\n    values += values[:1]  # close the radar loop\n\n    fig.add_trace(go.Scatterpolar(\n        r=values,\n        theta=categories + [categories[0]],  # close the loop\n        fill='toself',\n        name=name\n    ))\n\n# Customize layout\nfig.update_layout(\n    polar=dict(\n        radialaxis=dict(\n            visible=True,\n            range=[0, 6]\n        )),\n    showlegend=True,\n    title=\"Team Skillset Levels\"\n)\n\n\n\nfig.write_html(\"./figures/skill_gap.html\", include_plotlyjs='cdn')\n\n#https://plotly.com/python/radar-chart/\n\n\nThe radar chart above displays each individual evaluation of our skills for the top five skills on demand for analyst roles.\n\nRecomendations\nGiven our analysis above, we have decided to focus on some of the key actions and learning goals that each of us can take in order to further our chances of landing a high quality position in our chosen industry.\n\nSQL\n\nBeginner\nWe recommend using tools such as SQLBolt to begin developing a foundational understanding of basic syntax, queries, and selecting columns from datasets. This will build familiarity with the program itself and develop a confidence in import and simple manipulation of data.\n\n\nIntermediate\nNext, we will incorporate real-world data sets (ex. Kaggle) to begin creating analysis. As an example, you could utilize sales data, new customers, inventory levels, certain trends over time, etc. Utilizng applications such as LinkedIn Learning or Coursera can assist with this.\n\n\nAdvanced\nAt this stage, we will aim for constructing pipelines that are sufficient from beginning to end and that integrate a prouction quality result. As a final step, DataLemur provides candidates with interview questions that correspond to SQL and have been confirmed by various companies such as Amazon, Google, etc.\n\n\n\nOneStream\nIt is important to note that this is a private software application so receiving quality training will be difficult without being sponsored by a company.\n\nBeginner\nYoutube is the best resource for beginning to familiarize yourself with the main functionality and goals of the software. The company does have their own channel, so it would be advised to watch their videos and learn more about what the application does and how it works.\n\n\nIntermediate\nConsider purchasing an online course through Udemy or Keyteach. While this does require personal spending, it would be the easiest way to gain an understanding without requiring an official license to operate the software. This would help to practice working with the application and exploring key concepts.\n\n\nAdvanced\nThis stage would be difficult, because you would need access to the software in order to achieve an advanced level. If you are employed (especially in the financial services industry) consider asking IT for access. There you can work more on complicated structures such as macros and visualization.\n\n\n\nAWS\n\nBeginner\nAWS offers free tier accounts where users can begin learning the basic structure of the applications that are offered. Additionally, there are various free courses that are offered by AWS Academy.\n\n\nIntermediate\nIncorporate real world data within AWS to create a storage structure, run queries, and visualize data. Additionally, you can reference Youtube or LinkedIn learning to learn more about EC2 capabilities and Quicksight (for visualization).\n\n\nAdvanced\nConsider learning more about best practices and cost structuring, which will be crucial components of AWS design in a real world company. The goal of this is to begin optimizing your pipelines to make sure that they are production quality. To achieve this, consider completing the AWS Certified Data Analytics course.\n\n\n\nPower BI\n\nBeginner\nAt the beginner level it is important to understand the basic UI interface of the application, such as importing data and generating visuals (bar charts, line charts, cards, etc.). Microsoft Learn can help to achieve this with their beginner course.\n\n\nIntermediate\nAt this point, you should have the ability to use more complicated processes such as data cleaning, DAX functions, and establishing relationships between multiple datasets. Again, Microsoft Learn has courses called âDesign Power BI reportsâ and âConfigure Power BI report filtersâ that will help achieve this competency.\n\n\nAdvanced\nBy now, you should be able to connect your dashboard to other applications like SQL or APIs. Your dashboards should have multiple pages, which include dynamic formatting and the ability to automatically refresh.\n\n\n\nOffice 365\nThis will be broken down into 3 of the most common applications with an emphasis on Excel. Microsoft 365 is the recommended training tool, as it has learning courses available for all levels.\n\nBeginner\nExcel: Standard formulas are used such as SUMIF and VLOOKUP. There is familarity with pivot charts and conditional formatting. Outlook: User has the ability to schedule meetings and establish designated email folders. PowerPoint: There is an understanding of presentation structure, as well as formatting and placement of text and visuals.\n\n\nIntermediate\nExcel: Power Query, Data Validation tabs, Index formulas, KPI dashboards with slicers. Outlook: Creating shared calendars and group inboxes. PowerPoint: Linking visuals from other applications, such as Excel.\n\n\nAdvanced\nExcel: VBA and macros combined with dynamic visuals. PowerPoint: Creating custom templates to align with a companyâs brand."
  },
  {
    "objectID": "geographic_analysis.html",
    "href": "geographic_analysis.html",
    "title": "Geographic Analysis",
    "section": "",
    "text": "Introduction\nbla balal\n\n\nimport findspark\nfindspark.init()\n\nfrom pyspark.sql import SparkSession\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\nimport numpy as np\n\nnp.random.seed(42)\n\npio.renderers.default = \"notebook\"\n\n# Initialize Spark Session\nspark = SparkSession.builder.appName(\"LightcastData\").getOrCreate()\n\n# Load Data\ndf = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").option(\"escape\", \"\\\"\").csv(\"./data/lightcast_job_postings.csv\")\n\n# Show Schema and Sample Data\nprint(\"---This is Diagnostic check, No need to print it in the final doc---\")\n\ndf.printSchema() # comment this line when rendering the submission\ndf.show(5)\n\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/06/27 16:41:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n                                                                                \n\n\n---This is Diagnostic check, No need to print it in the final doc---\nroot\n |-- ID: string (nullable = true)\n |-- LAST_UPDATED_DATE: string (nullable = true)\n |-- LAST_UPDATED_TIMESTAMP: timestamp (nullable = true)\n |-- DUPLICATES: integer (nullable = true)\n |-- POSTED: string (nullable = true)\n |-- EXPIRED: string (nullable = true)\n |-- DURATION: integer (nullable = true)\n |-- SOURCE_TYPES: string (nullable = true)\n |-- SOURCES: string (nullable = true)\n |-- URL: string (nullable = true)\n |-- ACTIVE_URLS: string (nullable = true)\n |-- ACTIVE_SOURCES_INFO: string (nullable = true)\n |-- TITLE_RAW: string (nullable = true)\n |-- BODY: string (nullable = true)\n |-- MODELED_EXPIRED: string (nullable = true)\n |-- MODELED_DURATION: integer (nullable = true)\n |-- COMPANY: integer (nullable = true)\n |-- COMPANY_NAME: string (nullable = true)\n |-- COMPANY_RAW: string (nullable = true)\n |-- COMPANY_IS_STAFFING: boolean (nullable = true)\n |-- EDUCATION_LEVELS: string (nullable = true)\n |-- EDUCATION_LEVELS_NAME: string (nullable = true)\n |-- MIN_EDULEVELS: integer (nullable = true)\n |-- MIN_EDULEVELS_NAME: string (nullable = true)\n |-- MAX_EDULEVELS: integer (nullable = true)\n |-- MAX_EDULEVELS_NAME: string (nullable = true)\n |-- EMPLOYMENT_TYPE: integer (nullable = true)\n |-- EMPLOYMENT_TYPE_NAME: string (nullable = true)\n |-- MIN_YEARS_EXPERIENCE: integer (nullable = true)\n |-- MAX_YEARS_EXPERIENCE: integer (nullable = true)\n |-- IS_INTERNSHIP: boolean (nullable = true)\n |-- SALARY: integer (nullable = true)\n |-- REMOTE_TYPE: integer (nullable = true)\n |-- REMOTE_TYPE_NAME: string (nullable = true)\n |-- ORIGINAL_PAY_PERIOD: string (nullable = true)\n |-- SALARY_TO: integer (nullable = true)\n |-- SALARY_FROM: integer (nullable = true)\n |-- LOCATION: string (nullable = true)\n |-- CITY: string (nullable = true)\n |-- CITY_NAME: string (nullable = true)\n |-- COUNTY: integer (nullable = true)\n |-- COUNTY_NAME: string (nullable = true)\n |-- MSA: integer (nullable = true)\n |-- MSA_NAME: string (nullable = true)\n |-- STATE: integer (nullable = true)\n |-- STATE_NAME: string (nullable = true)\n |-- COUNTY_OUTGOING: integer (nullable = true)\n |-- COUNTY_NAME_OUTGOING: string (nullable = true)\n |-- COUNTY_INCOMING: integer (nullable = true)\n |-- COUNTY_NAME_INCOMING: string (nullable = true)\n |-- MSA_OUTGOING: integer (nullable = true)\n |-- MSA_NAME_OUTGOING: string (nullable = true)\n |-- MSA_INCOMING: integer (nullable = true)\n |-- MSA_NAME_INCOMING: string (nullable = true)\n |-- NAICS2: integer (nullable = true)\n |-- NAICS2_NAME: string (nullable = true)\n |-- NAICS3: integer (nullable = true)\n |-- NAICS3_NAME: string (nullable = true)\n |-- NAICS4: integer (nullable = true)\n |-- NAICS4_NAME: string (nullable = true)\n |-- NAICS5: integer (nullable = true)\n |-- NAICS5_NAME: string (nullable = true)\n |-- NAICS6: integer (nullable = true)\n |-- NAICS6_NAME: string (nullable = true)\n |-- TITLE: string (nullable = true)\n |-- TITLE_NAME: string (nullable = true)\n |-- TITLE_CLEAN: string (nullable = true)\n |-- SKILLS: string (nullable = true)\n |-- SKILLS_NAME: string (nullable = true)\n |-- SPECIALIZED_SKILLS: string (nullable = true)\n |-- SPECIALIZED_SKILLS_NAME: string (nullable = true)\n |-- CERTIFICATIONS: string (nullable = true)\n |-- CERTIFICATIONS_NAME: string (nullable = true)\n |-- COMMON_SKILLS: string (nullable = true)\n |-- COMMON_SKILLS_NAME: string (nullable = true)\n |-- SOFTWARE_SKILLS: string (nullable = true)\n |-- SOFTWARE_SKILLS_NAME: string (nullable = true)\n |-- ONET: string (nullable = true)\n |-- ONET_NAME: string (nullable = true)\n |-- ONET_2019: string (nullable = true)\n |-- ONET_2019_NAME: string (nullable = true)\n |-- CIP6: string (nullable = true)\n |-- CIP6_NAME: string (nullable = true)\n |-- CIP4: string (nullable = true)\n |-- CIP4_NAME: string (nullable = true)\n |-- CIP2: string (nullable = true)\n |-- CIP2_NAME: string (nullable = true)\n |-- SOC_2021_2: string (nullable = true)\n |-- SOC_2021_2_NAME: string (nullable = true)\n |-- SOC_2021_3: string (nullable = true)\n |-- SOC_2021_3_NAME: string (nullable = true)\n |-- SOC_2021_4: string (nullable = true)\n |-- SOC_2021_4_NAME: string (nullable = true)\n |-- SOC_2021_5: string (nullable = true)\n |-- SOC_2021_5_NAME: string (nullable = true)\n |-- LOT_CAREER_AREA: integer (nullable = true)\n |-- LOT_CAREER_AREA_NAME: string (nullable = true)\n |-- LOT_OCCUPATION: integer (nullable = true)\n |-- LOT_OCCUPATION_NAME: string (nullable = true)\n |-- LOT_SPECIALIZED_OCCUPATION: integer (nullable = true)\n |-- LOT_SPECIALIZED_OCCUPATION_NAME: string (nullable = true)\n |-- LOT_OCCUPATION_GROUP: integer (nullable = true)\n |-- LOT_OCCUPATION_GROUP_NAME: string (nullable = true)\n |-- LOT_V6_SPECIALIZED_OCCUPATION: integer (nullable = true)\n |-- LOT_V6_SPECIALIZED_OCCUPATION_NAME: string (nullable = true)\n |-- LOT_V6_OCCUPATION: integer (nullable = true)\n |-- LOT_V6_OCCUPATION_NAME: string (nullable = true)\n |-- LOT_V6_OCCUPATION_GROUP: integer (nullable = true)\n |-- LOT_V6_OCCUPATION_GROUP_NAME: string (nullable = true)\n |-- LOT_V6_CAREER_AREA: integer (nullable = true)\n |-- LOT_V6_CAREER_AREA_NAME: string (nullable = true)\n |-- SOC_2: string (nullable = true)\n |-- SOC_2_NAME: string (nullable = true)\n |-- SOC_3: string (nullable = true)\n |-- SOC_3_NAME: string (nullable = true)\n |-- SOC_4: string (nullable = true)\n |-- SOC_4_NAME: string (nullable = true)\n |-- SOC_5: string (nullable = true)\n |-- SOC_5_NAME: string (nullable = true)\n |-- LIGHTCAST_SECTORS: string (nullable = true)\n |-- LIGHTCAST_SECTORS_NAME: string (nullable = true)\n |-- NAICS_2022_2: integer (nullable = true)\n |-- NAICS_2022_2_NAME: string (nullable = true)\n |-- NAICS_2022_3: integer (nullable = true)\n |-- NAICS_2022_3_NAME: string (nullable = true)\n |-- NAICS_2022_4: integer (nullable = true)\n |-- NAICS_2022_4_NAME: string (nullable = true)\n |-- NAICS_2022_5: integer (nullable = true)\n |-- NAICS_2022_5_NAME: string (nullable = true)\n |-- NAICS_2022_6: integer (nullable = true)\n |-- NAICS_2022_6_NAME: string (nullable = true)\n\n\n\n25/06/27 16:41:39 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n                                                                                \n\n\n+--------------------+-----------------+----------------------+----------+--------+---------+--------+--------------------+--------------------+--------------------+-----------+-------------------+--------------------+--------------------+---------------+----------------+--------+--------------------+-----------+-------------------+----------------+---------------------+-------------+-------------------+-------------+------------------+---------------+--------------------+--------------------+--------------------+-------------+------+-----------+----------------+-------------------+---------+-----------+--------------------+--------------------+-------------+------+--------------+-----+--------------------+-----+----------+---------------+--------------------+---------------+--------------------+------------+--------------------+------------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+----------+--------------------+----------+---------------+----------+---------------+---------------+--------------------+--------------+--------------------+--------------------------+-------------------------------+--------------------+-------------------------+-----------------------------+----------------------------------+-----------------+----------------------+-----------------------+----------------------------+------------------+-----------------------+-------+--------------------+-------+--------------------+-------+---------------+-------+---------------+-----------------+----------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+\n|                  ID|LAST_UPDATED_DATE|LAST_UPDATED_TIMESTAMP|DUPLICATES|  POSTED|  EXPIRED|DURATION|        SOURCE_TYPES|             SOURCES|                 URL|ACTIVE_URLS|ACTIVE_SOURCES_INFO|           TITLE_RAW|                BODY|MODELED_EXPIRED|MODELED_DURATION| COMPANY|        COMPANY_NAME|COMPANY_RAW|COMPANY_IS_STAFFING|EDUCATION_LEVELS|EDUCATION_LEVELS_NAME|MIN_EDULEVELS| MIN_EDULEVELS_NAME|MAX_EDULEVELS|MAX_EDULEVELS_NAME|EMPLOYMENT_TYPE|EMPLOYMENT_TYPE_NAME|MIN_YEARS_EXPERIENCE|MAX_YEARS_EXPERIENCE|IS_INTERNSHIP|SALARY|REMOTE_TYPE|REMOTE_TYPE_NAME|ORIGINAL_PAY_PERIOD|SALARY_TO|SALARY_FROM|            LOCATION|                CITY|    CITY_NAME|COUNTY|   COUNTY_NAME|  MSA|            MSA_NAME|STATE|STATE_NAME|COUNTY_OUTGOING|COUNTY_NAME_OUTGOING|COUNTY_INCOMING|COUNTY_NAME_INCOMING|MSA_OUTGOING|   MSA_NAME_OUTGOING|MSA_INCOMING|   MSA_NAME_INCOMING|NAICS2|         NAICS2_NAME|NAICS3|         NAICS3_NAME|NAICS4|         NAICS4_NAME|NAICS5|         NAICS5_NAME|NAICS6|         NAICS6_NAME|             TITLE|         TITLE_NAME|         TITLE_CLEAN|              SKILLS|         SKILLS_NAME|  SPECIALIZED_SKILLS|SPECIALIZED_SKILLS_NAME|      CERTIFICATIONS| CERTIFICATIONS_NAME|       COMMON_SKILLS|  COMMON_SKILLS_NAME|     SOFTWARE_SKILLS|SOFTWARE_SKILLS_NAME|      ONET|           ONET_NAME| ONET_2019|      ONET_2019_NAME|                CIP6|           CIP6_NAME|                CIP4|           CIP4_NAME|                CIP2|           CIP2_NAME|SOC_2021_2|     SOC_2021_2_NAME|SOC_2021_3|     SOC_2021_3_NAME|SOC_2021_4|SOC_2021_4_NAME|SOC_2021_5|SOC_2021_5_NAME|LOT_CAREER_AREA|LOT_CAREER_AREA_NAME|LOT_OCCUPATION| LOT_OCCUPATION_NAME|LOT_SPECIALIZED_OCCUPATION|LOT_SPECIALIZED_OCCUPATION_NAME|LOT_OCCUPATION_GROUP|LOT_OCCUPATION_GROUP_NAME|LOT_V6_SPECIALIZED_OCCUPATION|LOT_V6_SPECIALIZED_OCCUPATION_NAME|LOT_V6_OCCUPATION|LOT_V6_OCCUPATION_NAME|LOT_V6_OCCUPATION_GROUP|LOT_V6_OCCUPATION_GROUP_NAME|LOT_V6_CAREER_AREA|LOT_V6_CAREER_AREA_NAME|  SOC_2|          SOC_2_NAME|  SOC_3|          SOC_3_NAME|  SOC_4|     SOC_4_NAME|  SOC_5|     SOC_5_NAME|LIGHTCAST_SECTORS|LIGHTCAST_SECTORS_NAME|NAICS_2022_2|   NAICS_2022_2_NAME|NAICS_2022_3|   NAICS_2022_3_NAME|NAICS_2022_4|   NAICS_2022_4_NAME|NAICS_2022_5|   NAICS_2022_5_NAME|NAICS_2022_6|   NAICS_2022_6_NAME|\n+--------------------+-----------------+----------------------+----------+--------+---------+--------+--------------------+--------------------+--------------------+-----------+-------------------+--------------------+--------------------+---------------+----------------+--------+--------------------+-----------+-------------------+----------------+---------------------+-------------+-------------------+-------------+------------------+---------------+--------------------+--------------------+--------------------+-------------+------+-----------+----------------+-------------------+---------+-----------+--------------------+--------------------+-------------+------+--------------+-----+--------------------+-----+----------+---------------+--------------------+---------------+--------------------+------------+--------------------+------------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+----------+--------------------+----------+---------------+----------+---------------+---------------+--------------------+--------------+--------------------+--------------------------+-------------------------------+--------------------+-------------------------+-----------------------------+----------------------------------+-----------------+----------------------+-----------------------+----------------------------+------------------+-----------------------+-------+--------------------+-------+--------------------+-------+---------------+-------+---------------+-----------------+----------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+\n|1f57d95acf4dc67ed...|         9/6/2024|  2024-09-06 20:32:...|         0|6/2/2024| 6/8/2024|       6|   [\\n  \"Company\"\\n]|[\\n  \"brassring.c...|[\\n  \"https://sjo...|         []|               NULL|Enterprise Analys...|31-May-2024\\n\\nEn...|       6/8/2024|               6|  894731|          Murphy USA| Murphy USA|              false|       [\\n  2\\n]| [\\n  \"Bachelor's ...|            2|  Bachelor's degree|         NULL|              NULL|              1|Full-time (&gt; 32 h...|                   2|                   2|        false|  NULL|          0|          [None]|               NULL|     NULL|       NULL|{\\n  \"lat\": 33.20...|RWwgRG9yYWRvLCBBUg==|El Dorado, AR|  5139|     Union, AR|20980|       El Dorado, AR|    5|  Arkansas|           5139|           Union, AR|           5139|           Union, AR|       20980|       El Dorado, AR|       20980|       El Dorado, AR|    44|        Retail Trade|   441|Motor Vehicle and...|  4413|Automotive Parts,...| 44133|Automotive Parts ...|441330|Automotive Parts ...|ET29C073C03D1F86B4|Enterprise Analysts|enterprise analys...|[\\n  \"KS126DB6T06...|[\\n  \"Merchandisi...|[\\n  \"KS126DB6T06...|   [\\n  \"Merchandisi...|                  []|                  []|[\\n  \"KS126706DPF...|[\\n  \"Mathematics...|[\\n  \"KS440W865GC...|[\\n  \"SQL (Progra...|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|[\\n  \"45.0601\",\\n...|[\\n  \"Economics, ...|[\\n  \"45.06\",\\n  ...|[\\n  \"Economics\",...|[\\n  \"45\",\\n  \"27...|[\\n  \"Social Scie...|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231010|Business Intellig...|                  23101011|           General ERP Analy...|                2310|     Business Intellig...|                     23101011|              General ERP Analy...|           231010|  Business Intellig...|                   2310|        Business Intellig...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|        [\\n  7\\n]|  [\\n  \"Artificial ...|          44|        Retail Trade|         441|Motor Vehicle and...|        4413|Automotive Parts,...|       44133|Automotive Parts ...|      441330|Automotive Parts ...|\n|0cb072af26757b6c4...|         8/2/2024|  2024-08-02 17:08:...|         0|6/2/2024| 8/1/2024|    NULL| [\\n  \"Job Board\"\\n]| [\\n  \"maine.gov\"\\n]|[\\n  \"https://job...|         []|               NULL|Oracle Consultant...|Oracle Consultant...|       8/1/2024|            NULL|  133098|Smx Corporation L...|        SMX|               true|      [\\n  99\\n]| [\\n  \"No Educatio...|           99|No Education Listed|         NULL|              NULL|              1|Full-time (&gt; 32 h...|                   3|                   3|        false|  NULL|          1|          Remote|               NULL|     NULL|       NULL|{\\n  \"lat\": 44.31...|    QXVndXN0YSwgTUU=|  Augusta, ME| 23011|  Kennebec, ME|12300|Augusta-Watervill...|   23|     Maine|          23011|        Kennebec, ME|          23011|        Kennebec, ME|       12300|Augusta-Watervill...|       12300|Augusta-Watervill...|    56|Administrative an...|   561|Administrative an...|  5613| Employment Services| 56132|Temporary Help Se...|561320|Temporary Help Se...|ET21DDA63780A7DC09| Oracle Consultants|oracle consultant...|[\\n  \"KS122626T55...|[\\n  \"Procurement...|[\\n  \"KS122626T55...|   [\\n  \"Procurement...|                  []|                  []|                  []|                  []|[\\n  \"BGSBF3F508F...|[\\n  \"Oracle Busi...|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|                  []|                  []|                  []|                  []|                  []|                  []|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231010|Business Intellig...|                  23101012|           Oracle Consultant...|                2310|     Business Intellig...|                     23101012|              Oracle Consultant...|           231010|  Business Intellig...|                   2310|        Business Intellig...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|             NULL|                  NULL|          56|Administrative an...|         561|Administrative an...|        5613| Employment Services|       56132|Temporary Help Se...|      561320|Temporary Help Se...|\n|85318b12b3331fa49...|         9/6/2024|  2024-09-06 20:32:...|         1|6/2/2024| 7/7/2024|      35| [\\n  \"Job Board\"\\n]|[\\n  \"dejobs.org\"\\n]|[\\n  \"https://dej...|         []|               NULL|        Data Analyst|Taking care of pe...|      6/10/2024|               8|39063746|            Sedgwick|   Sedgwick|              false|       [\\n  2\\n]| [\\n  \"Bachelor's ...|            2|  Bachelor's degree|         NULL|              NULL|              1|Full-time (&gt; 32 h...|                   5|                NULL|        false|  NULL|          0|          [None]|               NULL|     NULL|       NULL|{\\n  \"lat\": 32.77...|    RGFsbGFzLCBUWA==|   Dallas, TX| 48113|    Dallas, TX|19100|Dallas-Fort Worth...|   48|     Texas|          48113|          Dallas, TX|          48113|          Dallas, TX|       19100|Dallas-Fort Worth...|       19100|Dallas-Fort Worth...|    52|Finance and Insur...|   524|Insurance Carrier...|  5242|Agencies, Brokera...| 52429|Other Insurance R...|524291|    Claims Adjusting|ET3037E0C947A02404|      Data Analysts|        data analyst|[\\n  \"KS1218W78FG...|[\\n  \"Management\"...|[\\n  \"ESF3939CE1F...|   [\\n  \"Exception R...|[\\n  \"KS683TN76T7...|[\\n  \"Security Cl...|[\\n  \"KS1218W78FG...|[\\n  \"Management\"...|[\\n  \"KS126HY6YLT...|[\\n  \"Microsoft O...|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|                  []|                  []|                  []|                  []|                  []|                  []|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231113|Data / Data Minin...|                  23111310|                   Data Analyst|                2311|     Data Analysis and...|                     23111310|                      Data Analyst|           231113|  Data / Data Minin...|                   2311|        Data Analysis and...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|             NULL|                  NULL|          52|Finance and Insur...|         524|Insurance Carrier...|        5242|Agencies, Brokera...|       52429|Other Insurance R...|      524291|    Claims Adjusting|\n|1b5c3941e54a1889e...|         9/6/2024|  2024-09-06 20:32:...|         1|6/2/2024|7/20/2024|      48| [\\n  \"Job Board\"\\n]|[\\n  \"disabledper...|[\\n  \"https://www...|         []|               NULL|Sr. Lead Data Mgm...|About this role:\\...|      6/12/2024|              10|37615159|         Wells Fargo|Wells Fargo|              false|      [\\n  99\\n]| [\\n  \"No Educatio...|           99|No Education Listed|         NULL|              NULL|              1|Full-time (&gt; 32 h...|                   3|                NULL|        false|  NULL|          0|          [None]|               NULL|     NULL|       NULL|{\\n  \"lat\": 33.44...|    UGhvZW5peCwgQVo=|  Phoenix, AZ|  4013|  Maricopa, AZ|38060|Phoenix-Mesa-Chan...|    4|   Arizona|           4013|        Maricopa, AZ|           4013|        Maricopa, AZ|       38060|Phoenix-Mesa-Chan...|       38060|Phoenix-Mesa-Chan...|    52|Finance and Insur...|   522|Credit Intermedia...|  5221|Depository Credit...| 52211|  Commercial Banking|522110|  Commercial Banking|ET2114E0404BA30075|Management Analysts|sr lead data mgmt...|[\\n  \"KS123QX62QY...|[\\n  \"Exit Strate...|[\\n  \"KS123QX62QY...|   [\\n  \"Exit Strate...|                  []|                  []|[\\n  \"KS7G6NP6R6L...|[\\n  \"Reliability...|[\\n  \"KS4409D76NW...|[\\n  \"SAS (Softwa...|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|                  []|                  []|                  []|                  []|                  []|                  []|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231113|Data / Data Minin...|                  23111310|                   Data Analyst|                2311|     Data Analysis and...|                     23111310|                      Data Analyst|           231113|  Data / Data Minin...|                   2311|        Data Analysis and...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|        [\\n  6\\n]|  [\\n  \"Data Privac...|          52|Finance and Insur...|         522|Credit Intermedia...|        5221|Depository Credit...|       52211|  Commercial Banking|      522110|  Commercial Banking|\n|cb5ca25f02bdf25c1...|        6/19/2024|   2024-06-19 07:00:00|         0|6/2/2024|6/17/2024|      15|[\\n  \"FreeJobBoar...|[\\n  \"craigslist....|[\\n  \"https://mod...|         []|               NULL|Comisiones de $10...|Comisiones de $10...|      6/17/2024|              15|       0|        Unclassified|      LH/GM|              false|      [\\n  99\\n]| [\\n  \"No Educatio...|           99|No Education Listed|         NULL|              NULL|              3|Part-time / full-...|                NULL|                NULL|        false| 92500|          0|          [None]|               year|   150000|      35000|{\\n  \"lat\": 37.63...|    TW9kZXN0bywgQ0E=|  Modesto, CA|  6099|Stanislaus, CA|33700|         Modesto, CA|    6|California|           6099|      Stanislaus, CA|           6099|      Stanislaus, CA|       33700|         Modesto, CA|       33700|         Modesto, CA|    99|Unclassified Indu...|   999|Unclassified Indu...|  9999|Unclassified Indu...| 99999|Unclassified Indu...|999999|Unclassified Indu...|ET0000000000000000|       Unclassified|comisiones de por...|                  []|                  []|                  []|                     []|                  []|                  []|                  []|                  []|                  []|                  []|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|                  []|                  []|                  []|                  []|                  []|                  []|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231010|Business Intellig...|                  23101012|           Oracle Consultant...|                2310|     Business Intellig...|                     23101012|              Oracle Consultant...|           231010|  Business Intellig...|                   2310|        Business Intellig...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|             NULL|                  NULL|          99|Unclassified Indu...|         999|Unclassified Indu...|        9999|Unclassified Indu...|       99999|Unclassified Indu...|      999999|Unclassified Indu...|\n+--------------------+-----------------+----------------------+----------+--------+---------+--------+--------------------+--------------------+--------------------+-----------+-------------------+--------------------+--------------------+---------------+----------------+--------+--------------------+-----------+-------------------+----------------+---------------------+-------------+-------------------+-------------+------------------+---------------+--------------------+--------------------+--------------------+-------------+------+-----------+----------------+-------------------+---------+-----------+--------------------+--------------------+-------------+------+--------------+-----+--------------------+-----+----------+---------------+--------------------+---------------+--------------------+------------+--------------------+------------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+----------+--------------------+----------+---------------+----------+---------------+---------------+--------------------+--------------+--------------------+--------------------------+-------------------------------+--------------------+-------------------------+-----------------------------+----------------------------------+-----------------+----------------------+-----------------------+----------------------------+------------------+-----------------------+-------+--------------------+-------+--------------------+-------+---------------+-------+---------------+-----------------+----------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+\nonly showing top 5 rows\n\n\n\n\n## Listing Columns So We Can Reference them in Visuals\n\nimport pandas as pd\ndf = pd.read_csv(\"./data/lightcast_job_postings.csv\")\nprint(df.columns.tolist())\n\n/tmp/ipykernel_1771/3265774581.py:4: DtypeWarning:\n\nColumns (19,30) have mixed types. Specify dtype option on import or set low_memory=False.\n\n\n\n['ID', 'LAST_UPDATED_DATE', 'LAST_UPDATED_TIMESTAMP', 'DUPLICATES', 'POSTED', 'EXPIRED', 'DURATION', 'SOURCE_TYPES', 'SOURCES', 'URL', 'ACTIVE_URLS', 'ACTIVE_SOURCES_INFO', 'TITLE_RAW', 'BODY', 'MODELED_EXPIRED', 'MODELED_DURATION', 'COMPANY', 'COMPANY_NAME', 'COMPANY_RAW', 'COMPANY_IS_STAFFING', 'EDUCATION_LEVELS', 'EDUCATION_LEVELS_NAME', 'MIN_EDULEVELS', 'MIN_EDULEVELS_NAME', 'MAX_EDULEVELS', 'MAX_EDULEVELS_NAME', 'EMPLOYMENT_TYPE', 'EMPLOYMENT_TYPE_NAME', 'MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE', 'IS_INTERNSHIP', 'SALARY', 'REMOTE_TYPE', 'REMOTE_TYPE_NAME', 'ORIGINAL_PAY_PERIOD', 'SALARY_TO', 'SALARY_FROM', 'LOCATION', 'CITY', 'CITY_NAME', 'COUNTY', 'COUNTY_NAME', 'MSA', 'MSA_NAME', 'STATE', 'STATE_NAME', 'COUNTY_OUTGOING', 'COUNTY_NAME_OUTGOING', 'COUNTY_INCOMING', 'COUNTY_NAME_INCOMING', 'MSA_OUTGOING', 'MSA_NAME_OUTGOING', 'MSA_INCOMING', 'MSA_NAME_INCOMING', 'NAICS2', 'NAICS2_NAME', 'NAICS3', 'NAICS3_NAME', 'NAICS4', 'NAICS4_NAME', 'NAICS5', 'NAICS5_NAME', 'NAICS6', 'NAICS6_NAME', 'TITLE', 'TITLE_NAME', 'TITLE_CLEAN', 'SKILLS', 'SKILLS_NAME', 'SPECIALIZED_SKILLS', 'SPECIALIZED_SKILLS_NAME', 'CERTIFICATIONS', 'CERTIFICATIONS_NAME', 'COMMON_SKILLS', 'COMMON_SKILLS_NAME', 'SOFTWARE_SKILLS', 'SOFTWARE_SKILLS_NAME', 'ONET', 'ONET_NAME', 'ONET_2019', 'ONET_2019_NAME', 'CIP6', 'CIP6_NAME', 'CIP4', 'CIP4_NAME', 'CIP2', 'CIP2_NAME', 'SOC_2021_2', 'SOC_2021_2_NAME', 'SOC_2021_3', 'SOC_2021_3_NAME', 'SOC_2021_4', 'SOC_2021_4_NAME', 'SOC_2021_5', 'SOC_2021_5_NAME', 'LOT_CAREER_AREA', 'LOT_CAREER_AREA_NAME', 'LOT_OCCUPATION', 'LOT_OCCUPATION_NAME', 'LOT_SPECIALIZED_OCCUPATION', 'LOT_SPECIALIZED_OCCUPATION_NAME', 'LOT_OCCUPATION_GROUP', 'LOT_OCCUPATION_GROUP_NAME', 'LOT_V6_SPECIALIZED_OCCUPATION', 'LOT_V6_SPECIALIZED_OCCUPATION_NAME', 'LOT_V6_OCCUPATION', 'LOT_V6_OCCUPATION_NAME', 'LOT_V6_OCCUPATION_GROUP', 'LOT_V6_OCCUPATION_GROUP_NAME', 'LOT_V6_CAREER_AREA', 'LOT_V6_CAREER_AREA_NAME', 'SOC_2', 'SOC_2_NAME', 'SOC_3', 'SOC_3_NAME', 'SOC_4', 'SOC_4_NAME', 'SOC_5', 'SOC_5_NAME', 'LIGHTCAST_SECTORS', 'LIGHTCAST_SECTORS_NAME', 'NAICS_2022_2', 'NAICS_2022_2_NAME', 'NAICS_2022_3', 'NAICS_2022_3_NAME', 'NAICS_2022_4', 'NAICS_2022_4_NAME', 'NAICS_2022_5', 'NAICS_2022_5_NAME', 'NAICS_2022_6', 'NAICS_2022_6_NAME']\n\n\n\nimport pandas as pd\n\ndf = pd.read_csv(\"./data/lightcast_job_postings.csv\")\n\ncolumns_to_drop = [\n    \"ID\", \"URL\", \"ACTIVE_URLS\", \"DUPLICATES\", \"LAST_UPDATED_TIMESTAMP\",\n    \"NAICS2\", \"NAICS3\", \"NAICS4\", \"NAICS5\", \"NAICS6\",\n    \"SOC_2\", \"SOC_3\", \"SOC_5\"\n]\n\ndf.drop(columns=columns_to_drop, inplace=True)\n\n/tmp/ipykernel_1771/304705447.py:3: DtypeWarning:\n\nColumns (19,30) have mixed types. Specify dtype option on import or set low_memory=False.\n\n\n\n\nprint(df.columns.tolist())\n\n['LAST_UPDATED_DATE', 'POSTED', 'EXPIRED', 'DURATION', 'SOURCE_TYPES', 'SOURCES', 'ACTIVE_SOURCES_INFO', 'TITLE_RAW', 'BODY', 'MODELED_EXPIRED', 'MODELED_DURATION', 'COMPANY', 'COMPANY_NAME', 'COMPANY_RAW', 'COMPANY_IS_STAFFING', 'EDUCATION_LEVELS', 'EDUCATION_LEVELS_NAME', 'MIN_EDULEVELS', 'MIN_EDULEVELS_NAME', 'MAX_EDULEVELS', 'MAX_EDULEVELS_NAME', 'EMPLOYMENT_TYPE', 'EMPLOYMENT_TYPE_NAME', 'MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE', 'IS_INTERNSHIP', 'SALARY', 'REMOTE_TYPE', 'REMOTE_TYPE_NAME', 'ORIGINAL_PAY_PERIOD', 'SALARY_TO', 'SALARY_FROM', 'LOCATION', 'CITY', 'CITY_NAME', 'COUNTY', 'COUNTY_NAME', 'MSA', 'MSA_NAME', 'STATE', 'STATE_NAME', 'COUNTY_OUTGOING', 'COUNTY_NAME_OUTGOING', 'COUNTY_INCOMING', 'COUNTY_NAME_INCOMING', 'MSA_OUTGOING', 'MSA_NAME_OUTGOING', 'MSA_INCOMING', 'MSA_NAME_INCOMING', 'NAICS2_NAME', 'NAICS3_NAME', 'NAICS4_NAME', 'NAICS5_NAME', 'NAICS6_NAME', 'TITLE', 'TITLE_NAME', 'TITLE_CLEAN', 'SKILLS', 'SKILLS_NAME', 'SPECIALIZED_SKILLS', 'SPECIALIZED_SKILLS_NAME', 'CERTIFICATIONS', 'CERTIFICATIONS_NAME', 'COMMON_SKILLS', 'COMMON_SKILLS_NAME', 'SOFTWARE_SKILLS', 'SOFTWARE_SKILLS_NAME', 'ONET', 'ONET_NAME', 'ONET_2019', 'ONET_2019_NAME', 'CIP6', 'CIP6_NAME', 'CIP4', 'CIP4_NAME', 'CIP2', 'CIP2_NAME', 'SOC_2021_2', 'SOC_2021_2_NAME', 'SOC_2021_3', 'SOC_2021_3_NAME', 'SOC_2021_4', 'SOC_2021_4_NAME', 'SOC_2021_5', 'SOC_2021_5_NAME', 'LOT_CAREER_AREA', 'LOT_CAREER_AREA_NAME', 'LOT_OCCUPATION', 'LOT_OCCUPATION_NAME', 'LOT_SPECIALIZED_OCCUPATION', 'LOT_SPECIALIZED_OCCUPATION_NAME', 'LOT_OCCUPATION_GROUP', 'LOT_OCCUPATION_GROUP_NAME', 'LOT_V6_SPECIALIZED_OCCUPATION', 'LOT_V6_SPECIALIZED_OCCUPATION_NAME', 'LOT_V6_OCCUPATION', 'LOT_V6_OCCUPATION_NAME', 'LOT_V6_OCCUPATION_GROUP', 'LOT_V6_OCCUPATION_GROUP_NAME', 'LOT_V6_CAREER_AREA', 'LOT_V6_CAREER_AREA_NAME', 'SOC_2_NAME', 'SOC_3_NAME', 'SOC_4', 'SOC_4_NAME', 'SOC_5_NAME', 'LIGHTCAST_SECTORS', 'LIGHTCAST_SECTORS_NAME', 'NAICS_2022_2', 'NAICS_2022_2_NAME', 'NAICS_2022_3', 'NAICS_2022_3_NAME', 'NAICS_2022_4', 'NAICS_2022_4_NAME', 'NAICS_2022_5', 'NAICS_2022_5_NAME', 'NAICS_2022_6', 'NAICS_2022_6_NAME']\n\n\n\n!pip install missingno\n\nRequirement already satisfied: missingno in ./.venv/lib/python3.12/site-packages (0.5.2)\nRequirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from missingno) (2.2.6)\nRequirement already satisfied: matplotlib in ./.venv/lib/python3.12/site-packages (from missingno) (3.10.3)\nRequirement already satisfied: scipy in ./.venv/lib/python3.12/site-packages (from missingno) (1.15.3)\nRequirement already satisfied: seaborn in ./.venv/lib/python3.12/site-packages (from missingno) (0.13.2)\nRequirement already satisfied: contourpy&gt;=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib-&gt;missingno) (1.3.2)\nRequirement already satisfied: cycler&gt;=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib-&gt;missingno) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib-&gt;missingno) (4.58.0)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib-&gt;missingno) (1.4.8)\nRequirement already satisfied: packaging&gt;=20.0 in ./.venv/lib/python3.12/site-packages (from matplotlib-&gt;missingno) (25.0)\nRequirement already satisfied: pillow&gt;=8 in ./.venv/lib/python3.12/site-packages (from matplotlib-&gt;missingno) (11.2.1)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib-&gt;missingno) (3.2.3)\nRequirement already satisfied: python-dateutil&gt;=2.7 in ./.venv/lib/python3.12/site-packages (from matplotlib-&gt;missingno) (2.9.0.post0)\nRequirement already satisfied: pandas&gt;=1.2 in ./.venv/lib/python3.12/site-packages (from seaborn-&gt;missingno) (2.2.3)\nRequirement already satisfied: pytz&gt;=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas&gt;=1.2-&gt;seaborn-&gt;missingno) (2025.2)\nRequirement already satisfied: tzdata&gt;=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas&gt;=1.2-&gt;seaborn-&gt;missingno) (2025.2)\nRequirement already satisfied: six&gt;=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib-&gt;missingno) (1.17.0)\n\n\n\nimport missingno as msno\nimport matplotlib.pyplot as plt\n# Visualize missing values\nmsno.heatmap(df)\nplt.title(\"Missing Values Heatmap\")\nplt.show()\n\n# Drop columns with &gt;50% missing values\ndf.dropna(thresh=len(df) * 0.5, axis=1, inplace=True)\n\n# Fill only the columns you actually have\nif 'Industry' in df.columns:\n    df[\"Industry\"].fillna(\"Unknown\", inplace=True)\n    df[\"Salary\"].fillna(df[\"Salary\"].median(), inplace=True)\n\n\n\n\n\n\n\n\n\ndf = df.drop_duplicates(subset=[\"TITLE\", \"COMPANY\", \"LOCATION\", \"POSTED\"], keep=\"first\")\n\n\ndf = df[df['NAICS_2022_2_NAME'] != 'Unclassified Industry']\n\n\ndf['REMOTE_TYPE_NAME'] = df['REMOTE_TYPE_NAME'].replace('[None]', 'Not Remote')\n\n\n\n\nimport pandas as pd\nimport plotly.express as px\n\n# Step 1: Prepare data\ndata = {\n    'Industry': [\n        'Wholesale Trade', 'Retail Trade', 'Real Estate and Rental and Leasing',\n        'Professional, Scientific, and Technical Services', 'Manufacturing',\n        'Information', 'Health Care and Social Assistance',\n        'Finance and Insurance', 'Educational Services',\n        'Administrative and Support and Waste Management and Remediation Services'\n    ],\n    'Flexible Remote': [87.8, 94.4, 97.6, 92.2, 89.7, 95.8, 92.1, 94.8, 89.0, 94.8],\n    'Onsite': [12.2, 5.6, 2.4, 7.8, 10.3, 4.2, 7.9, 5.2, 11.0, 5.2]\n}\n\ndf = pd.DataFrame(data)\n\n# Step 2: Sort in ascending order of Flexible Remote\ndf_sorted = df.sort_values(by='Flexible Remote', ascending=True)\ndf_sorted['Industry'] = pd.Categorical(df_sorted['Industry'], categories=df_sorted['Industry'], ordered=True)\n\n# Step 3: Melt data for stacked bar format\ndf_melted = df_sorted.melt(\n    id_vars='Industry',\n    value_vars=['Flexible Remote', 'Onsite'],\n    var_name='Remote Type',\n    value_name='Percentage'\n)\n\n# Step 4: Plot\nfig = px.bar(\n    df_melted,\n    x='Percentage',\n    y='Industry',\n    color='Remote Type',\n    orientation='h',\n    text='Percentage',\n    color_discrete_map={\n        'Flexible Remote': '#1aab89',\n        'Onsite': '#88d4c3'\n    },\n    title=\"Remote Job Distribution by Industry (Top 10 Industries)\"\n)\n\n# Step 5: Layout adjustments\nfig.update_layout(\n    xaxis_title=\"Percentage of Jobs\",\n    yaxis_title=\"\",\n    xaxis=dict(tickformat=\".0f\"),\n    legend_title=\"Remote Type\",\n    barmode='stack',\n    margin=dict(l=10, r=10, t=60, b=40),\n    height=500\n)\n\n# Step 6: Label formatting\nfig.update_traces(texttemplate='%{text:.1f}%', textposition='inside')\n\n# Save plot\nfig.write_html(\"./figures/top_industries.html\")\n\n\n# Show plot\nfig.show()\n\n\n\n\n\n\n\n\n\n\n        \n        \n        \n\n\n                                                    \n\n\n\nimport pandas as pd\nfrom IPython.display import display\n# 1. Load the dataset and parse dates\ndf = pd.read_csv(\"./data/lightcast_job_postings.csv\", parse_dates=[\"POSTED\", \"EXPIRED\"])\n# Calculate DURATION in days\ndf['DURATION'] = (df['EXPIRED'] - df['POSTED']).dt.days\n# Extract POSTED month\ndf['POSTED_MONTH'] = df['POSTED'].dt.to_period(\"M\")\n\n/tmp/ipykernel_1771/784347523.py:4: DtypeWarning:\n\nColumns (19,30) have mixed types. Specify dtype option on import or set low_memory=False.\n\n\n\n\n# Define AI and non-AI impacted NAICS codes\nai_impacted_naics = [31, 32, 33, 42, 44, 45, 51, 52, 54, 55, 56]\nnon_ai_impacted_naics = [11, 21, 22, 23, 48, 49, 61, 62, 71, 72]\n\n\n# Create IS_AI_JOB flag\ndf['IS_AI_JOB'] = df['NAICS_2022_2'].apply(lambda x: 1 if pd.notna(x) and x in ai_impacted_naics else 0)\n\n# Remove rows with missing STATE_NAME or POSTED_MONTH\ndf = df[df['STATE_NAME'].notna() & df['POSTED_MONTH'].notna()]\n\n# Group by POSTED_MONTH, STATE_NAME, IS_AI_JOB to count jobs\njob_counts = (\n    df.groupby(['POSTED_MONTH', 'STATE_NAME', 'IS_AI_JOB'])\n      .size()\n      .reset_index(name='JOB_COUNT')\n)\n\n\n# Pivot to put months as rows, states as columns\npivot_df = job_counts.pivot_table(index=['STATE_NAME', 'IS_AI_JOB'], \n                                   columns='POSTED_MONTH', \n                                   values='JOB_COUNT', \n                                   fill_value=0)\n\n# print(pivot_df.columns)\n# print(pivot_df.columns[-1])\n# pivot_df.rename(columns={'2024-05': 'May', \n# '2024-06': 'June',\n# '2024-07': 'July',\n# '2024-08': 'August',\n# '2024-09': 'September'\n# },inplace=True)\npivot_df['2024-06'].reset_index(drop=True)\n\n0       38.0\n1      110.0\n2       19.0\n3       42.0\n4       87.0\n       ...  \n97      17.0\n98      53.0\n99     151.0\n100      1.0\n101     18.0\nName: 2024-06, Length: 102, dtype: float64\n\n\n\n# Calculate % growth from first to last month\n\n# pivot_df[\"PCT_7\"]=(pivot_df['2024-07']-pivot_df['2024-06'])*100/pivot_df['2024-06']\n# pivot_df[\"PCT_8\"]=(pivot_df['2024-08']-pivot_df['2024-07'])*100/pivot_df['2024-07']\n# pivot_df[\"PCT_9\"]=(pivot_df['2024-09']-pivot_df['2024-08'])*100/pivot_df['2024-08']\npivot_df[\"PCT_CHANGE\"]=(pivot_df['2024-09']-pivot_df['2024-05'])*100/pivot_df['2024-05']\n\npivot_df.head()\n\n\n\n\n\n\n\n\nPOSTED_MONTH\n2024-05\n2024-06\n2024-07\n2024-08\n2024-09\nPCT_CHANGE\n\n\nSTATE_NAME\nIS_AI_JOB\n\n\n\n\n\n\n\n\n\n\nAlabama\n0\n62.0\n38.0\n18.0\n31.0\n30.0\n-51.612903\n\n\n1\n100.0\n110.0\n77.0\n132.0\n92.0\n-8.000000\n\n\nAlaska\n0\n13.0\n19.0\n7.0\n18.0\n25.0\n92.307692\n\n\n1\n26.0\n42.0\n30.0\n29.0\n27.0\n3.846154\n\n\nArizona\n0\n102.0\n87.0\n71.0\n72.0\n85.0\n-16.666667\n\n\n\n\n\n\n\n\n\n# Drop rows with undefined % growth (divide by zero)\npivot_df = pivot_df.dropna(subset=['PCT_CHANGE'])\n\n# Reset index for sorting\npivot_df = pivot_df.reset_index()\n\n\n# Get top 10 states by % growth\ntop_ai = pivot_df[pivot_df['IS_AI_JOB'] == 1].sort_values('PCT_CHANGE', ascending=False).head(10)\ntop_non_ai = pivot_df[pivot_df['IS_AI_JOB'] == 0].sort_values('PCT_CHANGE', ascending=False).head(10)\n\n# Combine and label\ntop_combined = pd.concat([\n    top_ai.assign(JOB_TYPE='AI'),\n    top_non_ai.assign(JOB_TYPE='Non-AI')\n])\n\n# Display \ndisplay(top_combined[['STATE_NAME', 'IS_AI_JOB', 'PCT_CHANGE', 'JOB_TYPE']])\n\n\n\n\n\n\n\nPOSTED_MONTH\nSTATE_NAME\nIS_AI_JOB\nPCT_CHANGE\nJOB_TYPE\n\n\n\n\n101\nWyoming\n1\n380.000000\nAI\n\n\n97\nWest Virginia\n1\n380.000000\nAI\n\n\n81\nSouth Dakota\n1\n175.000000\nAI\n\n\n21\nHawaii\n1\n75.000000\nAI\n\n\n67\nNorth Dakota\n1\n64.285714\nAI\n\n\n61\nNew Mexico\n1\n56.000000\nAI\n\n\n73\nOregon\n1\n54.716981\nAI\n\n\n41\nMassachusetts\n1\n40.074906\nAI\n\n\n95\nWashington, D.C. (District of Columbia)\n1\n36.363636\nAI\n\n\n53\nNebraska\n1\n36.065574\nAI\n\n\n66\nNorth Dakota\n0\n275.000000\nNon-AI\n\n\n88\nVermont\n0\n225.000000\nNon-AI\n\n\n96\nWest Virginia\n0\n130.000000\nNon-AI\n\n\n52\nNebraska\n0\n100.000000\nNon-AI\n\n\n72\nOregon\n0\n100.000000\nNon-AI\n\n\n2\nAlaska\n0\n92.307692\nNon-AI\n\n\n32\nKentucky\n0\n53.125000\nNon-AI\n\n\n94\nWashington, D.C. (District of Columbia)\n0\n40.000000\nNon-AI\n\n\n48\nMissouri\n0\n37.931034\nNon-AI\n\n\n60\nNew Mexico\n0\n36.842105\nNon-AI\n\n\n\n\n\n\n\n\ndef label_ai_impact(naics):\n    try:\n        code = int(naics)\n        if code in ai_impacted_naics:\n            return \"AI-Impacted\"\n        elif code in non_ai_impacted_naics:\n            return \"Non-AI-Impacted\"\n        else:\n            return \"Unclassified\"\n    except:\n        return \"Unclassified\"\n\n\ndf['AI_IMPACTED'] = df['NAICS_2022_2'].apply(label_ai_impact)\n# Filter only AI and Non-AI impacted\ndf = df[df[\"AI_IMPACTED\"].isin([\"AI-Impacted\", \"Non-AI-Impacted\"])]\n\n# Now split the data\nai_df = df[df[\"AI_IMPACTED\"] == \"AI-Impacted\"][\n    [\"STATE_NAME\", \"NAICS_2022_2\", \"NAICS_2022_2_NAME\", \"LOT_SPECIALIZED_OCCUPATION_NAME\"]\n]\n\nnon_ai_df = df[df[\"AI_IMPACTED\"] == \"Non-AI-Impacted\"][\n    [\"STATE_NAME\", \"NAICS_2022_2\", \"NAICS_2022_2_NAME\", \"LOT_SPECIALIZED_OCCUPATION_NAME\"]\n]\n\n\nfrom IPython.display import display\n\nprint(\"AI-Impacted Industries:\")\ndisplay(ai_df)\n\nprint(\"\\nNon-AI-Impacted Industries:\")\ndisplay(non_ai_df)\n\nAI-Impacted Industries:\n\n\n\n\n\n\n\n\n\nSTATE_NAME\nNAICS_2022_2\nNAICS_2022_2_NAME\nLOT_SPECIALIZED_OCCUPATION_NAME\n\n\n\n\n0\nArkansas\n44.0\nRetail Trade\nGeneral ERP Analyst / Consultant\n\n\n1\nMaine\n56.0\nAdministrative and Support and Waste Managemen...\nOracle Consultant / Analyst\n\n\n2\nTexas\n52.0\nFinance and Insurance\nData Analyst\n\n\n3\nArizona\n52.0\nFinance and Insurance\nData Analyst\n\n\n5\nArkansas\n51.0\nInformation\nData Analyst\n\n\n...\n...\n...\n...\n...\n\n\n72493\nVirginia\n54.0\nProfessional, Scientific, and Technical Services\nData Analyst\n\n\n72494\nMassachusetts\n51.0\nInformation\nEnterprise Architect\n\n\n72495\nMichigan\n56.0\nAdministrative and Support and Waste Managemen...\nData Analyst\n\n\n72496\nMaine\n51.0\nInformation\nData Analyst\n\n\n72497\nTexas\n54.0\nProfessional, Scientific, and Technical Services\nOracle Consultant / Analyst\n\n\n\n\n52444 rows Ã 4 columns\n\n\n\n\nNon-AI-Impacted Industries:\n\n\n\n\n\n\n\n\n\nSTATE_NAME\nNAICS_2022_2\nNAICS_2022_2_NAME\nLOT_SPECIALIZED_OCCUPATION_NAME\n\n\n\n\n15\nMassachusetts\n61.0\nEducational Services\nData Analyst\n\n\n18\nAlabama\n62.0\nHealth Care and Social Assistance\nEnterprise Architect\n\n\n37\nVirginia\n62.0\nHealth Care and Social Assistance\nData Analyst\n\n\n77\nOhio\n23.0\nConstruction\nEnterprise Architect\n\n\n94\nTexas\n61.0\nEducational Services\nBusiness Analyst (General)\n\n\n...\n...\n...\n...\n...\n\n\n72451\nSouth Carolina\n62.0\nHealth Care and Social Assistance\nData Analyst\n\n\n72460\nCalifornia\n11.0\nAgriculture, Forestry, Fishing and Hunting\nData Analyst\n\n\n72472\nCalifornia\n61.0\nEducational Services\nGeneral ERP Analyst / Consultant\n\n\n72479\nWyoming\n61.0\nEducational Services\nData Analyst\n\n\n72489\nTexas\n22.0\nUtilities\nSAP Analyst / Admin\n\n\n\n\n7942 rows Ã 4 columns\n\n\n\n\n# Step 1: Count total jobs by state\ntotal_jobs_by_state = df.groupby('STATE_NAME').size().rename('Total_Jobs')\n\n\n# Step 2: Count AI-impacted jobs by state\nai_jobs_by_state = df[df['AI_IMPACTED'] == 'AI-Impacted'].groupby('STATE_NAME').size().rename('AI_Impacted_Jobs')\n# Step 3: Merge the two counts into a single DataFrame\nai_impact_summary = pd.concat([total_jobs_by_state, ai_jobs_by_state], axis=1).fillna(0)\n\n\n# Step 1: Clean and convert date columns\ndf = df[df['NAICS_2022_2_NAME'] != 'Unclassified Industry']\ndf['POSTED'] = pd.to_datetime(df['POSTED'], errors='coerce')\ndf['EXPIRED'] = pd.to_datetime(df['EXPIRED'], errors='coerce')\n\n# Drop rows with missing POSTED or EXPIRED dates\ndf = df.dropna(subset=['POSTED', 'EXPIRED'])\n\n\nimport plotly.express as px\n\n# Count postings per industry\nindustry_counts = df['NAICS2_NAME'].value_counts().reset_index()\nindustry_counts.columns = ['NAICS2_NAME', 'count']\n\n# Sort values for better readability\nindustry_counts = industry_counts.sort_values(by='count', ascending=True)\n\n# Horizontal bar plot\nfig = px.bar(\n    industry_counts,\n    x='count',\n    y='NAICS2_NAME',\n    orientation='h',\n    title='Job Postings by Industry',\n    labels={'NAICS2_NAME': 'Industry', 'count': 'Number of Postings'},\n    color='count',\n    color_continuous_scale='Blues'\n)\n\n# Clean layout\nfig.update_layout(\n    yaxis_title='Industry',\n    xaxis_title='Number of Postings',\n    title_font_size=20,\n    plot_bgcolor='white',\n    xaxis=dict(showgrid=True),\n    yaxis=dict(showgrid=False)\n)\n\nfig.show()\n\n                                                    \n\n\n\n# import plotly.express as px\n\n# # AI job growth plot\n# fig_ai = px.bar(\n#     top_ai,\n#     x='STATE_NAME',\n#     y='Avg_AI_Job_Growth',\n#     title='Top 10 States by Average AI Job Growth',\n#     labels={'STATE_NAME': 'State', 'Avg_AI_Job_Growth': 'Avg % AI Job Growth'},\n#     text='Avg_AI_Job_Growth'\n# )\n# fig_ai.update_traces(texttemplate='%{text:.2f}%', textposition='outside')\n# fig_ai.update_layout(yaxis_title='Average % Growth', xaxis_title='State')\n# fig_ai.show()\n\n# # Non-AI job growth plot\n# fig_non_ai = px.bar(\n#     top_non_ai,\n#     x='STATE_NAME',\n#     y='Avg_Non_AI_Job_Growth',\n#     title='Top 10 States by Average Non-AI Job Growth',\n#     labels={'STATE_NAME': 'State', 'Avg_Non_AI_Job_Growth': 'Avg % Non-AI Job Growth'},\n#     text='Avg_Non_AI_Job_Growth'\n# )\n# fig_non_ai.update_traces(texttemplate='%{text:.2f}%', textposition='outside')\n# fig_non_ai.update_layout(yaxis_title='Average % Growth', xaxis_title='State')\n# fig_non_ai.show()\n\n\n\ndf['AVERAGE_SALARY'] = df[['SALARY_FROM', 'SALARY_TO']].mean(axis=1)\n\n\navg_salary_by_state_type = (\n    df.groupby(['STATE_NAME', 'AI_IMPACTED'])['AVERAGE_SALARY']\n    .mean()\n    .reset_index()\n)\n\n\n\n# import numpy as np\n\n# # Step 1: Replace infinite values (result of division by zero) with NaN\n# pivoted.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n# # Step 2: Drop rows with missing percentage growth values\n# pivoted.dropna(subset=['AI_Growth_Pct', 'Non_AI_Growth_Pct'], inplace=True)\n\n# # Step 3: Group by state and calculate average percent growth\n# avg_growth = pivoted.groupby('STATE_NAME')[['AI_Growth_Pct', 'Non_AI_Growth_Pct']].mean().reset_index()\n\n# # Step 4: Sort states by AI and Non-AI growth\n# ranked = avg_growth.sort_values(by=['AI_Growth_Pct', 'Non_AI_Growth_Pct'], ascending=False)\n\n# # Step 5: Display top states for AI and Non-AI job growth\n# from IPython.display import display\n# print(\"Top states by AI job growth:\")\n# display(ranked[['STATE_NAME', 'AI_Growth_Pct']].head(10))\n\n# print(\"\\nTop states by Non-AI job growth:\")\n# display(ranked[['STATE_NAME', 'Non_AI_Growth_Pct']].sort_values(by='Non_AI_Growth_Pct', ascending=False).head(10))\n\n\n# import seaborn as sns\n# import matplotlib.pyplot as plt\n\n# # Step 9: Visualize\n# plt.figure(figsize=(14, 6))\n# avg_growth[['AI_Growth', 'Non_AI_Growth']].head(10).plot(kind='bar')\n# plt.title('Average Monthly Job Growth (Top 10 States) for AI vs. Non-AI Careers')\n# plt.ylabel('Average Monthly Job Growth')\n# plt.xticks(rotation=45)\n# plt.tight_layout()\n# plt.grid(True)\n# plt.show()\n\n\n# Filter for Boston, MA and Austin, TX\nselected_state = ['California', 'Florida', 'Massachusetts', 'Texas', 'New York']\nfiltered_df = df[df['STATE_NAME'].isin(selected_state)]\n\n# Further filter for NAICS_2022_6 = 518210 and show relevant columns\nfinal_df = filtered_df[filtered_df['LOT_SPECIALIZED_OCCUPATION_NAME'].str.contains('analyst', case=False, na=False)]\nfinal_df[['STATE_NAME', 'NAICS2_NAME', 'NAICS_2022_6', 'LOT_SPECIALIZED_OCCUPATION_NAME']].head(100)\n\n\n\n\n\n\n\n\nSTATE_NAME\nNAICS2_NAME\nNAICS_2022_6\nLOT_SPECIALIZED_OCCUPATION_NAME\n\n\n\n\n2\nTexas\nFinance and Insurance\n524291.0\nData Analyst\n\n\n9\nNew York\nProfessional, Scientific, and Technical Services\n541511.0\nData Analyst\n\n\n10\nCalifornia\nWholesale Trade\n423830.0\nData Analyst\n\n\n15\nMassachusetts\nEducational Services\n611310.0\nData Analyst\n\n\n28\nMassachusetts\nFinance and Insurance\n522320.0\nBusiness Intelligence Analyst\n\n\n...\n...\n...\n...\n...\n\n\n462\nTexas\nEducational Services\n611210.0\nGeneral ERP Analyst / Consultant\n\n\n465\nTexas\nFinance and Insurance\n522320.0\nBusiness Analyst (General)\n\n\n472\nTexas\nInformation\n518210.0\nSAP Analyst / Admin\n\n\n473\nTexas\nProfessional, Scientific, and Technical Services\n541511.0\nOracle Consultant / Analyst\n\n\n487\nNew York\nAdministrative and Support and Waste Managemen...\n561320.0\nData Analyst\n\n\n\n\n100 rows Ã 4 columns\n\n\n\n\n# Filter for Boston, MA and Austin, TX\nselected_state = ['California', 'Florida', 'Massachusetts', 'Texas', 'New York']\nfiltered_df = df[df['STATE_NAME'].isin(selected_state)]\n\n\n# Further filter for NAICS_2022_6 = 518210 and show relevant columns\nfinal_df = filtered_df[filtered_df['LOT_SPECIALIZED_OCCUPATION_NAME'].str.contains('analyst', case=False, na=False)]\nfinal_df[['STATE_NAME', 'NAICS2_NAME', 'NAICS_2022_6', 'LOT_SPECIALIZED_OCCUPATION_NAME']].head(100)\n\n\n\n\n\n\n\n\nSTATE_NAME\nNAICS2_NAME\nNAICS_2022_6\nLOT_SPECIALIZED_OCCUPATION_NAME\n\n\n\n\n2\nTexas\nFinance and Insurance\n524291.0\nData Analyst\n\n\n9\nNew York\nProfessional, Scientific, and Technical Services\n541511.0\nData Analyst\n\n\n10\nCalifornia\nWholesale Trade\n423830.0\nData Analyst\n\n\n15\nMassachusetts\nEducational Services\n611310.0\nData Analyst\n\n\n28\nMassachusetts\nFinance and Insurance\n522320.0\nBusiness Intelligence Analyst\n\n\n...\n...\n...\n...\n...\n\n\n462\nTexas\nEducational Services\n611210.0\nGeneral ERP Analyst / Consultant\n\n\n465\nTexas\nFinance and Insurance\n522320.0\nBusiness Analyst (General)\n\n\n472\nTexas\nInformation\n518210.0\nSAP Analyst / Admin\n\n\n473\nTexas\nProfessional, Scientific, and Technical Services\n541511.0\nOracle Consultant / Analyst\n\n\n487\nNew York\nAdministrative and Support and Waste Managemen...\n561320.0\nData Analyst\n\n\n\n\n100 rows Ã 4 columns\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n#read files\nfile_path = \"./data/lightcast_job_postings.csv\"\ndf = pd.read_csv(file_path)\n\n/tmp/ipykernel_1771/1191241380.py:3: DtypeWarning:\n\nColumns (19,30) have mixed types. Specify dtype option on import or set low_memory=False.\n\n\n\n\nprint(df['REMOTE_TYPE_NAME'].value_counts(dropna=False).head(10))\n\nREMOTE_TYPE_NAME\nNot Remote       57741\nRemote           12497\nHybrid Remote     2260\nName: count, dtype: int64\n\n\n\n# Step 1: Standardize formatting\ndf['REMOTE_TYPE_NAME'] = (\n    df['REMOTE_TYPE_NAME']\n    .astype(str)\n    .str.strip()\n    .str.title()\n    .replace({'None': pd.NA, 'Nan': pd.NA})\n)\n\n\n# Step 2: Fill missing or ambiguous entries with 'Not Remote'\ndf['REMOTE_TYPE_NAME'] = df['REMOTE_TYPE_NAME'].fillna('Not Remote')\ndf.loc[df['REMOTE_TYPE_NAME'] == \"[None]\", 'REMOTE_TYPE_NAME'] = \"Not Remote\"\nprint(df['REMOTE_TYPE_NAME'].value_counts(dropna=False).head(10))\n\nREMOTE_TYPE_NAME\nNot Remote       57741\nRemote           12497\nHybrid Remote     2260\nName: count, dtype: int64\n\n\n\n# Convert all values to strings and strip whitespace\ndf['REMOTE_TYPE_NAME'] = df['REMOTE_TYPE_NAME'].astype(str).str.strip()\n\n\n# Apply new classification logic\ndf['REMOTE_BINARY'] = df['REMOTE_TYPE_NAME'].apply(\n    lambda x: 1 if x in ['Remote', 'Hybrid Remote'] else 0\n)\n\n\n\n# Check result\nprint(df['REMOTE_TYPE_NAME'].value_counts())\nprint(\"\\nBinary classification:\")\nprint(df['REMOTE_BINARY'].value_counts())\n\nREMOTE_TYPE_NAME\nNot Remote       57741\nRemote           12497\nHybrid Remote     2260\nName: count, dtype: int64\n\nBinary classification:\nREMOTE_BINARY\n0    57741\n1    14757\nName: count, dtype: int64\n\n\n\nimport pandas as pd\nimport numpy as np\n\n# Ensure salary columns are numeric and handle missing values\ndf['SALARY_FROM'] = pd.to_numeric(df['SALARY_FROM'], errors='coerce').replace(0, np.nan)\ndf['SALARY_TO'] = pd.to_numeric(df['SALARY_TO'], errors='coerce').replace(0, np.nan)\n\n# Calculate average salary (mean of SALARY_FROM and SALARY_TO)\ndf['AVERAGE_SALARY'] = df[['SALARY_FROM', 'SALARY_TO']].mean(axis=1)\n\n# Drop rows with missing values in AVERAGE_SALARY, REMOTE_TYPE_NAME, or STATE_NAME\ndf_salary = df.dropna(subset=['AVERAGE_SALARY', 'REMOTE_TYPE_NAME', 'STATE_NAME'])\n\n# Group by state and remote type, then calculate average salary\navg_salary_by_state_remote = df_salary.groupby(['STATE_NAME', 'REMOTE_TYPE_NAME'])['AVERAGE_SALARY'].mean().reset_index()\n\n# Round the results for easier reading\navg_salary_by_state_remote['AVERAGE_SALARY'] = avg_salary_by_state_remote['AVERAGE_SALARY'].round(2)\n\n# Show results\nprint(avg_salary_by_state_remote)\n\n    STATE_NAME REMOTE_TYPE_NAME  AVERAGE_SALARY\n0      Alabama    Hybrid Remote        95779.32\n1      Alabama       Not Remote       110741.22\n2      Alabama           Remote       112252.79\n3       Alaska    Hybrid Remote        85384.29\n4       Alaska       Not Remote        91767.29\n..         ...              ...             ...\n147  Wisconsin       Not Remote       114918.55\n148  Wisconsin           Remote       104307.56\n149    Wyoming    Hybrid Remote       221727.00\n150    Wyoming       Not Remote       106695.18\n151    Wyoming           Remote       111982.65\n\n[152 rows x 3 columns]\n\n\n\ndf = df.merge(avg_salary_by_state_remote,\n              on=['STATE_NAME', 'REMOTE_TYPE_NAME'],\n              how='left')\n\n\ndf = df.merge(\n    avg_salary_by_state_remote,\n    on=['STATE_NAME', 'REMOTE_TYPE_NAME'],\n    how='left',\n    suffixes=('', '_STATE_REMOTE_AVG')\n)\n\n\n[col for col in df.columns if 'AVG' in col or 'SALARY' in col]\n\n['SALARY',\n 'SALARY_TO',\n 'SALARY_FROM',\n 'AVERAGE_SALARY',\n 'AVERAGE_SALARY_STATE_REMOTE_AVG']\n\n\n\ndf = df.rename(columns={'AVERAGE_SALARY_y': 'AVERAGE_SALARY_STATE_REMOTE_AVG'})\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import (accuracy_score, f1_score, confusion_matrix,\n                             classification_report, precision_score,\n                             recall_score, balanced_accuracy_score)\nfrom sklearn.inspection import permutation_importance\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Remove duplicate column names across full dataframe\ndf = df.loc[:, ~df.columns.duplicated()]\n\n\ndf['AVG_YEARS_EXPERIENCE'] = (df['MIN_YEARS_EXPERIENCE'] + df['MAX_YEARS_EXPERIENCE']) / 2\ndf['EXP_SPREAD'] = df['MAX_YEARS_EXPERIENCE'] - df['MIN_YEARS_EXPERIENCE']\n\n\n\ndf = df.drop(columns=['MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE'])\n\n\nnum_feats = [\n    'AVG_YEARS_EXPERIENCE',\n    'AVERAGE_SALARY_STATE_REMOTE_AVG',\n    'IS_INTERNSHIP'\n]\n\ncat_feats = [\n    'STATE_NAME',\n    'NAICS_2022_2_NAME',\n    'EDUCATION_LEVELS_NAME',\n    'COMMON_SKILLS_NAME',\n    'SOFTWARE_SKILLS_NAME',\n    'TITLE_CLEAN'\n    \n]\n\nX = df[num_feats + cat_feats]\ny = df['REMOTE_BINARY']\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n# Preprocessing\npreprocess = ColumnTransformer(transformers=[\n    (\"num\", StandardScaler(), num_feats),\n    (\"cat\", OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_feats)\n])\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n# Preprocessing step\npreprocess = ColumnTransformer(transformers=[\n    (\"num\", StandardScaler(), num_feats),\n    (\"cat\", OneHotEncoder(handle_unknown='ignore', max_categories=500, sparse_output=False), cat_feats)\n])\nclf = RandomForestClassifier(random_state=42, class_weight='balanced')\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\n\nrf = RandomForestClassifier(\n    n_estimators=200,\n    max_depth=None,\n    class_weight='balanced',\n    random_state=42,\n    n_jobs=-1\n)\n\npipe = Pipeline(steps=[\n    ('prep', preprocess),\n    ('model', rf)\n])\n\n\npipe.fit(X_train, y_train)\n\nPipeline(steps=[('prep',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['AVG_YEARS_EXPERIENCE',\n                                                   'AVERAGE_SALARY_STATE_REMOTE_AVG',\n                                                   'IS_INTERNSHIP']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                max_categories=500,\n                                                                sparse_output=False),\n                                                  ['STATE_NAME',\n                                                   'NAICS_2022_2_NAME',\n                                                   'EDUCATION_LEVELS_NAME',\n                                                   'COMMON_SKILLS_NAME',\n                                                   'SOFTWARE_SKILLS_NAME',\n                                                   'TITLE_CLEAN'])])),\n                ('model',\n                 RandomForestClassifier(class_weight='balanced',\n                                        n_estimators=200, n_jobs=-1,\n                                        random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('prep',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['AVG_YEARS_EXPERIENCE',\n                                                   'AVERAGE_SALARY_STATE_REMOTE_AVG',\n                                                   'IS_INTERNSHIP']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                max_categories=500,\n                                                                sparse_output=False),\n                                                  ['STATE_NAME',\n                                                   'NAICS_2022_2_NAME',\n                                                   'EDUCATION_LEVELS_NAME',\n                                                   'COMMON_SKILLS_NAME',\n                                                   'SOFTWARE_SKILLS_NAME',\n                                                   'TITLE_CLEAN'])])),\n                ('model',\n                 RandomForestClassifier(class_weight='balanced',\n                                        n_estimators=200, n_jobs=-1,\n                                        random_state=42))]) prep: ColumnTransformer?Documentation for prep: ColumnTransformerColumnTransformer(transformers=[('num', StandardScaler(),\n                                 ['AVG_YEARS_EXPERIENCE',\n                                  'AVERAGE_SALARY_STATE_REMOTE_AVG',\n                                  'IS_INTERNSHIP']),\n                                ('cat',\n                                 OneHotEncoder(handle_unknown='ignore',\n                                               max_categories=500,\n                                               sparse_output=False),\n                                 ['STATE_NAME', 'NAICS_2022_2_NAME',\n                                  'EDUCATION_LEVELS_NAME', 'COMMON_SKILLS_NAME',\n                                  'SOFTWARE_SKILLS_NAME', 'TITLE_CLEAN'])]) num['AVG_YEARS_EXPERIENCE', 'AVERAGE_SALARY_STATE_REMOTE_AVG', 'IS_INTERNSHIP'] StandardScaler?Documentation for StandardScalerStandardScaler() cat['STATE_NAME', 'NAICS_2022_2_NAME', 'EDUCATION_LEVELS_NAME', 'COMMON_SKILLS_NAME', 'SOFTWARE_SKILLS_NAME', 'TITLE_CLEAN'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore', max_categories=500, sparse_output=False) RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(class_weight='balanced', n_estimators=200, n_jobs=-1,\n                       random_state=42) \n\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, balanced_accuracy_score\n\n# Get prediction\ny_pred = pipe.predict(X_test)\n\n# Prepare metrics text\nreport_text = classification_report(y_test, y_pred)\ncm = confusion_matrix(y_test, y_pred)\n\ncustom_text = (\n    f\"Confusion Matrix:\\n{cm}\\n\\n\"\n    f\"Custom Metrics:\\n\"\n    f\"Accuracy: {round(accuracy_score(y_test, y_pred), 3)}\\n\"\n    f\"F1 Score: {round(f1_score(y_test, y_pred), 3)}\\n\"\n    f\"Precision: {round(precision_score(y_test, y_pred), 3)}\\n\"\n    f\"Sensitivity (Recall 1): {round(recall_score(y_test, y_pred), 3)}\\n\"\n    f\"Specificity (Recall 0): {round(recall_score(y_test, y_pred, pos_label=0), 3)}\\n\"\n    f\"Balanced Accuracy: {round(balanced_accuracy_score(y_test, y_pred), 3)}\"\n)\n\n# Plot text in figure\nplt.figure(figsize=(10, 10))\nplt.axis('off')\nplt.text(0, 1, report_text + \"\\n\" + custom_text, fontsize=10, va='top', family='monospace')\n\n# Save figure\nplt.savefig(\"./figures/classification_report.png\", dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.title(\"Confusion Matrix\")\nplt.savefig(\"./figures/Confusion_matrix.png\", dpi=300) \nplt.show()\n\n\n\n\n\n\n\n\n\nrf_model       = pipe.named_steps[\"model\"]          # RandomForestClassifier\nfeature_names  = pipe.named_steps[\"prep\"].get_feature_names_out()\n\nimportances = rf_model.feature_importances_\n\nfeat_imp = (\n    pd.DataFrame({\"Feature\": feature_names, \"Importance\": importances})\n      .sort_values(by=\"Importance\", ascending=False)\n      .reset_index(drop=True)\n)\n\nprint(\"\\nTop 9 â Tree-based Importances\")\nprint(feat_imp.head(9).to_string(index=False))\n\n\nTop 9 â Tree-based Importances\n                                   Feature  Importance\n      num__AVERAGE_SALARY_STATE_REMOTE_AVG    0.392680\n                 num__AVG_YEARS_EXPERIENCE    0.014525\n                cat__STATE_NAME_California    0.014173\n                     cat__STATE_NAME_Texas    0.011198\n               cat__STATE_NAME_Connecticut    0.010292\ncat__COMMON_SKILLS_NAME_infrequent_sklearn    0.009475\n                cat__STATE_NAME_New Jersey    0.009274\n       cat__TITLE_CLEAN_infrequent_sklearn    0.008570\n                  cat__STATE_NAME_Virginia    0.008235\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ntop_n = 9                     # change to show more/less\nplt.figure(figsize=(8, 6))\nsns.barplot(\n    data=feat_imp.head(top_n),\n    x=\"Importance\", y=\"Feature\",\n    palette=\"crest\"\n)\nplt.title(f\"Top {top_n} Feature Importances (Random Forest)\")\nplt.xlabel(\"Mean Decrease in Impurity\")\nplt.ylabel(\"\")\nplt.tight_layout()\nplt.savefig(\"./figures/Featured_importance.png\", dpi=300) \nplt.show()\n\n/tmp/ipykernel_1771/2276307941.py:6: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport plotly.express as px\n\n# Step 1: Create state abbreviation mapping\nus_state_abbrev = {\n    'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR',\n    'California': 'CA', 'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE',\n    'Florida': 'FL', 'Georgia': 'GA', 'Hawaii': 'HI', 'Idaho': 'ID',\n    'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA', 'Kansas': 'KS',\n    'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD',\n    'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS',\n    'Missouri': 'MO', 'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV',\n    'New Hampshire': 'NH', 'New Jersey': 'NJ', 'New Mexico': 'NM', 'New York': 'NY',\n    'North Carolina': 'NC', 'North Dakota': 'ND', 'Ohio': 'OH', 'Oklahoma': 'OK',\n    'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI', 'South Carolina': 'SC',\n    'South Dakota': 'SD', 'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT',\n    'Vermont': 'VT', 'Virginia': 'VA', 'Washington': 'WA', 'West Virginia': 'WV',\n    'Wisconsin': 'WI', 'Wyoming': 'WY', 'District of Columbia': 'DC'\n}\n\n# Step 2: Map state names to abbreviations\ndf['STATE_ABBR'] = df['STATE_NAME'].map(us_state_abbrev)\n\n# Step 3: Group by state and compute metrics\nchoropleth_data = df.groupby('STATE_ABBR').agg(\n    remote_ratio=('REMOTE_BINARY', 'mean'),\n    avg_salary=('AVERAGE_SALARY_STATE_REMOTE_AVG', 'mean'),\n    avg_experience=('AVG_YEARS_EXPERIENCE', 'mean'),\n    job_count=('STATE_NAME', 'count')\n).reset_index()\n\n# Step 4: Define custom green scale (start from light, move to #1aab89)\ncustom_green_scale = [\n    [0, \"#e0f7f1\"],     # light mint\n    [0.5, \"#70d8b5\"],   # mid-green\n    [1, \"#1aab89\"]      # deep teal green\n]\n\n# Step 5: Create the choropleth with custom green\nfig = px.choropleth(\n    data_frame=choropleth_data,\n    locations='STATE_ABBR',\n    locationmode=\"USA-states\",\n    color='remote_ratio',\n    color_continuous_scale=custom_green_scale,\n    scope=\"usa\",\n    labels={'remote_ratio': 'Remote Job Ratio'},\n    hover_data={\n        'remote_ratio': ':.2f',\n        'avg_salary': ':.0f',\n        'avg_experience': ':.1f',\n        'job_count': True\n    },\n    title='Remote Job Ratio by State (Custom Green), Avg Salary & Experience in Hover'\n)\n\nfig.update_layout(margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0})\nfig.write_html(\"./figures/state_remote_job_ratio.html\")\n\nfig.show()\n\n\n\n                                                    \n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Step 1: Ensure 'POSTED' is in datetime format and create Year-Month\ndf['POSTED'] = pd.to_datetime(df['POSTED'])\ndf['POSTED_YM'] = df['POSTED'].dt.to_period('M').astype(str)\n\n\nindustry_trends = (\n    df.groupby(['NAICS_2022_2_NAME', 'POSTED_YM'])['REMOTE_BINARY']\n    .mean()\n    .reset_index(name='REMOTE_RATIO')\n)\n\n\n# Step 3: Select top 5 industries with highest overall average remote ratio\ntop_industries = (\n    industry_trends.groupby('NAICS_2022_2_NAME')['REMOTE_RATIO']\n    .mean()\n    .sort_values(ascending=False)\n    .head(5)\n    .index.tolist()\n)\nfiltered_trends = industry_trends[industry_trends['NAICS_2022_2_NAME'].isin(top_industries)]\n\n\nimport plotly.express as px\n\nfig = px.line(\n    filtered_trends,\n    x='POSTED_YM',\n    y='REMOTE_RATIO',\n    color='NAICS_2022_2_NAME',\n    markers=True,\n    title=\"Top Industries: Remote Job Trends Over Time\"\n)\n\nfig.update_layout(\n    xaxis_title=\"Posted Month\",\n    yaxis_title=\"Remote Job Ratio\",\n    legend_title=\"Industry\",\n    legend=dict(x=1.02, y=1, bordercolor=\"Black\"),\n    margin=dict(l=40, r=40, t=60, b=40),\n    width=1000,\n    height=500\n)\n\nfig.update_xaxes(tickangle=45)\nfig.write_html(\"./figures/remote_job_over_time.html\")\nfig.show()\n\n                                                    \n\n\n\n#Groupby industry + month and calculate both:\nindustry_month_stats = df.groupby(['NAICS_2022_2_NAME', 'POSTED_YM']).agg(\n    TOTAL_JOBS=('REMOTE_BINARY', 'count'),\n    REMOTE_RATIO=('REMOTE_BINARY', 'mean')\n).reset_index()\n\n\njob_count = df.groupby(['NAICS_2022_2_NAME', 'POSTED_YM']).size().reset_index(name='JOB_COUNT')\n\n\nremote_ratio = df.groupby(['NAICS_2022_2_NAME', 'POSTED_YM'])['REMOTE_BINARY'].mean().reset_index(name='REMOTE_RATIO')\n\n\nindustry_month_stats = pd.merge(remote_ratio, job_count, on=['NAICS_2022_2_NAME', 'POSTED_YM'])\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Choose 2â3 industries to plot (or loop one at a time)\nselected_industries = [\n    'Administrative and Support and Waste Management and Remediation Services',\n    'Arts, Entertainment, and Recreation',\n    'Finance and Insurance',\n    'Real Estate and Rental and Leasing',\n    'Utilities'\n]\n\n\n\nfor industry in selected_industries:\n    data = industry_month_stats[industry_month_stats['NAICS_2022_2_NAME'] == industry]  \n\n    fig, ax1 = plt.subplots(figsize=(10, 4))\n\n    # Plot remote ratio\n    ax1.plot(data['POSTED_YM'], data['REMOTE_RATIO'], color='tab:blue', marker='o')\n    ax1.set_xlabel('Month')\n    ax1.set_ylabel('Remote Job Ratio', color='tab:blue')\n    ax1.tick_params(axis='y', labelcolor='tab:blue')\n    ax1.set_title(f\"Remote Job Ratio & Volume Over Time: {industry}\")\n\n    # Plot job count on secondary y-axis\n    ax2 = ax1.twinx()\n    ax2.bar(data['POSTED_YM'], data['JOB_COUNT'], color='tab:gray', alpha=0.3)\n    ax2.set_ylabel('Total Job Postings', color='gray')\n    ax2.tick_params(axis='y', labelcolor='gray')\n\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.savefig(\"./figures/Remote_jobs_\"+str(industry)+\".jpg\", dpi=300)\n    plt.show()"
  },
  {
    "objectID": "AI.html",
    "href": "AI.html",
    "title": "Ai Vs Non AI Jobs",
    "section": "",
    "text": "Code\nimport findspark\nfindspark.init()\n\nfrom pyspark.sql import SparkSession\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\nimport numpy as np\n\nnp.random.seed(423548)\n\npio.renderers.default = \"notebook\"\n\n# Initialize Spark Session\nspark = SparkSession.builder.appName(\"LightcastData\").getOrCreate()\n\n# Load Data\ndf = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").option(\"escape\", \"\\\"\").csv(\"./data/lightcast_job_postings.csv\")\n\n\ndf.printSchema() # comment this line when rendering the submission\ndf.show(5)\n\n\n\n\nCode\n# clean the data\n\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql.functions import when, col\n\ndf= df.drop(\"ID\" ,\"LAST_UPDATED_DATE\",\"LAST_UPDATED_TIMESTAMP\",\"DUPLICATES\", \"EXPIRED\",\"SOURCE_TYPES\",\"SOURCES\",\n            \"URL\", \"ACTIVE_URLS\", \"ACTIVE_SOURCES_INFO\",\"TITLE_RAW\",\"BODY\", \"MODELED_EXPIRED\", \"MODELED_DURATION\", \"COMPANY\", \n            \"COMPANY_NAME\", \"COMPANY_RAW\", \"COMPANY_IS_STAFFING\", \"EDUCATION_LEVELS\")\n\ndf = df.withColumn(\"REMOTE_TYPE_NAME\", \n                   when(col(\"REMOTE_TYPE_NAME\") == \"[None]\", \"Not Remote\")\n                   .otherwise(col(\"REMOTE_TYPE_NAME\")))\n\ndf_clean = df.na.drop(subset=[\n    \"salary\", \"MIN_YEARS_EXPERIENCE\", \"MAX_YEARS_EXPERIENCE\",\n    \"EDUCATION_LEVELS_NAME\", \"EMPLOYMENT_TYPE_NAME\", \"REMOTE_TYPE_NAME\",\n    \"DURATION\",\n])\n\ndf_clean.show(5)\n\n\n\n\nCode\ntitles = df_clean.select(\"TITLE_NAME\").distinct().rdd.flatMap(lambda x: x).collect()\nfor title in titles:\n    print(title)\n\n\n\n\nCode\n#gave chatgpt the list and told it to split into ai impacted and not\nai_impacted_jobs = [\n    'Trust Officers',\n    'Cloud Migration Engineers',\n    'EDI Developers',\n    'Permit Specialists',\n    'Data Integration Leads',\n    'Blockchain Developers',\n    'Lead Intelligence Analysts',\n    'Sales Architects',\n    'Business Intelligence Leads',\n    'Data Analyst Managers',\n    'Project Support Analysts',\n    'Sales Planning Analysts',\n    'Hourly Managers',\n    'Industrial Equipment Mechanics',\n    'Value Engineers',\n    'Enterprise Applications Consultants',\n    'SAP FICO Consultants',\n    'SAP ABAP Consultants',\n    'Oracle Erp Consultants',\n    'Member Liaisons',\n    'Data Solutions Analysts',\n    'Transformation Analysts',\n    'GIS Data Analysts',\n    'Japanese Bilingual Administrative Assistants',\n    'Project Leads',\n    'People Analytics Analysts',\n    'Human Resources Reporting Analysts',\n    'Implementation Consultants',\n    'Lead Enterprise Architects',\n    'Branch Bankers',\n    'Growth Marketing Analysts',\n    'Chargeback Analysts',\n    'Strategy Leads',\n    'Innovation Analysts',\n    'Business Insights Managers',\n    'CRM Business Analysts',\n    'Localization Producers',\n    'EDI Analysts',\n    'Scientific Data Analysts',\n    'Bilingual Japanese Customer Service Representatives',\n    'SQL/ETL Developers',\n    'Data Quality Leads',\n    'Data Visualization Analysts',\n    'Data Analytics Engineers',\n    'Foundation Administrators',\n    'SQL Reporting Analysts',\n    'Procurement Analysts',\n    'Manual Testers',\n    'Analytics Associates',\n    'Supply Chain Architects',\n    'SAP SD Analysts',\n    'Oracle Cloud Financials Consultants',\n    'Data Quality Assurance Analysts',\n    'Client Finance Directors',\n    'Population Health Analysts',\n    'Enterprise Solutions Consultants',\n    'Digital Product Analysts',\n    'Line Pilots',\n    'Processing Clerks',\n    'Client Solutions Strategists',\n    'Business Intelligence Specialists',\n    'Accounting Consultants',\n    'Business Intelligence Analysts',\n    'SAP CRM Consultants',\n    'Human Capital Management Consultants',\n    'IAM Architects',\n    'SAP Ariba Consultants',\n    'Territory Assistants',\n    'Immigration Analysts',\n    'Customer Experience Associates',\n    'Global Analysts',\n    'Analysts',\n    'Document Management Clerks',\n    'Data Reporting Analysts',\n    'Quality Analysts',\n    'SAP HANA Consultants',\n    'Site Analysts',\n    'Business Architects',\n    'Data Analytics Developers',\n    'Digital Solution Architect Managers',\n    'Information Governance Analysts',\n    'Business Coaches',\n    'Configuration Management Analysts',\n    'Commercial Analysts',\n    'Analytics Consultants',\n    'Buyers',\n    'Enterprise Solutions Architects',\n    'Remediation Analysts',\n    'Search Planners',\n    'Insurance Associates',\n    'People Operations Generalists',\n    'Appeals Specialists',\n    'IT Project Assistants',\n    'Data Analytics Architects',\n    'Methods Analysts',\n    'Liquidity Analysts',\n    'Functional Consultants',\n    'SAP HR Consultants',\n    'Performance Directors',\n    'ERP Implementation Specialists',\n    'Tribal Administrators',\n    'Study Design Leads',\n    'Equipment Analysts',\n    'Quality Assurance Monitors',\n    'Integration Team Leads',\n    'Invoice Analysts',\n    'SAP FICO Functional Consultants',\n    'Enterprise Relationship Managers',\n    'Enterprise Data Architects',\n    'SAP PP/QM Consultants',\n    'Management Analysts',\n    'Speech Assistants',\n    'SAP Business Analysts',\n    'Data Migration Analysts',\n    'SAP Techno Functional Consultants',\n    'SAP Supply Chain Consultants',\n    'Enterprise Directors',\n    'Data Processing Analysts',\n    'SAS Data Analysts',\n    'Manufacturing Consultants',\n    'Demand Analysts',\n    'Netsuite Principal Consultants',\n    'OTM Consultants',\n    'Modernization Superintendents',\n    'Mapping Analysts',\n    'Enterprise Systems Architects',\n    'Business Intelligence Associates',\n    'Oracle Business Analysts',\n    'SAP Support Analysts',\n    'Automation Engineers',\n    'Excel VBA Developers',\n    'Financial Data Analysts',\n    'Power Analysts',\n    'Sales Analysts',\n    'Lead IT Analysts',\n    'Development Analysts',\n    'Analytics Managers',\n    'Financial Applications Specialists',\n    'Sales Solution Architects',\n    'Survey Analysts',\n    'Data Analysts/Data Scientists',\n    'Quality Control Reviewers',\n    'Oracle EBS Consultants',\n    'Data Services Analysts',\n    'ERP Implementation Managers',\n    'Knowledge Analysts',\n    'Enterprise Business Analysts',\n    'Test Data Analysts',\n    'Techno Functional Analysts',\n    'Netsuite Consultants',\n    'Cryptologists',\n    'PMO Analysts',\n    'Reference Data Analysts',\n    'Clinical Data Analysts',\n    'Enterprise Business Architects',\n    'Enterprise Cloud Architects',\n    'SAP Security Consultants',\n    'Data Leads',\n    'Tools Developers',\n    'Marketing Analysts',\n    'Financial Planning and Analysis Analysts',\n    'Finance Systems Analysts',\n    'Production Operators',\n    'Oracle Functional Analysts',\n    'IT Buyers',\n    'Process Engineers',\n    'Privacy Analysts',\n    'Enterprise Resources Planning Managers',\n    'Real Estate Analysts',\n    'Systems Integration Architects',\n    'End User Computing Analysts',\n    'Data Analysts/Developers',\n    'Publishing Specialists',\n    'SQL Analysts',\n    'Account Analysts',\n    'Engineering Data Analysts',\n    'Oracle EBS Business Analysts',\n    'Directors of Business Intelligence',\n    'Reporting Associates',\n    'SAP HCM Consultants',\n    'Feasibility Managers',\n    'Data Management Administrators',\n    'Walkers',\n    'Production Analysts',\n    'HRIS Associates',\n    'Data Analytics Leads',\n    'Data Analytics Specialists',\n    'Data Security Analysts',\n    'Principal Data Scientists',\n    'Researchers',\n    'Procurement Business Analysts',\n    'Oracle Applications Analysts',\n    'Forecast Analysts',\n    'Supply Chain Data Analysts',\n    'Analytics and Insights Managers',\n    'MDM Developers',\n    'Business Support Analysts',\n    'Food and Beverage Analysts',\n    'Intelligence Research Analysts',\n    'Validation Leads',\n    'Associate Business Managers',\n    'Enterprise Data Analysts',\n    'IT Governance Analysts',\n    'Domain Architects',\n    'Compliance Business Analysts',\n    'Implementation Specialists',\n    'Placement Managers',\n    'Corporate Architects',\n    'Splunk Developers',\n    'Work Force Management Analysts',\n    'Banking Consultants',\n    'Data Stewards',\n    'SAP MDM Consultants',\n    'SAP Specialists',\n    'Data Validation Analysts',\n    'Business Intelligence Data Warehouse Architects',\n    'Data Science Associates',\n    'Solution Leads',\n    'SAP Data Analysts',\n    'SAP Finance Consultants',\n    'Doctors',\n    'SQL Data Analysts',\n    'Patient Revenue Cycle Specialists',\n    'People Analytics Managers',\n    'Data Scientists',\n    'Digital Data Analysts',\n    'Data Control Clerks',\n    'Storeroom Clerks',\n    'Finance Business Analysts',\n    'SAP HR Analysts',\n    'Business Intelligence and Analytics Managers',\n    'Brand Activation Managers',\n    'Enterprise Project Managers',\n    'Data Analytics Consultants',\n    'Programmer Analysts',\n    'Sales Data Analysts',\n    'Data Reviewers',\n    'Contract Analysts',\n    'Decision Support Analysts',\n    'Data Associates',\n    'E-Commerce Architects',\n    'Risk Control Managers',\n    'Debt Specialists',\n    'Risk and Controls Managers',\n    'Data Entry Analysts',\n    'Platform Analysts',\n    'Financial Systems Analysts',\n    'Claims Resolution Analysts',\n    'Lead Business Intelligence Analysts',\n    'Inside Auto Claims Representatives',\n    'Customer Contact Center Managers',\n    'Data Governance Analysts',\n    'Business Operations Specialists',\n    'SAP Technical Consultants',\n    'Data Engineering Managers',\n    'SAP EWM Consultants',\n    'Tax Controllers',\n    'Transmission Analysts',\n    'Business Analysts',\n    'Credit Analysts',\n    'CSV Consultants',\n    'Patient Services Associates',\n    'Research Data Analysts',\n    'Medical Economics Analysts',\n    'SQL Administrators',\n    'SAP Master Data Analysts',\n    'Customer Care Analysts',\n    'Title Processors',\n    'Power Business Intelligence Developers',\n    'Data Quality Analysts',\n    'Quality Assurance Analysts',\n    'Consulting Engagement Managers',\n    'Vice Presidents of Operational Excellence',\n    'Insights Analysts',\n    'Inventory Analysts',\n    'Data Support Analysts',\n    'SAP Business One Consultants',\n    'Data Management Leads',\n    'Platform Administrators',\n    'Oracle Financial Functional Consultants',\n    'Architecture Managers',\n    'ERP Analysts',\n    'Growth Associates',\n    'Directors of School Nutrition',\n    'Operations and Policy Analysts',\n    'Lead Data Analysts',\n    'SAP SD Functional Consultants',\n    'Assistant Federal Security Directors',\n    'Oracle Cloud Architects',\n    'Quality Assurance Leads',\n    'SAP FICO Analysts',\n    'Repair Analysts',\n    'SAP SD Consultants',\n    'SAP SRM Consultants',\n    'Hyperion Consultants',\n    'SAP Systems Analysts',\n    'Adobe Experience Manager Architects',\n    'Quality Assurance Analysts/Engineers',\n    'Oracle Utilities Professional Services CC&B Implementation Consultants',\n    'SAP OTC Consultants',\n    'Data Warehouse Business Analysts',\n    'Client Data Analysts',\n    'Documentum Administrators',\n    'Project Analysts',\n    'Korean Linguists',\n    'Data Governance Managers',\n    'Oracle Consultants',\n    'Navigators',\n    'Customer Data Analysts',\n    'Enterprise Technical Architects',\n    'Cash Management Associates',\n    'Equity Traders',\n    'Financial Analysts',\n    'ERP Architects',\n    'Business Intelligence and Data Analysts',\n    'Automation Consultants',\n    'Governance Analysts',\n    'ERP Systems Analysts',\n    'Commercial Banking Credit Analysts',\n    'Certified Public Accountants',\n    'Data Operations Analysts',\n    'Category Analysts',\n    'Regional Business Managers',\n    'GCP Auditors',\n    'Architectural Project Coordinators',\n    'Delivery Analysts',\n    'SAP MM Functional Consultants',\n    'Techno Functional Leads',\n    'Clinical Data Managers',\n    'Administrators',\n    'Enterprise Network Managers',\n    'Healthcare Business Analysts',\n    'Solutions Architects',\n    'Ward Clerks',\n    'Account Coordinators',\n    'Data Abstractors',\n    'Data Scientists/Analysts',\n    'SAP Functional Consultants',\n    'Product Lifecycle Managers',\n    'Data Intelligence Analysts',\n    'Data Warehouse Analysts',\n    'CNC Administrators',\n    'Life Sciences Consultants',\n    'Data Processing Clerks',\n    'Production Planning Analysts',\n    'Business Growth Consultants',\n    'Chief Architects',\n    'Logistics Administrators',\n    'Vendor Analysts',\n    'Quantitative Analytics Managers',\n    'Treasury Consultants',\n    'Crime Analysts',\n    'SAP Business Process Analysts',\n    'Call Center Analysts',\n    'GRC Analysts',\n    'Competitive Intelligence Analysts',\n    'Oracle Technical Consultants',\n    'Headend Engineers',\n    'Claims Business Analysts',\n    'General Managers',\n    'Systems Architecture Engineers',\n    'SAP Ariba Managers',\n    'Customer Analytics Managers',\n    'Customer Experience Analysts',\n    'LIS Analysts',\n    'SAP Consultants',\n    'SAP Hybris Consultants',\n    'SAP PI Consultants',\n    'Enterprise Application Analysts',\n    'Data and Analytics Consultants',\n    'Agile Product Owners',\n    'Business Intelligence Reporting Analysts',\n    'Management Associates',\n    'Health Data Analysts',\n    'Enterprise Systems Engineers',\n    'Lead Analysts',\n    'Data Migration Consultants',\n    'Professors of Biological Sciences',\n    'Oracle Business Systems Analysts',\n    'Regional Fleet Managers',\n    'Enterprise Systems Analysts',\n    'Inflight Supervisors',\n    'Operations Process Engineers',\n    'SAP Developers',\n    'Customer Care Experts',\n    'Reporting Analysts',\n    'Commercial Excellence Managers',\n    'Lead Quality Analysts',\n    'Oracle EBS Technical Consultants',\n    'Records Assistants',\n    'Directors of Cloud Security',\n    '.NET Technical Architects',\n    'Associates',\n    'Trend Analysts',\n    'Digital Analysts',\n    'Innovation Specialists',\n    'Data Governance Leads',\n    'Data Analysts',\n    'Directors of Sponsored Programs',\n    'Enterprise Resource Planning Managers',\n    'Wealth Management Analysts',\n    'Quantitative Data Analysts',\n    'Origination Associates',\n    'Enterprise Business Consultants',\n    'SAP Application Consultants',\n    'Information Analysts',\n    'Process Improvement Analysts',\n    'Consumer Analysts',\n    'Communications Analysts',\n    'GIS Consultants',\n    'Data Analyst Specialists',\n    'Plant Maintenance Managers',\n    'Netsuite Functional Consultants',\n    'Plant Chemists',\n    'Integration Consultants',\n    'Data Management Analysts',\n    'Oracle Financial Consultants',\n    'SAP Plant Maintenance Consultants',\n    'Analytics Leads',\n    'Lead Level Designers',\n    'Solutions Analysts',\n    'Data and Reporting Analysts',\n    'Data Analytics Product Managers',\n    'SAP Analysts',\n    'Implementation Analysts',\n    'Revenue Cycle Analysts',\n    'Case Management Associates',\n    'SAP Successfactors Consultants',\n    'IT Governance Managers',\n    'Sales Professionals',\n    'Principal Architects',\n    'Oracle HCM Consultants',\n    'Data Science Analysts',\n    'HANA Consultants',\n    'OSP Managers',\n    'Interface Analysts',\n    'Transportation Systems Analysts',\n    'Forms Analysts',\n    'Master Data Coordinators',\n    'Enterprise Application Architects',\n    'Epic Security Analysts',\n    'Intercompany Accountants',\n    'Client Insights Analysts',\n    'Data Analytics Associates',\n    'SAP SD/MM Consultants',\n    'SAP Administrators',\n    'Supply Chain Business Managers',\n    'Assistant Service Experience Managers',\n    'Data Clerks',\n    'Data Visualization Specialists',\n    'Growth Specialists',\n    'SAP Basis Leads',\n    'Functional Analysts',\n    'SAP Functional Analysts',\n    'Knowledge Experts',\n    'Artificial Intelligence Engineers',\n    'Financial Planning and Analysis Managers',\n    'Business Intelligence Data Analysts',\n    'Peoplesoft Business Analysts',\n    'Platform Architects',\n    'Systems Architects',\n    'Assessment Analysts',\n    'Solutions Consultants',\n    'Surface Designers',\n    'SAP GTS Consultants',\n    'Data Insights Analysts',\n    'Oracle Cloud Technical Consultants',\n    'Data Modelers',\n    'Distribution Consultants',\n    'ERP Administrators',\n    'SAP MM Consultants',\n    'ERP Consultants',\n    'Financial Planning Analysts',\n    'Intelligence Analysts',\n    'SAP Basis Consultants',\n    'Consulting Technical Managers',\n    'Insights and Reporting Analysts',\n    'Advanced Analytics Analysts',\n    'Medical Data Analysts',\n    'Solution Architects/Principal Consultants',\n    'Technical Architects',\n    'Data and Analytics Managers',\n    'Travel and Expense Analysts',\n    'OFSAA Business Analysts',\n    'Facilities HVAC Technicians',\n    'Data Analytics Analysts',\n    'Web Data Analysts',\n    'Staff Data Engineers',\n    'Program Data Analysts',\n    'Associate Consultants',\n    'Safety Data Analysts',\n    'Controllers',\n    'CAD Operators',\n    'Security Analysts',\n    'Oracle Finance Functional Consultants',\n    'Enterprise Architects',\n    'Finance Data Analysts',\n    'ERP Business Systems Analysts',\n    'IT Data Analysts',\n    'Business Analysis Interns',\n    'Information Services Analysts',\n    'Data Solutions Consultants',\n    'Enterprise Program Managers',\n    'SAP MM Analysts',\n    'IT Applications Specialists',\n    'Oracle EBS Functional Consultants',\n    'Research and Evaluation Analysts',\n    'Customer Support Leads',\n    'Oracle SCM Functional Consultants',\n    'IT Data Analytics Analysts',\n    'Finance Assistants',\n    'Business Operations Associates',\n    'Quality Assurance Automation Testers',\n    'Technical Project Managers',\n    'Data Science and Analytics Managers',\n    'Oracle Cloud HCM Consultants',\n    'Business Intelligence Managers',\n    'Customer Support Administrators',\n    'Procurement Operations Associates',\n    'Analytics and Reporting Analysts',\n    'Research Specialists',\n    'BSA Consultants',\n    'Oracle Systems Analysts',\n    'Bilingual Office Managers',\n    'Institutional Sales Analysts',\n    'Consulting Solutions Architects',\n    'Corporate Finance Associates',\n    'Immunology Specialists',\n    'Global Trade Analysts',\n    'Search Coordinators',\n    'Higher Education Consultants',\n    'Enterprise Risk Analysts',\n    'ERP Support Specialists',\n    'Spanish and English Teachers',\n    'Assistant Vice Presidents',\n    'Data Protection Specialists',\n    'Manufacturing Services Managers',\n    'Cash Managers',\n    'Pricing Data Analysts',\n    'Directors of Toxicology',\n    'Data Acquisition Analysts',\n    'Process Analysts',\n    'Data Technicians',\n    'Clinical Quality Analysts',\n    'ERP Specialists',\n    'IAM Analysts',\n    'Data Infrastructure Engineers',\n    'Industry Analysts',\n    'Oracle Database Developers',\n    'Platform Support Specialists',\n    'School Education Managers',\n    'Clinical Data Associates',\n    'Oracle Functional Consultants',\n    'Enterprise Analysts',\n    'Manufacturing Analysts',\n    'Targeting Analysts',\n    'Master Data Analysts',\n    'SAP Basis Administrators',\n    'Data Center Analysts',\n    'Business Assistants',\n    'Philanthropy Assistants',\n    'IT Analysts',\n    'Middle Office Analysts',\n    'Investment Data Analysts',\n    'Salesforce Consultants'\n]\n\n\n\n\nCode\nfrom pyspark.sql.functions import when, col, lit, avg, count\n\ndf_final = df_clean.withColumn(\n    'ai_impacted',\n    when(col('title_name').isin(ai_impacted_jobs), lit('yes')).otherwise(lit('no'))\n)\n\navg_salary = df_final.groupBy(\"state_name\").agg(avg(\"salary\").alias(\"avg_salary\"))\n\n\ndf_final = df_final.join(avg_salary, on=\"state_name\", how=\"left\")\n\ncount = df_final.groupBy(\"state_name\").agg(count(\"*\").alias(\"count\"))\n\n\ndf_final = df_final.join(count, on=\"state_name\", how=\"left\")\n\ndf_final.select(\"count\").distinct().show()\n\n\n\n\nCode\ndf_final_pd = df_final.toPandas()\n\nfig = px.scatter(\n    df_final_pd,  \n    x=\"STATE_NAME\",                       \n    y=\"avg_salary\",                 \n    size=\"count\",                 \n    color=\"ai_impacted\",                  \n    title=\"Average Salary by State and AI Impact\",\n    size_max=60,\n    color_discrete_map={\n        \"yes\": \"#2dbf78\",   \n        \"no\": \"#8adfbd\"  \n    }\n\n)\n\nfig.update_layout(\n    legend_title_text=\"AI Impacted\",\n    xaxis_title=\"State\",\n    yaxis_title=\"Average Salary\",\n    xaxis=dict(tickangle=45),\n    yaxis=dict(categoryorder='category descending')\n)\n\nfig.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Preliminary Research",
    "section": "",
    "text": "What does the job market look like, and how can we refine our skills to better match the career we want?"
  },
  {
    "objectID": "index.html#purpose",
    "href": "index.html#purpose",
    "title": "Preliminary Research",
    "section": "Purpose",
    "text": "Purpose\nWith this in mind, our research will look at the rising trends within the job market, with a particular focus on roles that align with our own career interests. This includes analyst positions, some of which may be influenced by advances in AI. We will also look at what working conditions, more specifically remote versus not remote jobs."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello\nAbout this site"
  }
]